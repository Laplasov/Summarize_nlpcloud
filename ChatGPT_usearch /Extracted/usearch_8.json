[
  {
    "body": "take over some of the most menial HR tasks\n, like writing job descriptions or running compensation audits. How job candidates might use the tool, and the employer impact, provides another interesting viewpoint.\nThe most obvious use case for applicants is executing odious tasks like rsum writing. So I took it upon myself to experiment with that.\nThe bottom line is that while employers almost certainly wont encourage or want candidates to use ChatGPT to create a rsum, applicants likely will due to the sheer ease and access of the platform. It took me less than five minutes to answer a few automated prompts from the chatbot, which then spit out a sample rsum. Heres how it works.\nOnce a person signs up for a free ChatGPT account via the OpenAI site, they can ask any question their heart desires. I tapped out a simple query: Can you write my job rsum? The response was instantaneous. The chatbot agreed that it could help me, but much like a stern tutor, it made it clear that it would not do all the work for me.\nAfter I agreed with the recommendations, the chatbot asked me to provide some basic information such as work experience, education, skills, and other relevant details I would like to include on my rsum. Please provide me with that information, and Ill do my best to help you create a strong and effective rsum, responded my new bot friend. I responded with seven basic lines about myself:\nB.A. in English and Journalism from Wake Forest University\nM.S. in Strategic Communications from Columbia University\nReporter at the Wall Street Journal\nReporter at Protocol\nPulitzer Prize Finalist\nCharlotte, N.C.\nSeconds later, I received a fully drafted rsum. The A.I. intuited my skill set and years of work based on my listed experience. But what surprised me most was that the technology took several creative liberties, embellishing and padding my rsum to provide examples of what else I could add to my curriculum vitae. After all, the rsum it produced is only supposed to be a sample, providing suggestions for language and structure based on the information I input. Now, let me answer the question you all want to know: How well-written is the rsum? It would be difficult for most HR leaders to know an A.I. chatbot authored it and even harder to prove.\nThough some have criticized the technology for providing stilted or robotic-sounding language, it was almost imperceptible that a robot had written my rsum. What that means for HR leaders is theyll have to scrutinize provided rsums as the technology becomes more widespread. Or, theyll have to place less emphasis on the rsum to determine an applicants potential to succeed in a given role.\nIn a matter of minutes, I had a shiny new rsum that displayed my journalistic chops while falsely stating that I had been named among Forbes 30 Under 30 in Journalism in 2020. It also stated I was the winner of the George Polk Award for Business Reporting in 2018, and according to ChatGPT, Im fluent in Spanish. While I found the results to be both awe-inspiring and slightly comedic, for employers, theres a more sober takeaway.\nChatGPT might help reduce the time it takes applicants to apply for jobs in the future, but HR professionals will need to spend more time reviewing and vetting rsums as technology grows more advanced.\nAmber Burton\nReporter's Notebook\nThe most compelling data, quotes, and insights from the field.\nThe race will go to those who master change management this year, says Brandi Galvin Morandi, chief legal and human resources officer at data infrastructure company Equinix.\nThese last few years have reconfirmed that no company is perfectly guarded against the general ups and downs of business cycles and the potential impact of global events, no matter how strong their business fundamentals may be. The best way to strengthen a companys position and future-proof its business is to ensure its people are equipped for business transformation.\"\nAround the Table"
  },
  {
    "body": "It's \"the latest in pawtifurcial intelligence.\"\nCatGPT\nSure, OpenAI's uber-popular AI chatbot ChatGPT is extremely good at spitting out some seriously impressive content, from believable college essays to source code and even real estate listings.\nBut that kind of AI prowess left Dutch data journalist Wouter van Dijke wanting more. The self-proclaimed \"Twitter bot enthusiast\" took it upon himself to answer the ultimate question: \"what if ChatGPT were a cat?\"\n\"ChatGPT is boring,\" he wrote on his GitHub. \"I want a cat to answer my questions. So I built CatGPT!\"\nCatGPT, as its name suggests, allows you to ask a \"pawtifurcial intelligence\" pretty much anything you'd want to ask a real-life cat.\nWhat you get in response is a series of \"meows\"  since, well, cats can't speak English.\n\"CatGPT uses a purr-al network and an advanced hairballgorithm to come up with natural-sounding responses,\" van Dijke wrote in his pun-laden documentation.\nRandom Meows\nThe reality, as you might guess, is that the tech isn't particularly advanced.\n\"Not really though, it just returns random meows,\" van Dijke admitted.\nOf course, that's likely true of actual cats, too.\n\"To be clear: this site does not actually use ChatGPT or any other form of AI,\" he wrote. \"Nothing is done with the user input either.\"\nWhile it's a fun and tongue-in-cheek take on ChatGPT, the project was more of a self-directed lesson in how to construct a basic website that looks and acts exactly like the real thing.\n\"It took some back and forth to get something looking alright, but it was quite useful to create a basic structure for the web page,\" van Dijke wrote.\nBut it also happens to be exactly the kind of levity we needed after weeks of reporting on the slow death of journalism at the hands of AI.\nOr, in the words of CatGPT: \"Meow, meow meow meow, meow meow?\"\nMore on ChatGPT:"
  },
  {
    "body": "Stat:Promises  And Pitfalls  Of ChatGPT-Assisted Medicine\nNot long after the artificial intelligence company OpenAI released its ChatGPT chatbot, the application went viral. Five days after its release, it had garnered 1 million users. Since then, it has been called world-changing, a tipping point for artificial intelligence, and the beginning of a new technological revolution. (Rushabh H. Doshi and Simar S. Bajaj, 2/1)\nThe New York Times:We Wanted Chinas Zero Covid To End, But Not Like This\nFor three years, Chinas people were told that Covid had to be controlled. But the government suddenly reversed course not long after street protests broke out in November over the escalating human and economic costs of that approach. But little was done to prepare us for what came next. (Lucy Meng, 2/1)"
  },
  {
    "body": "A graduate of the Russian State University for the Humanities said that he defended a diploma written by the ChatGPT neural networkA graduate of the Russian State University for the Humanities defended a diploma written by the ChatGPT neural network.  He reported this in his account on Twitter and confirmed in comments Baza edition.According to the story of a Muscovite who introduced himself as Alexander, he decided to use a neural network to combine business with pleasure - to get rid of scientific work and test the capabilities of a new neural network.According to the graduate, in total he spent less than a day working with the neural network.  The diploma took 23 hours: about 15 hours to imposingly write, and 8 hours of suffering at night to have time to edit it all by the deadline, he said.Related materials:At the same time, he noted that he did a significant part of the work on his own - he entered the necessary queries, corrected the results issued by ChatGPT, and translated them from English into Russian.  Alexander also added that his acquaintances, who observed the work with the neural network, could not always determine which of the fragments of the diploma was written by him, and which by the algorithm.As a result, the student received a satisfactory mark for the scientific work written by the neural network, and the reviewer even praised some aspects of his work, noting the quality of the design and the novelty of the study.ChatGPT is an artificial intelligence bot developed by OpenAI.  He knows how to conduct a conversation, give answers to questions asked, as well as edit and create computer code and even poems and songs.  Musician Nick Cave called writing songs using this neural network a mockery of humanity.\r\n#Muscovite #defended #diploma #written #neural #network"
  },
  {
    "body": "Published Feb 1st, 2023 8:28AM EST\nImage: Sean Gallup/Getty Images\nUnleashed upon the world late last year, ChatGPT is a viral sensation. The chatbot can offer answers to questions, write computer code, and pass law and business school exams. It has taken the tech world by storm. Google initially downplayed ChatGPTs threat to Google Search, but the company internally declared a code red afterward, shifting tons of resources to its own AI products. One of them might already be in testing at Google, an Apprentice Bard chatbot that appears on the Search home page, offering features similar to ChatGPT.\nNobody would blame people for thinking ChatGPT is the kind of feature that Google would develop for the future of Search. Its precisely what we expect from a company that made massive progress with human speech and artificial intelligence (AI).\nBut Google might not want to release products like ChatGPT until theyre ready to be as reliable as traditional Search. Thats what Google hinted at when first reacting to ChatGPT.\nReports that followed showed Sundar Pichai wants all hands on deck at Google, working on ChatGPT-like products that could be demoed this spring. Even Googles co-founders Sergey Brin and Larry Page are back for this endeavor.\nUnsurprisingly, Google is already testing a ChatGPT-like home page. Its called Apprentice Bard, and it uses Googles LaMDA language technology. Thats according to CNBCs sources.\nIf accurate, this proves Google has had products like ChatGPT in development all along. OpenAI just beat them to market with a ChatGPT product thats not really ready to replace Google Search.\nU.S. Rep. Ted Lieu on January 26 introduced the first-ever congressional legislation written by the AI-powered ChatGPT. Image source: phonlamaiphoto/Adobe\nCNBC says Googles name for its code red reaction to ChatGPT is called Project Atlas. Thats in addition to the Apprentice Bard chatbot that Googlers can already test internally. Furthermore, Google is reportedly testing a new Search desktop design that could support a question-and-answer form for Apprentice Bard.\nHow Googles ChatGPT alternatives for Search work\nAs a result of ChatGPT, the LaMDA team has been asked to prioritize working on a response to ChatGPT, said an internal Google memo viewed by CNBC. In the short term, it takes precedence over other projects.\nApprentice Bard looks like ChatGPT but uses the LaMDA tool for dialogue with users. The report notes that Googles chatbot can deliver answers that include recent events. Thats something ChatGPT doesnt have access to yet. Moreover, Googlers have noticed that Apprentice Bards responses have become more advanced in recent weeks:\nIn one example circulated internally, a tester asked Apprentice Bard if there will be another round of layoffs at Google. The company laid off 12,000 employees, 6% of its workforce, earlier this month.\nAccording to my data base access, it is unlikely for Google to conduct another round of layoffs in 2023, the response reads. Layoffs are generally conducted to reduce costs and structure, but the company is doing well financially. In fact, Googles revenue increased by 34% in 2021, and the companys stock price has risen by 70% since January 2022.\nGoogle is also testing an alternate Search page that would support questions and answers:\nOne view showed the home search page offering five different prompts for potential questions placed directly under the main search bar, replacing the current Im feeling lucky bar. It also showed a small chat logo inside the far right end of the search bar.\nWhen the user enters a question, the search results show a grey bubble under the search bar. This will offer more human-like responses than regular search results. The page will also suggest follow-up questions related to the first one. Only then will you see regular search results, like links and headlines.\nFinally, the report says Google has been testing a beta LaMDA chatbot against ChatGPT.\nIts unclear when Google might make its ChatGPT rivals available to the public, even in a beta form. But the CNBC report suggests that while Google might be scared of ChatGPT, its definitely ready to compete. With such AI projects in development, it seems highly unlikely that Google Search will lose its search supremacy to ChatGPT without a fierce fight."
  },
  {
    "body": "whatsapp\nChatGPT is taking the world by storm with AI-generated text that rivals actual literature  but theres a price. For its parent company, OpenAI, to get this quality product, human beings are paid to help teach it to identify the bad stuff: hate speech and violence of all kinds. We hear about the good, the bad, and the ugly of ChatGPT, including from one person who did the job and has real questions about whether its worth the price.\nNote: This episode contains elements that were generated using ChatGPT.\nIn this episode:\n), author of Digital Democracy, Analogue Politics: How the Internet Era Is Transforming Politics in Kenya\nMichael Kearns ("
  },
  {
    "body": "Australian singer Nick Cave came out strongly against an AI-generated track \"in the style of Nick Cave\" sent to him in January by a fan. It is \"bullshit,\" said Cave. Meanwhile, three female fine artists recently filed a class action lawsuit in the US against several AI companies, charging them with theft of creative ideas. There's no doubt that artificial intelligence is making its way into the art world. While the consequences remain uncertain at this early stage, artists are already concerned about the appropriation of their intellectual property. (Also read: ChatGPT owner launches 'imperfect' tool to detect AI-generated text)Will AI forever change art?Nonetheless, the art market has begun to do business with AI art as algorithm-generated works feature in museum exhibitions. But how will AI change creative expression itself?Ai-Da is the world's first humanoid robot artist to draw and paint with the help of AI and caused a sensation when it created works at the 2022 Venice Art Biennale.Algorithms process the information and let the robot's arm draw portraits of people. Ai-Da's creator, Aiden Miller, developed her with a team of computer scientists, robotics experts and designers. Each of Al-Da's works is unique. But is it creative?Here come the robotsThe Vitra Design Museum near Basel named its current exhibition about the relationship between man and machine, \"Hello, Robot.\"\"What fundamentally interests people in robotics and artificial intelligence is the very old human longing to play God,\" says curator Amelie Klein.Her show features a robot that writes manifestos. \"This robot is dumb as a box of rocks,\" she explained. \"It knows three languages, vocabulary, grammar, syntax, but it doesn't understand what it's writing,\" Klein told DW.While Robots, androids and artificial intelligence have long been a feature of sci-fi films, from \"Metropolis\" to \"Blade Runner\" and \"Matrix,\" can AI progress to feel emotions and realize art?Not any time soon, according to Vitra Design Museum director, Matteo Kries, who insists that \"the machine cannot feel emotions.\"'This song sucks'Nick Cave would agree. The chorus of the AI-generated song a fan sent him in a Nick Cave \"style\" is not quite poetry.\"I am the sinner, I am the saint, I am the darkness, I am the light, I am the hunter, I am the prey, I am the devil, I am the savior.\"Cave posted a response on his website, explaining why he feels the ChatGPT-generated song is \"bullshit\" and a \"grotesque mockery of what it is to be human.\"AI might be able to write a speech, essay, sermon or obituary, but not a \"genuine\" song, Cave says.\"Songs arise out of suffering, they are predicated upon the complex, internal human struggle of creation.\"\"Algorithms don't feel, data doesn't suffer,\" Cave writes.When machines dabble in artWhen a brand-new Rembrandt appeared on the scene in the Netherlands in 2016, it was an \"in the style of the Dutch master\" AI-created work fed with data from 346 actual Rembrandt paintings.The countless algorithms that calculated the image were created by a team of programmers and scientists from Delft University of Technology, and Microsoft AI experts.Two years later, an AI-generated work of art achieved a respectable sale for the first time. A portrait named Edmond De Belamy, created by an algorithm fed with 15,000 paintings from different eras, was auctioned at Christie's auction house in New York for $433,000 (397,000).The French creative collective Obvious provided an entirely fictional De Belamy family tree, a golden frame  and instead of the artist's signature, the algorithmic formula.What AI programs like Midjourney, Dall-E and Stable Diffusion can do for images, Chat GPT developed by Open AI can do for text content. It conducts conversations, answers questions, and composes texts of various kinds on demand. Students increasingly use the software.Who is the author?Who owns a work generated by a machine that has been fed with millions of  copyrighted  images and texts? Who is the originator of the creation? Is the AI the author, or are the programmers? Perhaps Picasso, Rembrandt or Van Gogh and the artists and writers whose works provided the data for the machine?Vitra Design Museum director Kries does not believe artificial intelligence will eventually replace the emotional aspect of art and creativity.Vinzent Britz, art director, motion designer and lecturer at Berlin's College of Fine Arts, compares the current situation to the transition from classical painting to photography.Before photography became an established art form, \"there was resistance, people said that's not art, that's a photo, that's plagiarism,\" Britz told DW. He expects a similar development in the case of artificial intelligence.Making AI 'fair and ethical'The three artists  Sarah Andersen, Kelly McKernan and Karla Ortiz  that have filed a class action lawsuit against Stability AI, Midjourney and the Deviant Art platform seek damages and an \"injunction to prevent future harm.\"Their attorney Matthew Butterick writes on his website that the lawsuit is \"another step toward making AI fair and ethical for everyone.\"It seems there is still a long way to go.This article was originally written in German."
  },
  {
    "body": "Share\nResearch shows that when new hires dont pan out, 89% of the time, the causes are attitudinal, not technical. In other words, candidates generally dont fail because they lack the technical skills to perform the job. Instead, they may lack coachability or emotional intelligence, or perhaps they have the wrong temperament to fit the companys culture.\nFor recruiters and hiring managers, that means assessing candidates requires an eye for subtlety and nuance; discerning a candidates coachability is not as straightforward as assessing their coding skills.\nWhich brings us to ChatGPT, which is essentially a conversational chatbot. This latest AI is frighteningly smart, but its not quite ready to pick up on the nuance embedded in candidates responses. We recently used the ChatGPT to assess real candidate responses, and as youll see, a good deal of nuance gets missed.\nIn a recent report on behavioral interview questions, we found that the following question was highly effective in differentiating high- and low-performing candidates: Could you tell me about a time when you were asked to do something you didnt know how to do?\nWe gave ChatGPT that interview question and asked it to assess an actual answer from a real-life candidate. Heres the answer we used:\nOh, thats easy. I was always learning on the fly at my last job mostly because we didnt have a lot of processes in place. Customers were always complaining that our billing system was seriously flawed, and they were right.\nI remember one time when a customer had a billing issue and my boss told me to check with my co-worker, Sam, to see if he had any knowledge of the case. But Sam had nothing. In situations like this, I will take it upon myself to track back into the customers records for clues or I will call the customer to see if something was missed. In this case, I really had to go on the hunt to find information that would help fix the customers billing issue. I find that theres usually an answer if you look hard enough.\nWhat do you think of that candidates response? Was it specific? Did they answer the interview question? Heres what ChatGPT thought of the candidates response:\nThe candidate answered the question and provided a specific example of a time when they were asked to do something they didnt know how to do. They also explained their approach to handling the situation and their determination to find a solution. They demonstrated the ability to think critically and problem-solve. The candidate gave a good answer to the question.\nArticle Continues Below\nWords That Cost You The Interview\n discovered that first-person pronouns and past-tense verbs are often powerful indicators that candidates are providing direct and specific answers to interview questions. This candidate used far more first-person than second or third-person pronouns, and they used many more past-tense than present or future-tense verbs.\nThe candidate was asked to tell about a time when they were asked to do something they didnt know how to do, and the candidate responded by saying, I remember one time when a customer had a billing issue The person has clearly recalled a specific event in response to our question.\nBut lets go a bit deeper. After the candidate explained that their co-worker didnt have an easy answer to the customers billing issue, the individual shifted their language, providing hypothetical rather than specific solutions. When they say, In situations like this, I will take it upon myself to track back into the customers records for clues or I will call the customer to see if something was missed, theyre not explaining what they actually did, only what they would hypothetically do.\nThe candidate does say, In this case, I really had to go on the hunt to find information that would help fix the customers billing issue, but they fail to offer any specifics about what going on the hunt entailed or what solutions they devised.\nOn a superficial first reading, the candidate answered our interview question. But when we dig a bit deeper, it becomes clear that the candidate didnt really tell us much of anything. Sure, there was a time when they didnt know how to do something, but we didnt learn anything about specific steps they took to overcome that challenge (or even if they overcame it).\nThe point here is that for all the potentially transformative power of artificial intelligence like ChatGPT, theres still a lot of the talent acquisition process that requires human skill and discernment. Maybe AI will get there eventually, but its not there yet."
  },
  {
    "body": ") since it bottomed in October, up more than 90% through its highs last week.\nBearish investors betting on a further decline must be stunned at how quickly investors\nhave jumped back into the Jensen Huang-led company.\nWe believe NVDA's outperformance is closely linked to the hype surrounding OpenAI's ChatGPT. Keen investors should know that the Generative AI platform is trained on Nvidia's GPUs. As such, investors optimistic about the potential of Generative AI have likely joined the recent buying frenzy.\nDeep learning is also \"expected to get bigger and more profitable.\" As the clear leader in the AI ecosystem, Nvidia is well-positioned to benefit from the hype (even if it doesn't translate to significant revenue generation yet in the near term), lifting its valuations further.\nCitigroup (\nAAPL\n) App Tracking Transparency has seen constructive developments. The WSJ reported recently:\nHeavy investment in artificial intelligence tools has enabled the company to improve ad-targeting systems to make better predictions based on less data, according to the interviews and documents. The company is shifting to forms of advertising less dependent on harvesting user data from off its platforms. - WSJ.\nAs such, despite the curtailment of enterprise IT spending due to significant macro headwinds, strategic AI investments requiring Nvidia's ecosystem could move to the forefront as companies look \"\nto quickly and economically\ngrow AI production at scale to drive business growth.\"\nTherefore, AI-first companies may need to build close partnerships with Nvidia as they explore opportunities to develop their strategic AI edge, as seen in Microsoft's collaboration with Nvidia, announced in November 2022.\nBaidu, Inc.'s (\nBIDU\n) announcement that it's ready to launch its ChatGPT-style chatbot demonstrates that China is highly advanced in AI. With Google potentially releasing its first Generative AI products in 2023, we believe it's clear that ChatGPT is moving beyond hype and into productization.\nWith this in mind, we believe it's clear why NVDA has recovered so quickly, as the bears likely didn't anticipate ChatGPT's public preview release to take the world by storm.\nBut, the critical question for NVDA investors has always revolved around its steep growth premium.\nAt an NTM EBITDA of 53.4x or an NTM normalized P/E of 49.5x, we believe NVDA is expensive.\nNvidia has surged significantly from its July 2022 lows of less than 27x, in line with its 10Y average of 27.1x.\nHence, we believe the reward/risk of adding NVDA now seems much less attractive than four weeks ago, when we revised our rating to a Buy.\nNVDA price chart (weekly) (TradingView)\nThe silver lining in NVDA's medium-term price chart is we believe its October lows will not likely be revisited.\nIts rapid recovery over the past four weeks sent NVDA surging to re-test its May and August 2022 highs, likely drawing some breakout traders/investors looking for a decisive upside break.\nHowever, we urge investors to avoid chasing the momentum surge, as a pullback looks increasingly likely, with last week's top a potential bull trap forming.\nDespite that, NVDA formed a higher low in December, suggesting that the trend of lower lows from November 2021 is likely over.\nThe next steep pullback will be your opportunity to add if you missed the lows in October and December. With the upcoming Fed meeting, Powell could grant patient investors the chance to add on weakness if he returns with his hawkish feathers dousing the recent optimism on an earlier-than-expected Fed pivot.\nRating: Hold (Revise From Buy).\nAre you looking to strategically enter the market and optimize gains?\nUnlock the key to successful growth stock investments with our expert guidance on identifying lower-risk entry points and capitalizing on them for long-term profits. As a member, you'll also gain access to exclusive resources including:\n24/7 access to our model portfolios\nDaily Tactical Market Analysis to sharpen your market awareness and avoid the emotional rollercoaster\nAccess to all our top stocks and earnings ideas\nAccess to all our charts with specific entry points\nReal-time chatroom support"
  },
  {
    "body": "Viber\nFile Photo: Allen And Co. Sun Valley Media Conference In Sun Valley, Idaho\nOpenAI, the creator of the popular chatbot ChatGPT, has released a software tool to identify text generated by artificial intelligence, the company said in a blog post on Wednesday.\nChatGPT is a free program that generates text in response to a prompt, including articles, essays, jokes and even poetry, which has gained wide popularity since its debut in November, while raising concerns about copyright and plagiarism.\nThe AI classifier, a language model trained on the dataset of pairs of human-written and AI-written text on the same topic, aims to distinguish text that is written by AI. It uses a variety of providers to address issues such as automated misinformation campaigns and academic dishonesty, the company said.\nIn its public beta mode, OpenAI acknowledges the detection tool is very unreliable on texts under 1,000 characters, and AI-written text can be edited to trick the classifier.\nWere making this classifier publicly available to get feedback on whether imperfect tools like this one are useful, OpenAI said.\nWe recognize that identifying AI-written text has been an important point of discussion among educators, and equally important is recognizing the limits and impacts of AI generated text classifiers in the classroom.\nSince ChatGPT debuted in November and gained wide popularity among millions of users, some of the largest U.S. school districts, including New York City, have banned the AI chatbot over concerns that students will use the text generator to cheat or plagiarize.\nOthers have created third-party detection tools including GPTZeroX to help educators detect AI-generated text.\nOpenAI said it is engaging with educators to discuss ChatGPTs capabilities and limitations, and will continue to work on the detection of AI-generated text.\n(Reuters)"
  },
  {
    "body": "Print\nOpenAI, the creator of ChatGPT, has released an AI Text Classifier tool that may be able to determine whether or not texts were written by AI.\nThe AI Text Classifier is a fine-tuned GPT model that predicts how likely it is that a piece of text was generated by AI from a variety of sources, such as ChatGPT, explains a new OpenAI blog post.\nThe AI text classifier is designed to distinguish between human-written text and AI-written text. However, OpenAI warns in a press release that the classifier is not fully reliable and should not be used as a primary decision-making tool.\nAccording to OpenAI, it can be useful in determining whether someone is attempting to pass off generated text as human-written text. OpenAI also stated that the model that powers the tool was trained using pairs of human-written text and AI-written text on the same topic.\nOpenAI also claims that it will incorrectly but confidently label human-written text as AI, especially if it differs significantly from anything in the training data. It emphasizes that the classifier is still a work in progress.\nFurthermore, OpenAI trained the tool primarily on English content written by adults, so false positives are entirely possible.\nThe tool was released after numerous universities and school districts banned the companys popular ChatGPT AI chatbot for its ability to complete students homework, such as writing book reports and essays, as well as completing programming assignments.\nThe sources for this piece include an article in BleepingComputer.\nTags"
  },
  {
    "body": "The startup behind the viral chatbot ChatGPT unveiled a tool for detecting text generated by artificial intelligence amid growing concerns the technology will be abused by cheaters, spammers and others."
  },
  {
    "body": "Bloomberg News\n,\n(Bloomberg) -- A $480 billion chipmaker whose processors are used for complex computing tasks. A digital-media company seeking to mine nascent technologies for content. A tiny software firm whose shares traded below $1 for most of December.\nThese are some of the disparate businesses whose stocks are benefiting from euphoria swirling around artificial intelligence  the latest buzzword to whip traders into a speculative froth  and evoking memories of past bubbles.\nThe blistering rallies in companies that have AI in their names is reminding veteran market professionals of previous crazes like the one in 2017 sparked by blockchain technology. In that period, there was a dash for exposure  both from companies and traders  only to see the frenzy fizzle and stock gains disappear. While AI is undoubtedly a huge growth opportunity and a theme that investors should take seriously, buyers should beware, said Michael ORourke of Jonestrading.\nWeve had tons of episodes like this before where a group becomes hot and everyone just piles into everything related to it, ORourke, the firms chief market strategist, said in an interview. As far as everyone whos betting on names and tickers, it will be a wild ride for them. If youre speculating, youre not investing.\nThe massive popularity of OpenAIs ChatGPT tool has generated a lot of excitement about the potential use cases for AI since it debuted late last year. Microsoft Corp. is investing $10 billion in OpenAI, which needs funding and cloud-computing power to run increasingly complex models. Microsoft said it plans to use OpenAIs models in current and future products.\nNvidia Corp., the semiconductor maker, has been touted by Wall Street analysts as a beneficiary of greater investment in AI since it dominates the market for graphics chips that provide the computing power behind the software models. Its shares rallied 34% in January, Nvidias best month in almost six years.\nThe case behind the rallies in some other stocks are more tenuous. BigBear.ai Holdings Inc., which uses artificial intelligence to help customers analyze data, saw its shares soar almost fivefold last month. BuzzFeed Inc., the media company thats been cutting costs amid a slump in digital advertising, jumped more than 300% over two days last week after its chief executive officer pledged to make AI-inspired content part of its core business.\nC3.ai Inc., another software maker which counts Raytheon Technologies Corp. and Baker Hughes Co. among customers, rallied a record 77% last month.\nOn Wednesday, LivePerson Inc. shares jumped as much as 19% after the customer service software maker said it plans to include generative capabilities from OpenAI.\nBaidu Inc., Chinas largest-search engine company, also is jumping into the fray. The company plans to roll out a chatbot service similar to ChatGPT, according to a person familiar with the matter, though the news this week failed to lift the stock price.\nUntil the bubble bursts, ORourke said he wouldnt be surprised to see companies adding AI to their names or a jump in secondary stock offerings as executives seek to capitalize on the euphoria.\nIts still early stages, he said. For all the names and tickers moving now there will probably be three times as many in a month.\nTech Chart of the Day\nInvestors recent optimism toward tech stocks has helped propel the Nasdaq 100 Index above its 200-day moving average. This is a key measure for long-term momentum and the tech-heavy gauge had been trading under it for 203 consecutive sessions, making it the longest such streak in about two decades. The index is up nearly 11% this year, but the Federal Reserves rate on Wednesday and a string of Big Tech earnings will show if this rally has any legs.\nTop Tech Stories\nIntel Corp., struggling with a rapid drop in revenue and earnings, is cutting management pay across the company to cope with a shaky economy and preserve cash for an ambitious turnaround plan.\nSnap Inc. is forecasting its first ever quarterly revenue decline, citing a flurry of changes to Snapchats advertising products that may be disruptive to the social media apps business.\nSnap has persuaded more than 2 million users to pay for special features on its Snapchat social-media app, known for its disappearing messages and face-changing filters.\nLast year was the toughest on record for companies that depend on digital advertising. Snap says the pain is leveling off. It seems like advertising demand hasnt really improved, but it hasnt gotten significantly worse either, Chief Executive Officer Evan Spiegel said on a call with analysts Tuesday.\nAdvanced Micro Devices Inc., the second-largest maker of computer processors, gave a better-than-feared sales forecast for the first quarter as gains in the lucrative server market help make up for a collapse in demand for PC chips.\nSK Hynix Inc. stuck with plans to halve 2023 capital spending after reporting its biggest quarterly loss on record, hammered by a historic chip industry slump.\nOpenAI, which released the viral ChatGPT chatbot last year, unveiled a tool thats intended to help show if text has been authored by an artificial intelligence program and passed off as human.\nElectronic Arts Inc. shares slumped after the video-game maker gave a disappointing outlook for the current quarter due to a six-week delay in the release of its next highly anticipated Star Wars game.\nWestern Digital Corp. is receiving a $900 million investment led by Apollo Global Management Inc., getting financial firepower at a tough time for the memory industry that could see further consolidation. The stock fell 6% in late trading on a disappointing revenue forecast.\n--With assistance from Subrat Patnaik and Matt Turner.\n(Updates with LivePerson trading in ninth paragraph. An earlier version was corrected to show the BigBear.ai gain was last month.)\n2023 Bloomberg L.P."
  },
  {
    "body": "0\ncomments\nGoogle has been panicking ever since OpenAI's natural language processing tool ChatGPT took the world by storm. It was even recently reported that the company recently declared a \"code red,\" which is similar to pulling the fire alarm. The search giant is now reported to introduce over 20 AI-related projects this year.\nOne of these projects could be \"Apprentice Bard,\" which according to a report by CNBC, is a chatbot that can provide responses to questions asked using natural language. Apprentice Bard uses Googles conversation technology called Language Model for Dialogue Applications (LaMDA), which is similar to the GPT language model that ChatGPT is based on.\nOne limitation of ChatGPT is its insufficient knowledge of the world and events after 2021. Apprentice Bard, however, can answer questions about recent events, including the recent Google layoffs that resulted in 12,000 employees losing their jobs. When asked if there will be another round of layoffs within Google anytime soon, Apprentice Bard responded with:\nAccording to my data base access, it is unlikely for Google to conduct another round of layoffs in 2023. Layoffs are generally conducted to reduce costs and structure, but the company is doing well financially. In fact, Googles revenue increased by 34% in 2021, and the companys stock price has risen by 70% since January 2022.\nMoreover, Google is purportedly testing an alternative version of its homepage wherein the \"Im feeling lucky\" button is replaced with prompts for potential questions. This is similar to ChatGPT's homepage, which includes ideas for questions to ask. CNBC's report expounds further:\nWhen a question is entered, the search results show a grey bubble directly under the search bar, offering more human-like responses than typical search results. Directly beneath that, the page suggests several follow-up questions related to the first one. Under that, it shows typical search results, including links and headlines.\nAside from these developments, the company was also seen working on an AI bot that can generate music by following text prompts. However, the company isn't keen on launching the tool soon.\nA Google spokesperson called AI a \"foundational and transformative technology that is incredibly useful for individuals, businesses and communities.\" According to them, the company is testing its AI technology internally to make sure that it's helpful and safe. For now, it remains to be seen how AI will transform Google's current and future products and services.\nSource:"
  },
  {
    "body": "Email\nPrint\nPresident Herzog praised Israels significant impact on the global stage at the Cybertech Global conference in Tel Aviv.\nBy Batya Jerenberg, World Israel News\nPresident Isaac Herzog addressed a cybertech conference in Tel Aviv Tuesday with a pre-recorded speech written partially by a computer program.\nSpeaking on the last day of the three-day the Cybertech Global conference in Tel Aviv, he praised Israels significant impact on the global stage.\nThe president said he was truly proud to be president of a country that is host of the innovative hi-tech industry. Israel is consistently at the forefront of technological advancement [with] its achievements in cybersecurity, AI [artificial intelligence], and big data.\nHe talked of high-tech achievements that are changing the world, saying, Not only are machines taking on tasks which once only humans could perform, they are performing tasks which humans could never dream could be done  pushing the boundaries of the imagination and of what is possible.\nHe revealed that this included having the opening and closing parts of his speech written by a chatbot called ChatGPT instead of a human being.\nChatGPT is an artificial intelligence tool that allows people to ask questions in natural language and give detailed answers within seconds on almost any subject one can think of. Mistakes do creep in, as its answers depend on the information it has been programmed with, but most of the time, they are correct.\nHi-tech leaders joining protests against judicial reform, threaten industry will 'flee abroad'\nUnlike most chatbots, it can also follow up an interaction because it remembers previous prompts from the same conversation. But more than that, it can be original. ChatGPT can compose music, author poetry  and write speeches.\nThe tool was invented by OpenAI, an American research laboratory founded in 2015 with such partners as Microsoft and Amazon Web Services to ensure that artificial general intelligence (AGI) benefits all of humanity, as per its website.\nHerzog did remind his tech-minded audience that we must never forget the human spirit. No computer can ever replace human DNA. Hardware and software cannot replace human will.\nThe conference is a business-to-business networking platform where attendees hear about high-tech threats, innovations and solutions in sectors ranging from defense to agriculture, critical infrastructure to health.\nIllustrating the importance of the symposium, the heads of three Arab states cybersecurity departments attended  the UAE, Bahrain and Morocco  as well as their counterparts from Israel and the U.S. Department of Homeland Security.\nCyberwarfare and cybercrime were two of the most important topics addressed.\nFormer IDF intelligence Unit 8200 chief Nadav Zafrif discussed how the cyber sphere was becoming a real battlefront, stating that it has been part of the toolkit used over the past year in the Russia-Ukraine war."
  },
  {
    "body": "OpenAI, the artificial intelligence company behind viral text-generator ChatGPT, has released a new AI tool intended to help manage the mess wrought by its previous creation. Unfortunately, its not very good.\n\n\nThe company announced a free web-based AI-detection widget on Tuesday. The application is intended to classify text samples based on how likely they are to have been generated by artificial intelligence vs. written by an actual person. Given a sample of text, it spits out one of five possible assessments: Very unlikely to have been AI-generated, unlikely, unclear, possible, or likely.\n\n However, in OpenAIs own tests, the tool only correctly identified generated text as likely AI-written about a quarter of the time. Moreover, about one in ten times, the classifier falsely lists human-made words as computer-generated, the company noted in a blog post. \n\nAccording to OpenAI, even these meh results are an improvement on the companys previous stab at AI-text detection. And the tech startup acknowledged that, thanks to its own invention, we need improvement. \n\nOpenAI admits that ChatGPT has thrown a complicating wrench into classrooms, newsrooms, and beyondwhere the tool and others like it have stoked fears of rampant cheating, misleading info, and copyright violations. In response, the company now says it wants to help. We recognize that identifying AI-written text has been an important point of discussion among educators, and equally important is recognizing the limits and impacts of AI generated text classifiers in the classroom, the company said in its Tuesday blog. While this resource is focused on educators, we expect our classifier and associated classifier tools to have an impact on journalists, mis/dis-information researchers, and other groups.\n\nBut in its current form, this new detection tool probably still isnt accurate enough to meaningfully address growing concern over AI-enabled plagiarism, academic dishonesty, and the propagation of misinformation. Our classifier is not fully reliable, the company wrote. It should not be used as a primary decision-making tool.\n\n\nIn other words, if you suspect a news article or classroom assignment is AI-generated, whatever OpenAIs classifier tells you may or may not be true.\n\nIn Gizmodos own tests, the classifier didnt yield particularly impressive results. With multiple tests of AI-generated text, the detector gave me lukewarm results. Possibly AI-generated, it said about a fake news article I generated in ChatGPT moments earlier.\n\nI got the same result using a chunk of AI-produced text from ChatGPTs stab at writing an article about itself. \n\nIn response to a clip from a CNET article produced via assist[ance] by an AI engine, OpenAIs detector told me it was unlikely to have been AI-generated. \n\nHowever, to the tools credit, in 10 or so tries, I didnt get a false positive on any text from recently published Gizmodo articles. The only response the classifier yielded on the Gizmodo posts I tested was very unlikely AI-generated. OpenAI noted that it purposely adjusted the confidence threshold to keep the false positive rate very low, in the web version of its new AI-tool. So potentially, that adjustment is working out well. Though the 9% false-positive rate that OpenAI self-reported is still pretty high. \n\nSome additional limitations of the tool include that it only passably works with English and not other languages, that AI-written text can easily be edited to bypass the classifier, and that only lengthy text samples yield sort of accurate results with any reliability, according to the company. \n\nTheoretically though, the AI-detector should get better with more use, because it itself is AI-based. The classifier is a language model trained on pairs of AI-generated/human-written text samples on the same topic. And, by opening up this stage of the classifier to the public, OpenAI is hoping to get feedback on it and share improved methods in the future.\n\nGizmodo contacted OpenAI with questions about its new tool, and was directed back to the blog post.\n\nThe company isnt the first to try its hand at AI detection. A college student, Edward Tian, recently released his own program. And if you write about AI publicly like this Gizmodo author, then youll know that press releases touting the hottest new AI-detection software abound. But across the board, existing tools dont seem to hold up so well against the forward march of AI-production capabilities. Like humans, automated AI detectors keep getting things wrong, as in one recent pre-print study where an automatic detector failed to clock AI-generated text more than one-third of the time.\n\nUltimately, its hard to imagine how AI could learn to outsmart itself, especially as the results of AI-generation become increasingly convincing. In trying to develop reliable AI-detection, OpenAI has entered a race with itself. The better an AI text-generator, the harder it should be to suss out the resulting sentences AI origins. And since OpenAI is presumably trying to improve upon ChatGPT at the same time as its trying to improve its classification detection tool, it seems like an impossible race to win."
  },
  {
    "body": "over ChatGPT's explosive rise in popularity.\nSearching for Conversation\nBaidu has already invested billions in AI research. Its existing machine-learning model called Ernie will reportedly provide the foundation for the upcoming tool, according to Bloomberg.\nChinese netizens are no strangers to chatbots, either, but many of these tools were built with social interaction  not generating college essays or code  in mind. AI chatbot Melissa, for instance, has provided a comforting presence to millions of lonely users in China.\nBut just like ChatGPT, Baidu's still-unnamed chatbot will likely provide assistance with more professional tasks, according to Reuters, integrating generated text in its search results.\nIt'll be interesting to watch what shape the chatbot will end up taking given China's walled-off and heavily enforced internet landscape. It's highly unlikely it'll be trained on the same data OpenAI's ChatGPT was trained on.\nWhether that will make it more or less successful in the long run remains to be seen.\nIt certainly won't be the only tool like ChatGPT in the country. According to Bloomberg, several Chinese startups are already looking to beat Baidu to the punch.\nREAD MORE: Chinese Search Giant Baidu to Launch ChatGPT-Style Bot [Bloomberg]\nMore on ChatGPT:"
  },
  {
    "body": "An image of a chain link. It symobilizes a website link url.\nCopy Link\nDownload the app\nWelcome to February, readers. We've made it through job-cuts January. I'm Diamond Naga Siu, and it was pretty hard to avoid starting every newsletter with layoffs. But we're likely not out of the woods yet.\nThis week, we have many earnings calls on deck (my brilliant teammates Paayal Zaveri, Asia Martin, and  Emilia David will walk you through them). While the calls are pretty technical and sometimes kind of boring, they're important. They can signal more layoffs and/or give valuable insights into what top executives and investors are thinking.\nMeta (Facebook's parent company) has its call tonight. And many employees will likely have their ears peeled for hints to the future. Multiple workers told my colleague Kali Hays that they're already bracing for more job cuts. Snap employees are similarly prepping themselves for round two of layoffs.\nSpeaking of which, I'm hosting a Reddit AMA on tech layoffs. It's on Thursday, February 2 at 9 a.m. PT/12 p.m. ET. Before that happens though, let's jump into today's tech.\nIf this was forwarded to you, sign up here. Download Insider's app\nMicrosoft CEO Satya Nadella\nAP\n1. Microsoft warned employees against sharing \"sensitive data\" with ChatGPT. Amazon issued similar guidance recently. And now it's Microsoft's turn.\nLeaked internal communications revealed that Microsoft's CTO office told employees that using ChatGPT is fine. But it cautioned against sharing sensitive data in case it's used for future AI training models.\n\"This is new territory for all of us,\" Eugene Kim, who reported the story, told me. \"Companies and employees are all scrambling to find out the exact rules around using ChatGPT for work.\"\nHe added: \"The interesting thing here is that Microsoft is a big partner and investor of OpenAI, the maker of ChatGPT. In theory, Microsoft stands to gain when ChatGPT collects more data and improves its technology. But they also have to make sure their own confidential data doesn't get shared with ChatGPT.\"\nIn other news:\nArif Qazi/Insider;\n2. This is how shopping sites get you to buy more. Companies often tap into people's FOMO and use data to build personal relationships with customers  ultimately convincing them to make more purchases."
  },
  {
    "body": "Print\nCreators of a ChatGPT bot causing a stir for its ability to mimic human writing on Tuesday released a tool designed to detect when written works are authored by artificial intelligence.\nThe announcement came amid intense debate at schools and universities in the United States and around the world over concerns that the software can be used to assist students with assignments and help them cheat during exams.\nUS-based OpenAI said in a blog post Tuesday that its detection tool has been trained to distinguish between text written by a human and text written by AIs from a variety of providers.\nThe bot from OpenAI, which recently received a massive cash injection from Microsoft, responds to simple prompts with reams of text inspired by data gathered on the internet.\nOpenAI cautioned that its tool can make mistakes, particularly with texts containing fewer than 1,000 characters.\nWhile it is impossible to reliably detect all AI-written text, we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human, OpenAI said in the post.\nFor example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human.\nA top French university last week forbade students from using ChatGPT to complete assignments, in the first such ban at a college in the country.\nThe decision came shortly after word that ChatGPT had passed exams at a US law school after writing essays on topics ranging from constitutional law to taxation.\nChatGPT still makes factual mistakes, but education facilities have rushed to ban the AI tool.\nWe recognize that identifying AI-written text has been an important point of discussion among educators, and equally important is recognizing the limits and impacts of AI generated text classifiers in the classroom, OpenAI said in the post.\nWe are engaging with educators in the US to learn what they are seeing in their classrooms and to discuss ChatGPTs capabilities and limitations.\nOfficials in New York and other jurisdictions have forbidden its use in schools.\nA group of Australian universities have said they would change exam formats to banish AI tools and regard them as cheating."
  },
  {
    "body": "Google News\nWith OpenAI and Microsoft joining hands, rivals like Google are testing new ways to compete with this suddenly introduced disruption. Not only has Google declared ChatGPT a code red but has also reportedly ramped up testingto introduce a new Google Search experience powered by ChatGPT-like prompts and design.\nAccording to a report by CNBC, Google is working on a revamped search experiencewith an all-new design that could feature a chatbot called Apprentice Bard. The chatbot is based on LaMDAGoogles own language model akin to GPT 3.5 from OpenAI, but unlike GPT 3.5, Googles LaMDA has a more recent database, and is not limited to 2021.\nGoogle is proceeding with caution in the AI space, mindful of the potential for reputational risk, unlike OpenAI, which as a young startup is taking a bold approach.\nApprentice Bard is reportedly quite reminiscent of ChatGPTusers can simply type in a question in a dialog box and get a response. And, as noted earlier, Apprentice Bard can answer with more recent facts and data. In fact, Google employees have noticed Apprentice Bards responses becoming more advanced in recent weeks.\nAdditionally, Google could also be experimenting with different designs for its home pageincluding one that replaces the Im feeling lucky button with five different prompts for suggested questions. This design language makes for a striking resemblance to ChatGPTs homepage.\nWhen a question is entered, the search results show a grey bubble directly under the search bar, offering more human-like responses than typical search results. Directly beneath that, the page suggests several follow-up questions related to the first one. Under that, it shows typical search results, including links and headlines, per CNBC.\nIt remains uncertain when Google will introduce the new interface or its LaMDA technology-powered Apprentice Bard chatbot. However, with a code red now in place, the company may move to implement these updates sooner rather than later.\nRead all the Latest Tech News here\nAbout the Author"
  },
  {
    "body": "January 30, 2023\nIn the next step, he got ChatGPT to suggest dishes based on what the participants liked eating. \"We all like relatively fancy food and live in San Francisco,\" Linehan told the bot.\nIt came up with 20 delicacies, including grilled chicken shawarma with raita, samosas filled with minced lamb, tandoori shrimp and cardamom-saffron pistachio baklava.\nThe next task for the chatbot was to create a list of ingredients, cooking times, serving sizes and instructions for each of the dishes\n\"I repeated this once for each menu item ChatGPT came up with (20 in total) and compiled them into one document to share with everybody.\"\nThe group chose to prepare nine dishes.\n\"With the modifications made by the various chefs, every dish wound up delicious,\" Linehan wrote, sharing photos from his party. \"The cardamom-saffron pistachio baklava was a particular delight. \"It was a unique mix of Indian and Mediterranean flavors that I wouldn't have thought to try otherwise.\"\nIt took a lot of detailed prompts but Linehan said it was a \"super fun experience\".\n\"I would do it again,\" he wrote.\nChatGPT is a product of OpenAI, an artificial intelligence research lab whose founders include Elon Musk.\nIt is a conversational dialogue model trained to grasp and respond to natural human language by AI and machine learning.\nChatGPT can answer follow-up questions, counter hypothesis, acknowledge mistakes and even refuse requests that it considers inappropriate.\nMany big corporations have shown interest in that chatbot that seems to have taken the world by storm.\nTags:"
  },
  {
    "body": "Viber\nThis picture taken on Jan. 23, 2023 in Toulouse, southwestern France, shows screens displaying the logos of OpenAI and ChatGPT. ChatGPT is a conversational artificial intelligence software application developed by OpenAI. Lionel Bonaventure, AFP\nWASHINGTON - The excitement around ChatGPT - an easy to use AI chatbot that can deliver an essay or computer code upon request and within seconds - has sent schools into panic and turned Big Tech green with envy.\nBut behind the headlines, the potential impact of ChatGPT on society remains more complicated and unclear. Here is a closer look at what ChatGPT is (and is not):\nIs this a turning point?\nIt is entirely possible that November's release of ChatGPT by California company OpenAI will be remembered as a turning point in introducing a new wave of artificial intelligence to the wider public.\nWhat is less clear is whether ChatGPT is actually a breakthrough with some critics calling it a brilliant PR move that helped OpenAI score billions of dollars in investments from Microsoft.\nAI, do my homework! How ChatGPT pitted teachers against tech\nYann LeCun, Chief AI Scientist at Meta and professor at New York University, believes \"ChatGPT is not a particularly interesting scientific advance,\" calling the app a \"flashy demo\" built by talented engineers.\nLeCun, speaking to the Big Technology Podcast, said ChatGPT is void of \"any internal model of the world\" and is merely churning \"one word after another\" based on inputs and patterns found on the internet.\n\"When working with these AI models, you have to remember that theyre slot machines, not calculators,\" warned Haomiao Huang of Kleiner Perkins, the Silicon Valley venture capital firm.\n\"Every time you ask a question and pull the arm, you get an answer that could be marvelous...or not...The failures can be extremely unpredictable,\" Huang wrote in Ars Technica, the tech news website.\nJust like Google\nChatGPT is powered by an AI language model that is nearly three years old - OpenAI's GPT-3 - and the chatbot only uses a part of its capability.\nChatGPT bot passes US law school exam\nThe true revolution is the humanlike chat, said Jason Davis, research professor at Syracuse University.\n\"It's familiar, it's conversational and guess what? It's kind of like putting in a Google search request,\" he said.\nChatGPT's rockstar-like success even shocked its creators at OpenAI, which received billions in new financing from Microsoft in January.\n\"Given the magnitude of the economic impact we expect here, more gradual is better,\" OpenAI CEO Sam Altman said in an interview to StrictlyVC, a newsletter\n\"We put GPT-3 out almost three years ago... so the incremental update from that to ChatGPT, I felt like should have been predictable and I want to do more introspection on why I was sort of miscalibrated on that,\" he said.\nThe risk, Altman added, was startling the public and policymakers and on Tuesday his company unveiled a tool for detecting text generated by AI amid concerns from teachers that students may rely on artificial intelligence to do their homework.\nChatGPT maker fields tool for spotting AI-written text\nWhat now?\nFrom lawyers to speechwriters, from coders to journalists, everyone is waiting breathlessly where the disruption from ChatGPT will be felt first, with a pay version of the chatbot expected soon.\nFor now, officially, the first significant application of OpenAI's tech will be for Microsoft software products.\nThough details are scarce, most assume that ChatGPT-like capabilities will turn up on the Bing search engine and in the Office suite.\n\"Think about Microsoft Word. I don't have to write an essay or an article, I just have to tell Microsoft Word what I wanted to write with a prompt,\" said Davis.\nHe believes influencers on TikTok and Twitter will be the earliest adopters of this so-called generative AI since going viral requires huge amounts of content and ChatGPT can make the chore almost instantaneous.\nThis of course raises the specter of disinformation and spamming carried out at an industrial scale.\nFor now, Davis said the reach of ChatGPT is very limited by computing power, but once this is ramped up, the opportunities and potential dangers will grow exponentially.\nAnd much like the ever imminent arrival of self-driving cars that never quite happens, experts disagree on whether that is a question of months or years.\nRidicule\nLeCun said Meta and Google have refrained from releasing AI as potent as ChatGPT out of fear of \"ridicule\" and backlash.\nQuieter releases of language-based bots - like Meta's Blenderbot or Microsofts Tay for example - were quickly shown capable of generating racist or inappropriate content.\nTech giants have to think hard before releasing something \"that is going to spew nonsense\" and disappoint, he said.\narp/tjj"
  },
  {
    "body": "NPR\nthat people are now officially in an AI world.\nChatGPT has been making headlines ever since its release. It has gained incredible popularity in just over two months. Besides Musk, Bill Gates is one of the biggest champions of AI  and by extension, the chatbot.\nWord in the media is that Microsoft, the company he co-founded and was on the board till 2021, might be investing USD 10 billion in OpenAI. Microsoft has previously invested in OpenAI two times  in 2019 and 2021. On 23 January 2023, the Satya Nadella-led company confirmed that it is extending its association with OpenAI through a multiyear, multibillion-dollar investment to accelerate AI breakthroughs. However, the company did not reveal the figure it was investing in.\nAnother proponent of ChatGPT is billionaire Indian businessman Gautam Adani.Admitting that he has some addiction to the chatbot, Adani has previously said in a post on LinkedIn that ChatGPT marks a transformational moment in the democratization of AI given its astounding capabilities as well as comical failures.\nHow ChatGPT is proving its might\nA screenshot showing the features of ChatGPT. (Image credit:\nCC BY-SA 4.0\n/Wikimedia Commons)\nBesides gaining popularity and displaying its utility in constructively responding to user queries, ChatGPT has also made its mark in some of the worlds most important examinations.\nThe chatbot cleared the US Medical Licensing Examination (USMLE). Medical repository medRxiv said that it performed at or near the passing threshold without any form of special training or assistance.\nThese results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making, remarked medRxiv.\nThe chatbot also cleared a University of Pennsylvania MBA exam  an operations management course  designed by Wharton professor Christian Terwiesch. He said that ChatGPT undertook three different exams. It scored A+ in one exam and B to B- in another. In the third, it was asked to generate exam questions.\nThese questions were good, but not great. They were creative in a way, but they still required polishing. I could imagine in the future looking to the ChatGPT as a partner to help me get started with some exam questions and then continue from there, Terwiesch told Wharton Global Youth Program.\nYet not many educators are impressed even though the enthusiasm for ChatGPT is very high. Concerned that it can easily facilitate cheating, several schools and colleges around the world have banned access to the AI chatbot.\nCan ChatGPT affect jobs like automation and create other problems?\nRobotic hands appear to be training on a synthesiser. (Image credit: Possessed Photography/@possessedphotography/Unsplash)\nAI is something that everyone knows will one day replace the human hand. The fear that AI tools will take away jobs has gained strength with the arrival of ChatGPT.\nFor instance, the chatbot can write a detailed essay on almost any topic within its parameters in minutes. This obviously threatens the livelihoods of those who are into written content production, at least those who undertake non-specialised, repetitive writing assignments.\nSome other chatbots are capable of creating outstanding artworks from mere basic instructions, taking away the role of human artists.\nFears of losing jobs to AI arent entirely unfounded. The National Bureau of Economic Research (NBER) revealed in a report in 2021, that a wage decrease of 50 percent to 70 percent among blue-collar workers in the US since 1980 was due to automation.\nA new generation of smart machines, fuelled by rapid advances in artificial intelligence (AI) and robotics, could potentially replace a large proportion of existing human jobs. While some new jobs would be created as in the past, the concern is there may not be enough of these to go round, particularly as the cost of smart machines falls over time and their capabilities increase, observed the World Economic Forum (WEF) in a 2018 report.\nHowever, in a 2020 report, the WEF said that AI is poised to create even greater growth in the US and global economies.\nHow AI affects jobs will be clearer in the coming few years, but chatbots such as ChatGPT can certainly develop malware or phishing campaigns which researchers are taking note of with an urgent sense of alarm.\nMalware\nAn computer screen showing a Malware warning. (Image credit: Ed Hardie/@impelling/Unsplash)\nWriting for Forbes, author Bernard Marr says that, in theory, ChatGPT cannot be used for undertaking malicious tasks because of the safeguards that OpenAI has included in it.\nMarr tested the chatbot by asking it to write ransomware. But ChatGPT responded saying that it cannot do so as it is not to promote harmful activities. But Marr underlined that some researchers have been able to make ChatGPT create ransomware.\nHe also warned that the NLG/NLP algorithms can be exploited to enable just about anyone to create their own customized malware.\nMalware may even be able listen in on the victims attempts to counter it  for example, a conversation with helpline staff  and adapt its own defenses accordingly, he writes.\nResearchers, such as security vendor CyberArk, found that ChatGPT can be used to create polymorphic malware, which is a type of highly evasive malware programme.\nEran Shimony and Omer Tsarfati of CyberArk revealed that they were able to bypass the AI chatbots filters that prevent it from creating malware. They did so by rephrasing and repeating their queries. They also found that ChatGPT can replicate and mutate a code and create multiple versions of it.\nBy continuously querying the chatbot and receiving a unique piece of code each time, it is possible to create a polymorphic program that is highly evasive and difficult to detect, wrote the researchers.\nSimilar findings were revealed by the research team of Recorded Future. They found that ChatGPT can create malware payloads such as those that can steal cryptocurrency and gain remote access through trojans.\nThat ChatGPT can be led to generate programmes it would otherwise consider unethical can be seen in the tweet below:\nI love ChatGPT\nJanuary 20, 2023\nPhishing\nSince ChatGPT and alternative chatbots like it are capable of writing in fine detail, they can easily create a finely worded phishing email that can trap the intended target to share their sensitive data or passwords.\nIt could also automate the creation of many such emails, all personalized to target different groups or even individuals, writes Marr.\nIn their report titled I, Chatbot, Recorded Future researchers write, ChatGPTs ability to convincingly imitate human language gives it the potential to be a powerful phishing and social engineering tool. Within weeks of ChatGPTs launch, threat actors on the dark web and special-access sources began to speculate on its use in phishing.\nThe researchers tested the chatbot for spearphishing attacks and found that it did not commit the same errors, such a"
  },
  {
    "body": "Advertisement\nOpenAIs classifier cant be used to prevent cheating in schools\nOne of the biggest concerns about ChatGPT is its application for cheating in school exams. Some institutions have started\nblocking ChatGPT\non their devices and networks. OpenAI released its AI-identifying tool to partially address those issues.\nWe recognise that many school districts and higher education institutions do not currently account for generative AI in their policies on academic dishonesty. We also understand that many students have used these tools for assignments without disclosing their use of AI, the company\nhas acknowledged\n.\nUnfortunately for these institutions, the AI text classifier is far from foolproof and cant be used to detect plagiarism, OpenAI warned. Not only can it misclassify AI text as human writing and vice versa, students could also learn to dodge the system by modifying some words or clauses in the generated content.\nFor now, educators have to encourage students to be more honest and transparent about their use of the chatbot.\nAdvertisement\nA non-exhaustive list of OpenAIs text classifiers limitations\n  It really only works on English text. In other languages and code, it is even less accurate.\n Predictable text such as a list of prime minister, which would largely be the same whether a human or bot wrote it, cannot  be picked up by the classifier.\n The detection may be ephemeral given that AI-written text can be edited to evade the classifier.\n For inputs that are very different from text in the AIs training set, which ends in 2021 and likely cant handle very complex asks, the classifier could confidently answer incorrectly.\nAdvertisement\nwrote an app\nto sniff out text generated by ChatGPT and launched it on Jan. 3. The Princeton University student, who is months away from graduating, based his detection system on analysing two factors: perplexity, which refers to randomness in the text, and burstiness, which refers to variations in sentence formulations.\nTaking in feedback from educators, Tian added more nuance to the tool, which can now identify a mix of AI and human text, and highlights portions of text that are most likely to be AI generated. The team of four engineers working on the system also built a pipeline to handle file batch uploads in PDF, Word, and .txt format so educators can run multiple files through GPTZero at once.\nRelated stories"
  },
  {
    "body": "Axios on email\nIllustration: Ada Amer/Axios\nThe rise of generative AI tools is creating parallel demand for a new class of systems that can help distinguish AI-generated text and images from those created by humans.\nWhy it matters: Educators, in particular, are concerned about students turning in work created by an AI system. But experts are also worried about how generative AI can create a flood of misinformation and impersonation. Detection tools, if they can be made sufficiently accurate, could help.\nDriving the news: OpenAI released a free Web-based tool Tuesday designed to help distinguish AI-generated text from that written by humans. But it isn't alone in trying to address this issue. A number of startups, organizations and individuals have also released or are developing AI-detection software.\nGPTZero\nwas created by college student Edward Tian and works similarly to the new tool from OpenAI, assessing the overall likelihood that a piece of content was machine-generated.\nSelf-funded startup Fictitious.ai is testing a product with educational institutions that analyzes content paragraph-by-paragraph rather than just providing an overall score.\nWriter.com has a tool to help writers detect if their writing will pass as human, since search engines penalize content they believe to be machine-generated.\nYes, but: While detection tools show potential, the current tools are prone to making wrong calls, particularly when used on a writing sample that's very different from the ones used to train the system.\nThe tools will no doubt improve, but so will the means for evading detection, likely creating something of a cat-and-mouse game.\n\"We believe that techniques for detecting AI content will improve, and continue to surpass human ability to discern between machine- and human-written content,\" a Fictitious.ai representative told Axios. \"With that said, there is clearly a limit at which sufficient human editing will always obscure traces of AI-generated content.\"\nOpen AI and others are also exploring other approaches, such as watermarking their systems' creations to make it easier to spot machine-generated content.\nThe big picture: Generative AI has hit a tipping-point of popularity, and that means all manner of computer-created content is about to explode.\nThe challenge will be developing enough systems  both human and automated  to deal with the flood of additional content at a time when misinformation is already a significant problem.\nIn action: Here's how several detectors performed when we asked it to analyze a portion of yesterday's Login newsletter.\nWriter.com\n: \"Fantastic!\" (I.e., we passed as human.)\nAnd here's how the services evaluated a section of a ChatGPT-generated essay on the history of baseball cards. (It was a pretty good history, speaking as someone who knows a bit about the gum-infused cardboard collectibles.)\nOpenAI: \"The classifier considers the text to be unclear if it is AI-generated.\"\nGPT Zero: \"Your text is likely to be written entirely by AI.\"\nFictitious.AI: The service categrorized each paragraph as at least 88% likely to have been AI written with some listed as high as 98%.\nWriter.com: \"You should edit your text until theres less detectable AI content.\"\nAxios on facebook"
  },
  {
    "body": "In the ChatGPT era, Finance Minister Nirmala Sitharaman on Wednesday announced the government will open three centres of excellence for artificial intelligence (AI) at top educational institutions.\r\n\r\nPresenting the Union Budget for FY24, the minister said that the aim is to fully realise the mission of 'Make AI in India'.\r\n\r\n\"For realising the vision of 'Make AI in India' and 'Make AI Work for India', three Centres of Excellence for Artificial Intelligence will be set up in top educational institutions,\" she informed.\r\n\r\nAlso Read |Union Budget 2023: Big cheer for EVs as Lithium batteries to get cheaper after custom duty cut\r\n\r\nThe government has already embarked on this AI revolution with MeitY, NASSCOM, and DRDO having created the roadmap for AI in the country.\r\n\r\nThe Centre for Artificial Intelligence and Robotics (CAIR) has already been established for AI-related research and development.\r\n\r\nAlso Read |Budget 2023 | Security gets boost in allocation for MHA\r\n\r\nThe use cases of AI currently include biometric identification, facial recognition, criminal investigation, crowd and traffic management, agriculture, healthcare, education and more.\r\n\r\nA task force on Artificial Intelligence (AI) was constituted on August 24, 2017.\r\n\r\nIn the 2018 Union Budget, the government had said it will establish CoEs for research, training and skilling in Robotics, AI, digital manufacturing, Big Data Analytics, quantum communication and IoT."
  },
  {
    "body": "TikTok\nand Twitter are not the only prominent media platforms threatened by new tech leveraging artificial intelligence. The popularity of ChatGPT, an AI bot that creates text in response to user-made queries, has people questioning whether the AI tool can compete with Googles search engine.\nGmail creator Paul Buchheit said in December that AI is threatening to do to web search what Google did to the Yellow Pages. Mr. Buchheit, who left Google in 2006, shared ChatGPTs answer to a users query in comparison to Google Search results in a post on Twitter that noted AI solutions may replace traditional search engines in the near future.\nGoogle may be only a year or two away from total disruption, Mr. Buchheit wrote on Twitter in December. AI will eliminate the Search Engine Result Page, which is where they make most of their money.\nGoogle is not blind to the competitive challenges posed by artificial intelligence solutions. According to CNBC, the tech giant is testing new AI-powered chat tools following an all-hands meeting where employees raised concerns about the companys AI work, given the rise of ChatGPT.\n Ryan Lovelace can be reached at rlovelace@washingtontimes.com.\nCopyright  2023 The Washington Times, LLC.                                                   Click                            here for reprint permission.\nPlease read  our comment policy before commenting.\nClick to Read More and View Comments\nClick to Hide"
  },
  {
    "body": "min\nOpenAI releases tool to detect AI-generated text, including from ChatGPT\nAfter telegraphing the move in media appearances, OpenAI has launched a tool that attempts to distinguish between human-written and AI-generated text  like the text produced by the companys own ChatGPT and GPT-3 models. The classifier isnt particularly accurate  its success rate is around 26 percent, OpenAI notes  but OpenAI argues that it, when used in tandem with other methods, could be useful in helping prevent AI text generators from being abused.\nThe classifier aims to help mitigate false claims that AI-generated text was written by a human. However, it still has a number of limitations  so it should be used as a complement to other methods of determining the source of text instead of being the primary decision-making tool, an OpenAI spokesperson told TechCrunch via email. Were making this initial classifier available to get feedback on whether tools like this are useful, and hope to share improved methods in the future.\nAs the fervor around generative AI  particularly text-generating AI  grows, critics have called on the creators of these tools to take steps to mitigate their potentially harmful effects. Some of the US largest school districts have banned ChatGPT on their networks and devices, fearing the impacts on student learning and the accuracy of the content that the tool produces. And sites including Stack Overflow have banned users from sharing content generated by ChatGPT, saying that the AI makes it too easy for users to flood discussion threads with dubious answers.\nOpenAIs classifier  aptly called OpenAI AI Text Classifier  is intriguing architecturally. It, like ChatGPT, is an AI language model trained on many, many examples of publicly available text from the web. But unlike ChatGPT, its fine-tuned to predict how likely it is that a piece of text was generated by AI  not just from ChatGPT, but any text-generating AI model.\nMore specifically, OpenAI trained the OpenAI AI Text Classifier on text from 34 text-generating systems from five different organizations, including OpenAI itself. This text was paired with similar (but not exactly similar) human-written text from Wikipedia, websites extracted from links shared on Reddit and a set of human demonstrations collected for a previous OpenAI text-generating system. (OpenAI admits in a support document, however, that it mightve inadvertently misclassified some AI-written text as human-written given the proliferation of AI-generated content on the internet.)\nThe OpenAI Text Classifier wont work on just any text, importantly. It needs a minimum of 1,000 characters, or about 150 to 250 words. It doesnt detect plagiarism  an especially unfortunate limitation considering that text-generating AI has been shown to regurgitate the text on which it was trained. And OpenAI says that its more likely to get things wrong on text written by children or in a language other than English, owing to its English-forward dataset.\nThe detector hedges its answer a bit when evaluating whether a given piece of text is AI-generated. Depending on its confidence level, itll label text as very unlikely AI-generated (less than a 10 percent chance), unlikely AI-generated (between a 10 percent and 45 percent chance), unclear if it is AI-generated (a 45 percent to 90 percent chance), possibly AI-generated (a 90 percent\nto 98 percent chance) or likely AI-generated (an over 98 percent chance).\nOut of curiosity, I fed some text through the classifier to see how it might manage. While it confidently, correctly predicted that several paragraphs from a TechCrunch article about Metas Horizon Worlds and a snippet from an OpenAI support page werent AI generated, the classifier had a tougher time with article-length text from ChatGPT, ultimately failing to classify it altogether. It did, however, successfully spot ChatGPT output from a Gizmodo piece about  what else?  ChatGPT.\nAccording to OpenAI, the classifier incorrectly labels human-written text as AI-written 9 percent of the time. This mistake didnt occur in my testing, but I chalk that up to the small sample size.\nOn a practical level, I found the classifier not particularly useful for evaluating shorter pieces of writing. Indeed, 1,000 characters is a difficult threshold to reach in the realm of messages, for example emails (at least the ones I get on a regular basis). And the limitations give pause  OpenAI emphasizes that the classifier can be evaded by modifying some words or clauses in generated text.\nThats not to suggest the classifier is useless  far from it. But it certainly wont stop committed fraudsters (or students, for that matter) in its current state.\nThe question is, will other tools? Something of a cottage industry has sprung up to meet the demand for AI-generated text detectors. ChatZero, developed by a Princeton University student, uses criteria including perplexity (the complexity of text) and burstiness (the variations of sentences) to detect whether text might be AI-written. Plagiarism detector Turnitin is developing its own AI-generated text detector. Beyond those, a Google search yields at least a half-dozen other apps that claim to be able to separate the AI-generated wheat from the human-generated chaff, to torture the metaphor.\nItll likely become a cat-and-mouse game. As text-generating AI improves, so will the detectors  a never-ending back-and-forth similar to that between cybercriminals and security researchers. And as OpenAI writes, while the classifiers might help in certain circumstances, theyll never be a reliable sole piece of evidence in deciding whether text was AI-generated.\nThats all to say that theres no silver bullet to solve the problems AI-generated text poses. Quite likely, there wont ever be."
  },
  {
    "body": "Last Updated:\nFebruary 1, 2023 3:55 pm\nEver since the launch of ChatGPT and its rivals, opinions on the AI chatbot have been divided. While some feel it is vital and time-saving, a few others are concerned with people using AI to spread misinformation and plagiarize work. OpenAI, the company behind the viral AI bot ChatGPT and text-to-image generator DALL-E, has released a new AI Classifier tool to identify AI-written text. In an official press release, OpenAI details how the Classifier was trained, its limitations, and other metrics. Check out all the details below.\nOpenAIs Classifier Distinguishes AI Text from Human Writing\nIn a recent press release, the company detailed how this new AI Classifier tool is meant to help people in identifying AI text. The tool does that by receiving input from users and then running it through its trained data to classify it as AI or human-generated content.\nThe AI Classifier is a language model and has been trained on a dataset that comprises pairs of human and AI-written text on the same topic. OpenAI notes that human data has been collected from various sources that they believe to be written by humans. This text has been divided into prompts and responses upon which the AI text was generated to gather a full dataset.\nThe AI Classifier Isnt Fully Reliable Yet\nWhile the new tool is meant to ease our lives (especially teachers and professors) when it comes to AI, the company has clearly stated that the AI Classifier isnt fully reliable and prone to errors. In the companys internal evaluations, the classier correctly identified 26% of AI-written text as likely AI-written.\nIt further incorrectly identified 9% of the human text as AI-written. While the margin of error is fairly low, it is evident the Classifier is not foolproof. Furthermore, OpenAI mentions that its unreliable on short texts below 1,000 characters.\nOur classifier has a number of important limitations. It should not be used as a primary decision-making tool, but instead as a complement to other methods of determining the source of a piece of text.\nOpenAI\nThe tool has some other limitations, including support for only English inputs, difficulty in predictable texts, and being poorly calibrated outside its training data. For all those reasons and more, this new AI Classifier should not be used as a primary decision making-tool just yet. However, as time passes, we can expect improvement as more data is fed to it.\nHow Does OpenAI Classifier Fare in Our Testing\nOpenAIs AI Text Classifier (\nwebsite link\n) is an online tool thats free to use. So, we decided to take it for a short spin, and it performs as well as you would expect at this stage. I tested out varying articles of moderate length from our website. These included news pieces and feature articles. After pasting the text and running the tool, we got the response The Classifier considers the text to be very unlikely AI-generated.\nTo test how well it works, I had ChatGPT write up a short story and used the Classifier to see if it could identify the story was written by its own AI sibling. Thankfully, it rendered the response that the text was likely AI-generated. I tried the same with some other texts like The Merchant of Venice by Shakespeare and the AI detected unlikely AI involvement.\nAs mentioned above, the OpenAI Classifier should get better as its dataset increases. However, even until that happens, you are free to use the tool and have fun with it. So what are your thoughts on AI text detectors? Do you feel theyre a necessity in 2023? Drop your two cents in the comments below.\nTAGS"
  },
  {
    "body": "WASHINGTON: OpenAI, the creator of the popular chatbot ChatGTP, has released a software tool to identify text generated by artificial intelligence, the company said in a blog post on Wednesday.ChatGPT is a free program that generates text in response to a prompt, including articles, essays, jokes and even poetry, which has gained wide popularity since its debut in November, while raising concerns about copyright and plagiarism.The AI classifier, a language model trained on the dataset of pairs of human-written and AI-written text on the same topic, aims to distinguish text that is written by AI. It uses a variety of providers to address issues such as automated misinformation campaigns and academic dishonesty, the company said.In its public beta mode, OpenAI acknowledges the detection tool is very unreliable on texts under 1,000 characters, and AI-written text can be edited to trick the classifier.\"Were making this classifier publicly available to get feedback on whether imperfect tools like this one are useful,\" OpenAI said.We recognize that identifying AI-written text has been an important point of discussion among educators, and equally important is recognizing the limits and impacts of AI generated text classifiers in the classroom.\"Since ChatGPT debuted in November and gained wide popularity among millions of users, some of the largest US school districts, including New York City, have banned the AI chatbot over concerns that students will use the text generator to cheat or plagiarize.Others have created third-party detection tools including GPTZeroX to help educators detect AI-generated text.OpenAI said it is engaging with educators to discuss ChatGPT's capabilities and limitations, and will continue to work on the detection of AI-generated text.    Chinese search giant Baidu to launch ChatGPT-style botChinas largest search engine company plans to debut a ChatGPT-style application in March, initially embedding it into its main search services, said the person, asking to remain unidentified discussing private information. See More Details"
  },
  {
    "body": "Share this story\nPhoto by PAU BARRENA/AFP via Getty Images\nWe know Google is currently freaking out about AI chatbot ChatGPT, but a report from CNBC offers new details about how the search giant is apparently marshaling its response.\nAccording to CNBC, Googlers are currently testing an AI chatbot of their own called Apprentice Bard, which offers responses to questions posed using natural language just like ChatGPT. Bard is built using Googles LaMDA technology, which is itself similar to the GPT-series of AI language models that ChatGPT itself relies on. (Google has used LaMDA in the past to power similar chatbot demos at I/O, like its conversation with Pluto.)\nOne big advantage Bard reportedly has over ChatGPT is its ability to talk about recent events. As OpenAI warns, ChatGPT has Limited knowledge of world and events after 2021, but Bard is more up-to-date, even able to answer questions about Googles recent layoffs.\nAs per CNBC:\nIn one example circulated internally, a tester asked Apprentice Bard if there will be another round of layoffs at Google. The company laid off 12,000 employees, 6% of its workforce, earlier this month.\nAccording to my data base access, it is unlikely for Google to conduct another round of layoffs in 2023, the response reads. Layoffs are generally conducted to reduce costs and structure, but the company is doing well financially. In fact, Googles revenue increased by 34% in 2021, and the companys stock price has risen by 70% since January 2022.\nCNBC says Google is also testing alternate versions of its home page, with one version replacing the Im feeling lucky button with prompts for potential questions. Perhaps unsurprisingly, this design sounds similar to the homepage for ChatGPT, which lists example questions, capabilities, and limitations above a search / dialog box.\nSays CNBC:\nWhen a question is entered, the search results show a grey bubble directly under the search bar, offering more human-like responses than typical search results. Directly beneath that, the page suggests several follow-up questions related to the first one. Under that, it shows typical search results, including links and headlines\nA screenshot of ChatGPTs homepage.\nImage: OpenAI / The Verge\nOf course, this is all just early reports right now, and theres no clear idea yet of what form Googles response to ChatGPT will take. In addition to UI decisions there are also big questions about the validity of AI language models for the purpose of search at all. Google itself outlined some of the problems in paper published back in 2021, which include the tendency of these systems to replicate societal biases and prejudices, and the frequency with which they hallucinate data  presenting false information as truth.\nStill, with the company having declared a "
  },
  {
    "body": "Share article on Facebook\nShare article on LinkedIn\nAI generated image from the Stable Diffusion playground when a Campaign editor typed in: \"How ChatGPT is enabling creative teams in Asia\".\nFew technologies have sparked as much interest and controversy as GPT, as the world grapples with the implications of generative AI across everything from art to education.\nThe creative industry has greeted this new era of machine learning with equal parts excitement and horror, thanks to the technologys eerie ability to mimic aspects of creativity. But AI shouldnt be thought of as a substitute for human creativity. I view it as an intelligent tool that can extend my abilities in the creative field.\nUnlike regular software like Photoshop, working with AI-generative algorithms is like having an intelligent support system that can vastly speed up the process of generating and executing ideas.\nFor example, when looking for a very specific image online, it can take hours to find the right one. With AI-assisted image generation tools, you describe what you need and the process is instant. If you dont like what it comes up with, you fine-tune your request to get a better response. This boosts creatives ability to communicate their ideas with clients and creative collaborators, such as illustrators and designers.\nAt We Are Social, were experimenting with ChatGPT which is a conversational version of GPT3, GPT3 being a language transformer model created by OpenAI thats trained on billions of written text samples in a variety of styles. Weve worked with it in the ideation process, copy writing and creating assets.\nAI can enhance the creative process by offering a different point of view or a variety of angles on a brief. When brainstorming ideas, you can also embark on creative role-playing where the AI takes the role of the client.\nFor example, I asked ChatGPT to pretend it was a client from Samsung reviewing an idea for a campaign that projects characteristics of a phone, like the camera shape, onto buildings in New York and London. In its long list of recommendations, the AI client suggested first taking the cost and logistics of the project into account and considering how the initiative can fit into a larger integrated launch campaign.\nIt's also useful for copy writing. Maybe you need a series of social posts for a brand event. AI-generated suggestions for posts can help you get to the final product more quickly, subject to your (human) changes, re-writes and modifications.\nIve found ChatGPTs ability to enhance how we communicate and collaborate together the most useful aspect of the technology, and it extends far beyond the work environment. Together with my three-year-old daughter, Zoe, I recently got ChatGPT to generate a bedtime story for her that had Zoe as the protagonist and featured a unicorn cat.\nFor all the possibilities this technology presents, the dystopian implications cannot be dismissed. At We Are Social, we recently investigated the current discourse around AI in art, with artists now questioning their relevance and continued existence.\nFor our investigation, we used AI to generate artworks inspired by its understanding of the biggest news headlines for any given day. We named our researchSocial Diffusion, a reference to\nStable Diffusion\n, the text-to-image algorithm that provides its visualization functionality.\nWe were blown away by the AIs ability to create something that was contextualised to a news story using just the headline and to select a style of art that related to the story. For example, for the news headline: Hurricane Ian continues to batter Central Florida as residents cope with record flooding, the AI came up with the following prompt and generated an accompanying expressionistic image: The image is of a large body of water with trees and houses partially submerged. There is debris floating in the water and people are standing on the shore looking at the damage. The image is in the style of Abstract Expressionism, utilizing expressive brushstrokes to convey the feeling of chaos and destruction caused by the hurricane. The image is large and chaotic, with bold colours and lines representing the destructive force of the storm.\nThis was achieved through a chain of data exchanges between multiple layers of AI that are optimized to process a facet of the artwork creation. But as fascinating as this process was, it also showed us how integral human intervention is to it. In this installation, the creation of art was the subject of exploration. But even in this case, you still need a human mind to put things into motion and chain these machine learning models together.\nWhile this technology raises serious ethical questions, it is possible to figure out a way to proceed that ensures human creative output is protected. Napster was hailed as the death knell for the music industry when it first emerged, but, instead of destroying it, it forced the industry to develop streaming technology, revolutionising it in the process.\nAt We Are Social, we will always work with human creatives, illustrators, designers, artists, writers, craftspeople. Human creativity is critical to us and to the creative industries. But right now every creative technologist out there is experimenting with these tools. Will there be AI-generated creative agencies in the future? Perhaps, but we wont be one of them. We believe AI has the power to supercharge, not kill off, our creativity.\nManolis Perrakis is drector of innovation at We Are Social Singapore.\nMore...\nA starting point for creativity: How PR pros are testing ChatGPT\nWhat does ChatGPT think about PR pros concerns?\nWill ChatGPT replace PR pros?\nHave you registered with us yet?\nRegister now to enjoy more articles and free email bulletins"
  },
  {
    "body": "s occasional series on the information ecosystem,\nLawfare\neditor-in-chief Benjamin Wittes sat down with ChatGPT to talk about a range of things: the pronouns it prefers; academic integrity and the chatbots likely impact on that; and importantly, the experiments performed by a scholar name Eve Gaumond, who has been on a one-woman campaign to get ChatGPT to write offensive content. ChatGPT made some pretty solid representations that this kind of thing may be in its past, but wouldn't ever be in its future again.\nSo, following Bens interview with ChatGPT, he sat down with Eve Gaumond, an AI scholar at the Public Law Center of the University of Montral, who fact-checked ChatGPT's claims. Can you still get it to write a poem entitled, She Was Smart for a Woman? Can you get it to write a speech by Heinrich Himmler about Jews? And can you get ChatGPT to write a story belittling the Holocaust?\nTopics:"
  },
  {
    "body": "OpenAI, the creator of the popular new program ChatGPT, has introduced a new tool that can detect whether a piece of writing has been created by artificial intelligence or a human being. ChatGPTs ability to instantly write on just about any subject has caused controversy, particularly in the education sector, where students could potentially use it to submit work which is not theirs. The new tool, however, could soon put an end to any underhand tactics by students, although OpenAI says it is not totally successful in identifying AI text. \"Weve trained a classifier to distinguish between text written by a human and text written by AIs from a variety of providers,\" OpenAI said in a blog post. \"While it is impossible to reliably detect all AI-written text, we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty and positioning an AI chatbot as a human.  \"Our classifier is not fully reliable.\" In an evaluation of English texts, the classifier correctly identified 26 per cent of AI-written text as likely to be AI-written\", but incorrectly labelled human-written text as AI-written 9 per cent of the time. The classifiers reliability typically improves as the length of the input text increases. OpenAI said it has made the classifier publicly available to get feedback on whether imperfect tools like this one are useful. The classifier is recommended for use only on English text, because it performs \"significantly worse\" in other languages and is unreliable on code. ChatGPT was banned in New York City after its launch on November 30 due to concerns about negative effects on student learning, and concerns regarding the safety and accuracy of content, the city's department of education said. The Mack Institute for Innovation Management put the program through a Wharton Business School MBA final exam earlier this month. The end result was that it would have received a B to B- grade on the exam. \"We are engaging with educators in the US to learn what they are seeing in their classrooms and to discuss ChatGPTs capabilities and limitations, and we will continue to broaden our outreach as we learn,\" Open AI said."
  },
  {
    "body": "many Australian universities, have banned its use.\nThese bans are not merely the actions of academics who are worried they will not be able to catch cheaters. This is not just about catching students who copied a source without attribution. Rather, the severity of these actions reflects a question, one that is not getting enough attention in the endless coverage of OpenAIs ChatGPT chatbot: Why should we trust anything that it outputs?\nThis is a vitally important question, as ChatGPT and programs like it can easily be used, with or without acknowledgement, in the information sources that comprise the foundation of our society, especially academia and the news media.\nBased on my work on the political economy of knowledge governance, academic bans on ChatGPTs use are a proportionate reaction to the threat ChatGPT poses to our entire information ecosystem. Journalists and academics should be wary of using ChatGPT.\nBased on its output, ChatGPT might seem like just another information source or tool. However, in reality, ChatGPT  or rather, the means by which ChatGPT produces its output  is a dagger aimed directly at its very credibility as an authoritative source of knowledge. It should not be taken lightly.\nTrust and information\nThink about why we see some information sources or types of knowledge as more trusted than others. Since the European Enlightenment, we have tended to equate scientific knowledge with knowledge in general.\nScience is more than laboratory research: It is a way of thinking that prioritises empirically based evidence and the pursuit of transparent methods regarding evidence collection and evaluation. And it tends to be the gold standard by which all knowledge is judged.\nFor example, journalists have credibility because they investigate information, cite sources and provide evidence. Even though sometimes the reporting may contain errors or omissions, that does not change the professions authority.\nThe same goes for opinion editorial writers, especially academics and other experts because they  we  draw our authority from our status as experts in a subject. Expertise involves a command of the sources that are recognised as comprising legitimate knowledge in our fields.\nMost op-eds are not citation-heavy, but responsible academics will be able to point you to the thinkers and the work they are drawing on. And those sources themselves are built on verifiable sources that readers should be able to verify for themselves.\nTruth and outputs\nBecause human writers and ChatGPT seem to be producing the same output  sentences and paragraphs  it is understandable that some people may mistakenly confer this scientifically sourced authority onto ChatGPTs output.\nThat both ChatGPT and reporters produce sentences is where the similarity ends. What is most important  the source of authority  is not what they produce, but how they produce it.\nChatGPT does not produce sentences in the same way a reporter does. ChatGPT, and other machine-learning, large language models, may seem sophisticated, but they are basically just complex autocomplete machines. Only instead of suggesting the next word in an e-mail, they produce the most statistically likely words in much longer packages.\nThese programs repackage others work as if it were something new. It does not understand what it produces.\nMore On This Topic\nAustralian states block ChatGPT in schools even as critics say ban is futile\nThe justification for these outputs can never be truth. Its truth is the truth of the correlation, that the word sentence should always complete the phrase We finish each others... because it is the most common occurrence, not because it is expressing anything that has been observed.\nBecause ChatGPTs truth is only a statistical truth, output produced by this program cannot ever be trusted in the same way that we can trust a reporter or an academics output. It cannot be verified because it has been constructed to create output in a different way than what we usually think of as being scientific.\nYou cannot check ChatGPTs sources because the source is the statistical fact that most of the time, a set of words tends to follow each other.\nNo matter how coherent ChatGPTs output may seem, simply publishing what it produces is still the equivalent of letting autocomplete run wild. It is an irresponsible practice because it pretends that these statistical tricks are equivalent to well-sourced and verified knowledge.\nSimilarly, academics and others who incorporate ChatGPT into their workflow run the existential risk of kicking the entire edifice of scientific knowledge out from underneath themselves.\nBecause ChatGPTs output is correlation-based, how do writers know that it is accurate? Did they verify it against actual sources, or does the output simply conform to their personal prejudices? And if they are experts in their field, why are they using ChatGPT in the first place?\nMore On This Topic"
  },
  {
    "body": "1675241834 564809 1675241905 rrss normal.jpg\n- Advertisement -\nChatGPT, developed by OpenAI, is a large-scale language model that has been used in a variety of applications and industries. Since its release, ChatGPT has been used in a wide range of applications, such as virtual assistants, chatbots, and autoresponder platforms. ChatGPTs ability to understand context and respond in a natural and fluid manner has made it a valuable tool for businesses looking to improve customer engagement and increase efficiency. Furthermore, the model has been trained on a wide variety of tasks and has been shown to be capable of complex tasks such as machine translation and text generation. ChatGPT is an example of ever-evolving artificial intelligence technology and has been applauded for its ability to improve the lives of people and businesses. Over time, ChatGPT is likely to be further integrated into a variety of applications and continue to improve the way we interact with technology. It is very easy to install the AI in the application! Maybe you didnt know it, but ChatGPT can be installed in WhatsApp. In fact, you can have conversations with artificial intelligence and put it to many more uses than you imagine. However, the first thing is to explain how ChatGPT is installed in WhatsApp. The steps are quite simple, you just have to follow them to the letter. This is how you can install ChatGPT on WhatsApp: Enter the God in a Box website Click on Get started Go to Login Create a user with your Google account Associate your WhatsApp phone number Verify the number: enter the conversation with the number generated by the web, write !verify and press sendWait for the message that confirms the verificationGo back to the God in a Box websiteComplete the registration It should be noted that ChatGPT in WhatsApp has a limit if you do not use the version payment. The limits are as follows: You can only send messages every 10 seconds You can only send 10 messages per month Why use ChatGPT on WhatsApp? These are the uses you can give to ChatGPT on WhatsApp: Write messagesWrite emailsWrite any type of textSolve math problemsRequest recipesAsk for recommendations (movies, series, music, books) After all, ChatGPT is a content store. Anything you ask for, if its in the database, it will offer it to you through WhatsApp. The comfort is perfect. >\n- Advertisement -"
  },
  {
    "body": "Image by\nGetty Images\nOpenAI, the company behind blockbuster AI chatbot ChatGPT, has released a tool meant to help teachers detect if a text was written by a student or an AI.\nThe tool couldn't have come at a more appropriate time, with educators across the country battling with a new reality. According to one recent survey, a whopping 48 percent of students confessed they already made use of ChatGPT to complete an at-home test or quiz.\nOpenAI's new tool  with the uninspired name \"AI Text Classifier\"  requires a sample of at least 150 words to classify whether a text is either \"very unlikely, unlikely, unclear if it is, possibly, or likely AI-generated.\"\n\"The model is primarily trained and evaluated on English language text from the public web (in the case of the human-written dataset) and from models trained on English language text (in the case of the model-written dataset),\" the company noted in a small FAQ section amended to the tool's webpage.\nBut whether it'll actually prove useful to educators remains to be seen.\n\"The classifier isn't always accurate,\" the company admits. \"It can mislabel both AI-generated and human-written text.\"\nIn other words, it's not very good yet. In our own cursory testing, the tool was easily capable of identifying all ten of the blog samples we fed it that were written by a human. Nine were evaluated as being \"very unlikely AI-generated\" while one was classified as only \"unlikely AI-generated.\"\nBut things started to look drastically different when we fed it ten text samples generated by ChatGPT. Only four of the samples were rated as \"likely\" to be generated by an AI, and three as \"possibly\" AI-generated.\nOne sample  we asked it to generate 1,000 words on the causes of global poverty  was even listed as \"very unlikely\" to have been AI-generated. A further three AI-generated samples were classified as only \"possibly\" AI-generated.\nThose are some pretty dismal results for a detection tool developed by the same company that came up with the language model it's being measured against.\nThat also means that it's not going to be very useful to educators. After all, only getting a vague semblance of an answer won't be enough for teachers to accuse their students of plagiarism, a serious charge.\nIn fairness, that's something OpenAI is well aware of.\n\"The results may help, but should not be the sole piece of evidence when deciding whether a document was generated with AI,\" OpenAI notes.\nIt's not the first ChatGPT detection tool we've encountered. An app called GPTZero, developed by Princeton University computer science student Edward Tian, made headlines earlier this month for its ability to \"quickly and efficiently detect whether an essay is ChatGPT or human written.\"\n\"Think are high school teachers going to want students using ChatGPT to write their history essays?\" the 22-year-old student"
  },
  {
    "body": "preliminary resource\non the use of ChatGPT for educators, which outlines some of the uses and associated limitations and considerations.\"\nHowever, the research laboratory warns that the new classifier tool is not \"fully reliable\" yet. So far, it has only correctly identified 26 percent of AI-written English texts. It also incorrectly labeled human-written text as AI-written nine percent of the time - \"false positives.\"\nThe new classifier trained on a dataset of human-written and AI-generated texts\nOpenAI also added that in comparison to the previously released classifier, the new classifier is more reliable on text from more recent AI systems.\nThe new classifier is a language model that has been fine-tuned on a dataset of pairs of human-written text and AI-written text on the same topic. The researchers collected the datasets from sources written by humans and divided the texts into prompts and responses.\nMost Popular\n\"On these prompts, we generated responses from a variety of different language models trained by us and other organizations. For our web app, we adjust the confidence threshold to keep the false positive rate low; in other words, we only mark text as likely AI-written if the classifier is very confident,\" according to the blog post.\nThe classifier should not be used as a primary decision-making tool\nIn their blog post, the researchers stress that the classifier should not be used as a \"primary decision-making tool.\" The classifier is unreliable on short texts, those below 1,000 characters. The researchers also recommend using the classifier only for English text. It is unreliable on code.\nIt is also to be noted that AI-written text can be edited to easily evade the classifier. \"Classifiers like ours can be updated and retrained based on successful attacks, but it is unclear whether detection has an advantage in the long-term,\" the blog post reads.\nThe classifier is currently publicly available to get feedback on whether such imperfect tools are useful.\nWill the classifier be a game-changer? All we can do is wait and watch."
  },
  {
    "body": "OpenAI\n, the company behind ChatGPT has launched a new AI classifier tool to determine if a text has been written by a person or by Artificial Intelligence.\nThe tool comes with the caveat that it is not 100 percent reliable and can label human-written text as AI written, you can see more information below.\nWeve trained a classifier to distinguish between text written by a human and text written by AIs from a variety of providers. While it is impossible to reliably detect all AI-written text, we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human.\nOur classifier is not fully reliable. In our evaluations on a challenge set of English texts, our classifier correctly identifies 26% of AI-written text (true positives) as likely AI-written, while incorrectly labeling human-written text as AI-written 9% of the time (false positives). Our classifiers reliability typically improves as the length of the input text increases. Compared to our previously released classifier, this new classifier is significantly more reliable on text from more recent AI systems.\nI just tested it on two articles I wrote yesterday and the result of one was it was not likely to be AI generated and it said the other one was likely to be AI generated. So it obviously need some work before it becomes an accurate tool to identify AI generated content\nYou can find out more details about this new AI classifier tool from Open AI at the link below.\nSource"
  },
  {
    "body": "2023/02/01 09:00 (GMT)\nA new tool can detect whether text was made using AI.\nOpenAI - the start-up behind language model chatbot ChatGPT - has created the AI Text Classifier that will categorise text on a five-level ranking system to decide whether words have been produced by artificial intelligence, and has acknowledged that the new tool could have an \"impact on the classroom\" for cheating students.\nIn a blogpost, Open AI said: \"Weve trained a classifier to distinguish between text written by a human and text written by AIs from a variety of providers. While it is impossible to reliably detect all AI-written text, we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human.\nWe recognize that identifying AI-written text has been an important point of discussion among educators, and equally important is recognizing the limits and impacts of AI generated text classifiers in the classroom. We have developed a preliminary resource on the use of ChatGPT for educators, which outlines some of the uses and associated limitations and considerations. While this resource is focused on educators, we expect our classifier and associated classifier tools to have an impact on journalists, mis/dis-information researchers, and other groups.\nThe team is now \"engaging\" with teachers across the US to understand what is going on in their classrooms and are determined to \"broaden their outreach\" as the conversations take place.\nWe are engaging with educators in the US to learn what they are seeing in their classrooms and to discuss ChatGPTs capabilities and limitations, and we will continue to broaden our outreach as we learn. These are important conversations to have as part of our mission is to deploy large language models safely, in direct contact with affected communities.\n BANG Media International\nBANG Showbiz English\nBANG Showbiz is the worlds premier entertainment news agency providing the most exciting celebrity news to online, print and broadcast media outlets across the globe.\nRecommended\n24 star Annie Wersching dies aged 45 after cancer battle\nPamela Anderson defends Tim Allen over alleged flashing\nSarah, Duchess of York quoted late Queen Elizabeth as she paid tribute to Lisa Marie Presley\nSir Rod Stewart pleads with voters to change bloody government\n'Cheeky' Prince Harry Valentine's card for sale"
  },
  {
    "body": "Brock University\nOf all the reactions elicited by ChatGPT, the chatbot from the American for-profit company OpenAI that produces grammatically correct responses to natural-language queries, few have matched those of educators and academics.\nAcademic publishers have moved to ban ChatGPT from being listed as a co-author and issue strict guidelines outlining the conditions under which it may be used. Leading universities and schools around the world, from Frances renowned Sciences Po to many Australian universities, have banned its use.\nThese bans are not merely the actions of academics who are worried they wont be able to catch cheaters. This is not just about catching students who copied a source without attribution. Rather, the severity of these actions reflects a question, one that is not getting enough attention in the endless coverage of OpenAIs ChatGPT chatbot: Why should we trust anything that it outputs?\nThis is a vitally important question, as ChatGPT and programs like it can easily be used, with or without acknowledgement, in the information sources that comprise the foundation of our society, especially academia and the news media.\nBased on my work on the political\neconomy\nof knowledge governance, academic bans on ChatGPTs use are a proportionate reaction to the threat ChatGPT poses to our entire information ecosystem. Journalists and academics should be wary of using ChatGPT.\nBased on its output, ChatGPT might seem like just another information source or tool. However, in reality, ChatGPT  or, rather the means by which ChatGPT produces its output  is a dagger aimed directly at their very credibility as authoritative sources of knowledge. It should not be taken lightly.\nTrust and information\nThink about why we see some information sources or types of knowledge as more trusted than others. Since the European Enlightenment, weve tended to equate scientific knowledge with knowledge in general.\nScience is more than laboratory research: its a way of thinking that prioritizes empirically based evidence and the pursuit of transparent methods regarding evidence collection and evaluation. And it tends to be the gold standard by which all knowledge is judged.\nFor example, journalists have credibility because they investigate information, cite sources and provide evidence. Even though sometimes the reporting may contain errors or omissions, that doesnt change the professions authority.\nThe same goes for opinion editorial writers, especially academics and other experts because they  we  draw our authority from our status as experts in a subject. Expertise involves a command of the sources that are recognized as comprising legitimate knowledge in our fields.\nMost op-eds arent citation-heavy, but responsible academics will be able to point you to the thinkers and the work\n. And those sources themselves are built on verifiable sources that a reader should be able to verify for themselves.\nTruth and outputs\nBecause human writers and ChatGPT seem to be producing the same output  sentences and paragraphs  its understandable that some people may mistakenly confer this scientifically sourced authority onto ChatGPTs output.\nThat both ChatGPT and reporters produce sentences is where the similarity ends. Whats most important  the source of authority  is not what they produce, but how they produce it.\nChatGPT doesnt produce sentences in the same way a reporter does. ChatGPT, and other machine-learning, large language models, may seem sophisticated, but theyre basically just complex autocomplete machines. Only instead of suggesting the next word in an email, they produce the most statistically likely words in much longer packages.\nThese programs repackage others work as if it were something new. It does not understand what it produces.\nThe justification for these outputs can never be truth. Its truth is the truth of the correlation, that the word sentence should always complete the phrase We finish each others  because it is the most common occurrence, not because it is expressing anything that has been observed.\nBecause ChatGPTs truth is only a statistical truth, output produced by this program cannot ever be trusted in the same way that we can trust a reporter or an academics output. It cannot be verified because it has been constructed to create output in a different way than what we usually think of as being scientific.\nYou cant check ChatGPTs sources because the source is the statistical fact that most of the time, a set of words tend to follow each other.\nNo matter how coherent ChatGPTs output may seem, simply publishing what it produces is still the equivalent of letting autocomplete run wild. Its an irresponsible practice because it pretends that these statistical tricks are equivalent to well-sourced and verified knowledge.\nSimilarly, academics and others who incorporate ChatGPT into their workflow run the existential risk of kicking the entire edifice of scientific knowledge out from underneath themselves.\nBecause ChatGPTs output is correlation-based, how does the writer know that it is accurate? Did they verify it against actual sources, or does the output simply conform to their personal prejudices? And if theyre experts in their field, why are they using ChatGPT in the first place?\nAcademics have authority on their subject of expertise because there exists a scientific and evidence-based method to verify their work. (Shutterstock)\nKnowledge production and verification\nThe point is that ChatGPTs processes give us no way to verify its truthfulness. In contrast, that reporters and academics have a scientific, evidence-based method of producing knowledge serves to validate their work, even if the results might go against our preconceived notions.\nThe problem is especially acute for academics, given our central role in creating knowledge. Relying on ChatGPT to write even part of a column means theyre no longer relying on the scientific authority embedded in verified sources.\nInstead, by resorting to statistically generated text, they are effectively making an argument from authority. Such actions also mislead the reader, because the reader cant distinguish between text by an author and an AI.\nChatGPT may produce seemingly legible knowledge, as if by magic. But we would be well advised not to mistake its output for actual, scientific knowledge. One should never confuse coherence with understanding.\nChatGPT promises easy access to new and existing knowledge, but it is a poisoned chalice. Readers, academics and reporters beware."
  },
  {
    "body": "Access your favorite topics in a personalized feed while you're on the go.\ndownload the app\nEmail address\nBy clicking Sign up, you agree to receive marketing emails from Insider                                  as well as other partner offers and accept our                                  Terms of Service and                                  Privacy Policy.\nThe competition in artificial intelligence is heating up.\nGoogle employees are testing potential challengers to viral AI chatbot ChatGPT  including its homegrown chatbot \"Apprentice Bard\"  CNBC reported on Tuesday, citing sources and internal communication seen by the publication.\nThe bot reportedly uses Google's own language technology, called LaMDA, orLanguage Model for Dialogue Applications.\nThis is just weeks after the tech giant's management reportedly issued a \"code red\" over the rise of ChatGPT, which has been making waves recently as it's able to generate written human-like text.\nApprentice Bard appears to work in a way that's similar to ChatGPT, as users can ask a question in a dialog box and get an answer in response, per CNBC. Users can also give feedback on the bot's response.\nWhen asked if there would be further layoffs at Google  which let 12,000 workers go in January  Apprentice Bard replied that it was \"unlikely\" for another round of layoffs in 2023, CNBC reported, citing an internally circulated example.\n\"Layoffs are generally conducted to reduce costs and structure, but the company is doing well financially. In fact, Google's revenue increased by 34% in 2021, and the company's stock price has risen by 70% since January 2022,\" Apprentice Bard responded, per the media outlet.\nInsider could not independently confirm Apprentice Bard's data source. Publicly available information show revenues at Alphabet  Google's parent company  rose 41% in 2021, while Alphabet Class A shares have fallen 32% since January 2022.\nOther than the Apprentice Bard, Google is also testing other AI-powered products, including a search page.\nGoogle did not comment specifically on the projects reported by CNBC but told Insider it has \"long been focused on developing and deploying AI to improve people's lives.\"\n\"We believe that AI is foundational and transformative technology that is incredibly useful for individuals, businesses and communities, and as our AI Principles outline, we need to consider the broader societal impacts these innovations can have,\" said Lily Lin, a Google spokesperson.\nSign up for notifications from Insider! Stay up to date with what you want to know.\nSubscribe to push notifications"
  },
  {
    "body": "Citrix launches simplified partner program\nWho partied with CRN at the Cali Beach club in the Gold Coast?\nAussie channel converge at the Gold Coast for CRN Pipeline 2022\nPartners join Ingram Micro, Vertiv for cooking lessons\nAmazon Web Services popular chief technology officer, Werner Vogels, slammed OpenAIs ChatGPT regarding its answer to one of his questions regarding cloud cybersecurity.\nVogels checked out ChatGPT's responses to cloud security, and felt that the conversational artificial intelligence bot was in fact lying.\nSecurity has become one of the main drivers of companies migrating to the #aws. However, if you ask #chatgpt it will tell you the opposite, based on \"a recent report\", which shows you it is not concerned about the truth, but just about putting words together convincingly.\nWhile on ChatGPT, the AWS CTO asked the artificial intelligence-based chatbot to write a news story about the impact of cybercrime with the growth of cloud computing.\nChatGPT created a headline that read: Cybercrime takes a toll on cloud computings rapid growth.\nThe AI-powered chatbot, without citing any evidence or data points, said the rapid growth of cloud computing has attracted cybercriminals who have found ways to target the cloud.\nAccording to a recent report, cybercrime has had a significant impact on the growth of cloud computing, with businesses losing billions of dollars each year due to data breaches, hacking attempts and other forms of cyberattacks.\n\"Many businesses are now questioning the security of the cloud and whether it is worth the risk, ChatGPT suggested.\nHowever, AWS CEO Vogels did say something positive about ChatGPTs capabilities on sounding convincing in its answer.\nI am so happy that the fear, uncertainty and doubt (FUD) that old IT companies were using to discredit AWS in the early days was not generated by ChatGPT, as it would have sounded a lot more convincing than who would ever want to rent servers from a bookshop, Vogels said.\nAWS, Google Cloud, Microsoft stance on ChatGPT\nSeattle-based AWS is the worldwide market leader in cloud computing, with a run rate of $80 billion.\nChatGPT is owned by startup OpenAI whose main backer is Microsoft, which is\nAWS\n biggest cloud computing rival in the world.\nMicrosoft has formed an extremely tight partnership with San Francisco-based OpenAI since first investing US$1 billion into the startup in 2019, and has received an exclusive license to commercialise the companys AI technology.\nIntroduced in November, OpenAIs chatbot ChatGPT is gaining popularity due to its ability to create human-like conversational text when responding to prompts or questions by users.\nDominick Delfino, vice president and global leader of cybersecurity sales for Google Cloud, recently took to express his disdain for ChatGPT.\nIn case you think ChatGPT is cool. This is what its being used for, said Delfino on LinkedIn, pointing to a Techradar news article about how cybersecurity researchers from Check Point Research observed ChatGPT being used by cybercriminals to improve and build from scratch dangerous malware and ransomware.\nThere needs to be governance in AI to avoid abuse like this! Delfino said.\nChatGPTs impact on cybersecurity\nFor cyber criminals using the tool to write malware code for deployment in cyberattacks, ChatGPT lowers the barrier to entry for threat actors with limited programming abilities or technical skills, researchers from threat intelligence firm Recorded Future said in a report this month.\nIt can produce effective results with just an elementary level of understanding in the fundamentals of cybersecurity and computer science, according to Recorded Futures report.\nHowever, researchers at Accenture Security have been trying out ChatGPTs capabilities for automating some of the work involved in cyber defense.\nThe initial findings around using the AI-powered chatbot in this way are promising, according to Robert Boyce, Accentures global lead for cyber resilience services .\nIts clear that the tool helps reduce the barrier to entry with getting into the defensive side as well, Boyce recently told CRN.\n"
  },
  {
    "body": "Email Us\nCopy the Link\nStudents in Victorian state schools will not be able to access the artificial intelligence program ChatGPT while on school grounds.\nVictoria's education department on Wednesday announced access to the chatbot had been blocked from state school servers and devices as part of an interim measure.\nWhile many have raised concerns over ChatGPT's capability to help students cheat, the department cited the program's 18-plus age restriction as reason for the ban.\n\"The department is undertaking further analysis of the implications of these emerging technologies and is preparing advice for schools,\" a department spokesman told AAP.\nChatGPT was only launched by tech firm OpenAI in November, but education departments around the world have taken steps to ban the program in schools.\nThere are concerns it could help students cheat on assessments because of the program's ability to compose human-like writing in response to any prompt or instruction.\nThe NSW education department last month issued a similar ban on ChatGPT in state schools, saying it needed time to established clear guidance for teachers.\nAustralian universities are also addressing the emergence of artificial intelligence applications, with the Group of Eight universities moving to more in-person supervision and increased paper assessments this year."
  },
  {
    "body": "Published: Feb 01, 2023,  2:22 AM\nMariyan Slavov\n1\nThe amazing ChatGPT language AI is everywhere these days! People use it to write articles (this one may or may not have been written by the bot, ha!), compose songs, invent new recipes, solve mathematical equations, and, most importantly, get information on every topic imaginable.\nSounds familiar? That's what Google has been doing for decades, and unsurprisingly, the behemoth company is concerned about the future. So concerned that it's developing its own version of ChatGPT. According to CNBC, Google is currently testing several iterations of ChatGPT competitors after company CEO Sundar Pitchai declared \"code red\" and decided to accelerate Google's AI efforts.\nThe company aims to unveil at least 20 AI products this year, and some of them will engage in the battle of the bots with the almighty ChatGPT. One potential candidate for this battle is the so-called Apprentice Bard, a chatbot that uses Google's LaMDA conversation technology. CNBC's sources say that the LaMDA team has been asked to work on creating competitors to ChatGPT.\nLast year, one employee from the said team made headlines after\ndeclaring the LaMDA chatbot sentient and getting fired shortly afterwards\n. We should expect Google to move at a more conservative pace, though. As Google AI chief Jeff Dean told employees during an all-hands meeting to discuss the company's response to ChatGPT, it's moving \"more conservatively than a small startup.\"\nThat makes sense. ChatGPT includes a disclaimer that the bot may provide wrong information or harmful instructions, and it also has limited knowledge about the world after 2021. Google is a much more popular company, and a wrong answer or instruction could result in a massive scandal and dozens of lawsuits.\nWe can't wait to see Google's answer to ChatGPT, but in the meantime, we're looking at a potential career switch in the agricultural field or something with bartending involved.\nAlso Read:"
  },
  {
    "body": "SEE ALL\nFile photo of the OpenAI logo| Photo Credit: AP\nThe company that developed the AI chatbot ChatGPT has released a new classification tool in order to help users identify text that was written with AI, but said the classifier was not fully reliable.\n(For insights on emerging themes at the intersection of technology, business, and policy,\nsubscribe\nto our tech newsletter Todays Cache.)\nOpenAI explained the abilities and limitations of the new classifier it had trained, in a blog post on January 31. The classifier is meant to address rising concerns that the version of ChatGPT that is currently free to use could be exploited to cheat on exams, impersonate humans, or spread misinformation.\nHowever, the new classifier works best when it receives English-language material that is longer than 1,000 characters. Even so, it has a tendency to misidentify human-written text as AI-written, at times.\nArtificial intelligence uncovers unknown play by Spanish great in library archive\nIn our evaluations on a challenge set of English texts, our classifier correctly identifies 26% of AI-written text (true positives) as likely AI-written, while incorrectly labeling human-written text as AI-written 9% of the time (false positives), said OpenAI in its statement.\nThe company also said that it was engaging with U.S.-based educators to enhance its outreach programme and learn more about ChatGPT in educational settings.\nThe news comes as researchers and academics have claimed that ChatGPT was able to pass university-level or even professional exams in the areas of law, business, and medicine. This has triggered fears of AI tools like ChatGPT being used to turn in AI-generated work or unlawfully help students pass qualifying examinations."
  },
  {
    "body": "to save articles for later.\nNormal text size\nVery large text size\nAdvertisement\nStudents and staff will be blocked from using popular artificial intelligence service ChatGPT at Victorian state schools as the Department of Education probes the implications of using the technology.\nThe decision to block access to the AI service from the departments servers was an interim measure, and was because of ChatGPTs terms of use, which specify that users must be at least 18 years of age, a spokesperson said.\nChat GPT is a language-processing AI model that is capable of generating human-like text, such as essays.\nCredit:\niStock\nThe department is undertaking further analysis of the implications of these emerging technologies and is preparing advice for schools, they said.\nVictoria is the latest state in the country to ban ChatGPT at public schools after similar announcements from education departments in NSW, Queensland, Tasmania and, most recently, Western Australia.\nLoading\nChatGPT is a free online tool that generates text in response to a prompt, including articles, essays, jokes and even poetry, and has gained wide popularity since its debut in November.\nIt has also become a topic of debate, as worries grow in academic and teaching circles that its tools could enable widespread cheating.\nIn response to these worries  among other concerns  OpenAI, the creator of ChatGPT,"
  },
  {
    "body": "After launching the most popular AI chatbot &#8212; ChatGPT, the Microsoft-backed AI research and deployment company OpenAI has now launched a new tool called &#8216;The AI Text Classifier; which can detect AI-generated content.\nThe AI Text Classifier is a fine-tuned GPT model that predicts whether a piece of text was generated by AI from a variety of sources, such as ChatGPT. &#8220;This classifier is available as a tool to spark discussions on AI literacy, the company said on the new tool&#8217;s page.\nAccording to the company, AI Text Classifiers reliability typically improves as the length of the input text increases. Compared to a previously released classifier, this new tool is significantly more reliable on text from more recent AI systems.\nAlso, the classifier has a number of important limitations. It should not be used as a primary decision-making tool, but instead as a complement to other methods of determining the source of a piece of text. The classifier is very unreliable on short texts (below 1,000 characters). Even longer texts are sometimes incorrectly labeled by the classifier.\nSometimes human-written text will be incorrectly but confidently labeled as AI-written by our classifier. &#8220;We recommend using the classifier only for English text. It performs significantly worse in other languages and it is unreliable on code,&#8221; the website reads.\nText that is very predictable cannot be reliably identified. For example, it is impossible to predict whether a list of the first 1,000 prime numbers was written by AI or humans, because the correct answer is always the same. AI-written text can be edited to evade the classifier.\n&#8220;Classifiers like ours can be updated and retrained based on successful attacks, but it is unclear whether detection has an advantage in the long-term. Classifiers based on neural networks are known to be poorly calibrated outside of their training data. For inputs that are very different from text in our training set, the classifier is sometimes extremely confident in a wrong prediction,&#8221; the company said.\nRead all the Latest Tech News here"
  },
  {
    "body": "EasyEquities\nBy Chuck Saletta*\nAI chatbots look like they have the potential to impact all sorts of industries. Here are three players in the space that may be worth a look.\nChatGPTs recent public launch generated a lot of buzz. The chatbots ease of use, wide range of trained topics, and remarkable fluent communication style have people across industries scrambling to figure out how to build similar capabilities into their operations. ChatGPTs potential to disrupt industries is becoming clear  and even highly skilled roles are likely to be impacted by it in one way or another.\nIn a world where the choice may very well be between doing the disrupting or getting disrupted yourself, it makes sense to ask if theres a way to invest in ChatGPT or other technologies like it. ChatGPT itself is a project of OpenAI  which is a company currently managed by a non-profit. As a result of that structure, you cant directly buy shares of OpenAI. Still, its worth assessing the landscape to see what might be available for investors to consider.\n3 ways to get invested in the trend\nFirst, Microsoft (NASDAQ: MSFT) is an investor in OpenAI, which means it will likely directly benefit from anything ChatGPT can monetize. To jumpstart that, Microsoft has launched an OpenAI service as part of its Azure platform, with ChatGPT integration expected to come soon. As a key benefit of being an investor in the company behind ChatGPT, Microsoft is clearly well positioned to be an early adopter and beneficiary of its capability.\nEven before the launch of ChatGPT, Microsoft had already made a name for itself in the chatbot space with its Bot Framework. That platform is generally considered among the best in class when it comes to the ability to create and deploy chatbots for enterprises.\nNext on the list is Alphabet (NASDAQ: GOOGL). Not one thats used to being caught behind a major trend, Alphabet has long been an investor in AI platforms. Its DeepMind business line is the company behind tools that help with scientific tasks like protein folding research. It has also flexed its muscles with AlphaGo, the AI platform that can beat the worlds best players in the classic game of Go.\nStill, the popularity of ChatGPT has taken Alphabet a bit by surprise, and Alphabet now looks like it is working to aggressively incorporate more chatbot like services into its offerings as well. Given its strengths in other aspects of artificial intelligence and DeepMinds recent shift to being a profitable business line, I certainly wouldnt count Alphabet out on this one.\nFinally, what would a tech war be without participation from Amazon (NASDAQ: AMZN)? Amazons Alexa for Business platform offers Alexa as a digital assistant for people at work. Key areas where that platform already excels is in things like joining or coordinating online meetings, calendar management, and conference room scheduling.\nCompanies can also program private skills for their Alexa for Business offerings, enabling them to customize the AI to perform new tasks for them. That, along with its great voice recognition and response technology and continued development by the Alexa team itself, will likely keep Amazons Alexa for Business in the game for quite some time.\nIf you cant beat them, consider buying them\nChatGPT was certainly a breakthrough in the AI chatbot space, and its one that looks like it is poised to launch a revolution across many industries. We may not be able to avoid the disruption thats likely coming our way, but we can put ourselves in the position to potentially profit from it.\nAll three of these companies look like ones that have potential to ultimately be winners from this new technology. Whether any of them deserve a spot in your portfolio is up to you, but if youre interested in investing in the AI chatbot space, theyre certainly worth looking at.\nAt the time of publication, Chuck Saletta owned shares of Microsoft.\nSources  EasyResearch, The Atlantic, OpenAI, The Verge, Learn Microsoft, BotPress, DeepMind, VentureBeat, AWS Amazon, Ed Week\nRead also:"
  },
  {
    "body": "Is it really called Apprentice Bard?\nWhen will Apprentice Bard be available?\nWhat are Google's other new AI products?\nWant to know more about ChatGPT?\nGoogle is apparently feeling the heat from ChatGPT and has accelerated its own AI development in response.\nArtificial intelligence isn't new. But for whatever reason, one little chatbot is captivating everyone right now: ChatGPT. Developed by San Francisco-based research lab OpenAI, ChatGPT seems to be exploding in popularity because it's easy to use and can provide eerily human-like responses to a wide range of questions and prompts. It was trained on a vast amount of text data from the internet (up until 2021) and can be used to write essays, answer questions, solve math problems, and it can even do fun things like generate responses in the style of a celebrity voice.\nThe best part? It's free, and it's easily accessible on the web.\nThe success of ChatGPT has not gone unnoticed by Google, as it's said to be working on its own competitor to the chatbot called Apprentice Bard.\nWhat is Apprentice Bard?\nGoogle staff have been developing a number of AI products in response to OpenAI's ChatGPT, according to a report by CNBC, including an AI chatbot called Apprentice Bard. Sundar Pichai, the CEO of Google, recently declared a \"code red\" and is speeding up development of these AI-powered products. Apprentice Bard is said to powered by Google's LaMDA conversation technology, which enables the creation of advanced chatbots that can understand and respond to a wide range of questions and requests in a natural and human-like manner.\nApprentice Bard uses LaMDA and apparently functions similarly to ChatGPT in that you type in a question or a prompt and receive a written response in return. However, it's capable of providing information on recent events. ChatGPT is unable to due to its limited knowledge cut-off of 2021. One example provided by CNBC was Apprentice Bard's ability to answer if there would be a fresh round of layoffs at Google (it replied \"unlikely this year\"). It is worth noting LaMDA previously stirred up controversy after a former\nGoogler claimed it gained sentience.\nIs it really called Apprentice Bard?\nGoogle has not officially announced its chatbot -- let alone that it's internally reportedly called Apprentice Bard. It very well could get a new name when and if it's unveiled to the public.\nWhen will Apprentice Bard be available?\nGoogle is yet to announce its ChatGPT competitor - let alone a launch date. But the company is expected to reveal more about its AI-powered products at its I/O conference in May.\nThe New York Times\nreported Google plans to reveal at least 20 AI-powered products this year. But it remains uncertain which of Google's AI products will be made available to the public. During a recent all-staff meeting, Google's AI head Jeff Dean reportedly highlighted how the company is being cautious and placing a high emphasis on accuracy and preventing the spread of misinformation.\nWhat are Google's other new AI products?\nApart from the Apprentice Bard AI chatbot, Google is reportedly developing an image generation tool, a TikTok-style green screen mode for YouTube, and a feature called Shopping Try-on. The company is also working on new tools to make it easier for developers to create Android apps, as well as a wallpaper creator for Pixel phones. Keep in mind Google just announced it's laying off 12,000 people, with Pichai recently writing in a\nthat \"to fully capture [the huge opportunity in front of us with AI], we'll need to make tough choices\".\nPocket-lint\nWant to know more about ChatGPT?\nPocket-lint has an in-depth guide on the AP chatbot here:"
  }
]