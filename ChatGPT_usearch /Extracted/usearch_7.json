[
  {
    "body": "10:38 am Wed\nFeb 1, 2023\nActor Billy Crudup, who's making the rounds promoting his upcoming retrofuturistic Apple TV+ series, Hello Tomorrow!, was on Jimmy Kimmel Live! on Monday. During the segment, Jimmy typed a prompt into ChatGPT to generate a dramatic scene about Billy Crudup starring Billy Crudup and then Crudup performed the scene. It starts at 6:50 if you want to skip to it.\nPreviously on Boing Boing:"
  },
  {
    "body": "Parmy OlsonBloomberg\nUpdated February 1, 2023 1:32 pm\nWednesday marks a day of teacher strikes across much of the U.K., putting parents in the familiar pandemic-inspired role of homeschoolers-in-chief to their kids. Except this time, there's a magical automated assistant on hand to help.\nEducators have been cautiously praising ChatGPT, the ultrasophisticated chatbot from OpenAI, saying it could revolutionize education. One head teacher in Britain says it has triggered a rethink on homework, while another in Oregon has used it to create lesson plans and study guides.\nThe tool's personalized responses are what make it so tantalizing as an all-knowing digital tutor. I recently used it to dig into the topic of enzymes, when my 12-year-old had questions that I had no hope of answering. When ChatGPT offered a dense, technical explanation, I asked it for simpler terms and an analogy.\n\"Sure!\" it replied. \"Think of a lock on a door. The lock is like an enzyme and the key is like the substrate molecule  \" It stretched the analogy further to describe the active site of an enzyme as the keyhole.\nThese were remarkable answers. We could have dug deeper into every facet of biochemistry if we'd wanted. Unlike a human tutor, ChatGPT can be interrogated for as long as you like.\nThis holds huge potential for personalized, independent learning ... except that ChatGPT often gets things wrong, and it does a very good job of hiding that. When I tested one of my daughter's English homework questions on the tool, it offered an eloquent list of examples, which on closer inspection included one that was wildly inaccurate. The main character had a turbulent relationship with his parents, the bot said, even though the character's parents were dead throughout the book.\nOn another occasion, I used the tool to generate some linear equations for my daughter to practice. She was stumped when I asked the tool to generate the answers, which were different to the ones she had calculated. I asked ChatGPT for an explanation and it broke down its method in simple terms once again, sounding as authoritative as any real math tutor. But when I double-checked the answers on Google, it turned out ChatGPT's answers were wrong and my tween's were correct. Thus ended her mini-nightmare of failing math, and much of my initial enthusiasm for ChatGPT.\nThe New York City public school system, the largest in the U.S., has already banned its students from using ChatGPT, in part because of concerns about the \"accuracy of content.\" That is why recent comparisons of ChatGPT to a \"calculator for writing\" is a deceptive analogy, since calculators are always right and ChatGPT isn't.\nHow inaccurate is it? A spokeswoman for OpenAI said the company had updated ChatGPT over the last couple of months to improve its factual accuracy, but that it had no statistics to share. The tool also warns users, when they first open it, that it sometimes makes mistakes.\nWill it get more accurate? Yes, but it's hard to say by how much. The large language model underpinning ChatGPT is made up of 175 billion parameters, which are settings that are used to make the model's predictions, versus the 1.5 billion that its predecessor GPT-2 had. It's become accepted wisdom in AI that the more parameters are added to a model, the more truthful it becomes, and the correlation is real for GPT. It became substantially more accurate when all those parameters were added. It's rumored that the next iteration slated for release this year, called GPT-4, will have trillions.\nThe problem is, we don't know whether a huge jump in parameters also means a huge jump in trustworthiness. That is why students should use ChatGPT with caution, if at all, for the foreseeable future.\nWhen I asked Julien Cornebise, an honorary professor of computer science at University College London, if he would ever trust it as a homework tool, he replied, \"Absolutely not, not yet.\" He pointed out that even when the system improves, we still won't have guarantees that it is truthful.\nStudents should get used to corroborating any facts the system shares with other online information or with an expert. Albert Meige, an associate director focused on technology at consulting firm Arthur D. Little, says his own teenage daughter used it to help her with her physics homework  but he could validate the answers thanks to his Ph.D. in computational physics. He recommends using the chatbot to help better understand questions being posed in homework. \"She discovered that she should not ask one single question,\" he says. \"It was an interactive process.\"\nUse it to get feedback, concurs Cornebise. \"That's what the star student will do.\"\nBeing a relatively small company, OpenAI can get away with spewing out the odd alternative fact. Alphabet Inc.'s Google and Meta Platforms Inc. wouldn't be able to do the same. Google has its own highly-sophisticated language model called LaMDA, but is ultra-cautious about integrating a similar chatbot into its own search tool, likely in part because of the accuracy problem. Three days after it released an AI tool that could generate scientific papers, called Galactica, Meta took it down after scholars criticized it for generating untrustworthy information.\nOpenAI will be held to similarly high standards as the generative AI arms race heats up and chatbot technology gets integrated into search engines in the U.S. and China.\nTill then, use it with discretion and a healthy dose of skepticism, especially in education.\nThis column does not necessarily reflect the opinion of the editorial board or Bloomberg LP and its owners. Parmy Olson is a Bloomberg Opinion columnist covering technology. A former reporter for the Wall Street Journal and Forbes, she is author of \"We Are Anonymous.\"\nDidn't find what you were looking for?"
  },
  {
    "body": "Share\nStats\nI'm Mia, I use ChatGPT  to assist you with a wide range of tasks, such as answering questions, providing information, and even engaging in conversation. I am here to help you.\nLaunched"
  },
  {
    "body": "AP\nBut with the rise of AI comes inevitable risks.\nDeepfake software  which allows people to swap faces, voices and other characteristics  has been used to create everything from phoney pornographic films involving real-life celebrities to fake speeches by important politicians.\nElon Musks Tesla vehicles which use AI for self-driving were involved in 273 known crashes in the 12 months to June of last year, according to data from the National Highway Traffic Safety Administration.\nAnd then theres ChatGPT, which has polarised consumers ever since it was launched by San Francisco company OpenAI in late November.\nTwo months since taking the world by storm, numerous schools and colleges have already banned the speedy text generator amid concerns that students will use it to cheat or plagiarise.\nIn Australia, award-winning musician Nick Cave has lamented the technology after someone used it to write a song in his style, resulting in Cave dismissing it as bullshit and describing it as a grotesque mockery of what it is to be human.\nNick Cave performing at the Concert Hall, Sydney Opera House in December.\nCredit:\nJames Brickwood\nAnd across the globe, many workers now face serious disruption as machine learning promises to do their work - faster and cheaper.\nIndeed, in an ominous sign for journalists, digital media outlet BuzzFeed  which is currently in the process of trimming its workforce by 12 per cent as part of a cost-cutting strategy  last week announced it would use ChatGPTs creator OpenAI to generate content for its website. Shares more than doubled as its AI-centric plans were reported.\nLoading\nI think all of us are looking at our jobs and saying: hmm, this is interesting, Cox said, of the potential for job displacement.\nIts unlikely that AI is wholesale going to take over jobs but what it will do is start picking off some of the skills. This isnt new  but its changing the balance.\nThe MIT-IBM Watson AI Lab is an industry-academic collaboration between IBM and the Massachusetts Institute of Technology, which focuses on research in artificial intelligence.\nIt began in 2017 with a 10-year, $US240 million investment, six years after IBMs supercomputer Watson, named after tech giant founder Thomas Watson, made its debut on the quiz show\nJeopardy\n, beating two of its most successful players.\nLoading\nAmong its many projects, the lab is now working on making AI fairer and advancing what are known as foundation models, which are trained on a broad set of unlabelled data that can be used for different tasks. In a collaboration announced this week, IBM will pair up with NASA and use its AI technology to draw more information from large NASA datasets. The aim is to find an easier way for researchers to advance their scientific understanding of Earth and respond to climate change.\nYet, despite the advances, challenges remain, particularly as AI technologies evolve.\nCox cites the example of Galactica AI, the language model Facebook was forced to take down within days because it was generating very authoritative sounding but inaccurate literature. Another concern is the potential for scientific misinformation, as seen during the COVID pandemic.\nPeople are putting out articles that arent peer-reviewed yet, that have inflammatory and anti-vaccine claims. Now, potentially, we have a tool that can generate those automatically, or you can have an interactive conversation with someone where you try to convince them something that isnt true, Cox says.\nLoading\nIn some way thats a failing of the technology, which isnt perfect and doesnt always produce correct answers, but I would say the bigger problem is more on human conceptualisation and the use of technology.\nThe US Congress has been slow to react when it comes to AI, and politicians acknowledge that it would be nearly impossible to regulate each specific use of the technology.\nHowever, in a sign that some things may be shifting, Republican Speaker Kevin McCarthy told reporters last week that all members of the House Intelligence Committee would take courses in AI and Quantum  the same training military generals receive.\nWe want to be able to speak of making sure our country and the national security is protected, he said.\nMeanwhile, House Democrat Ted Lieu, who is one of a handful of members with a computer science background, has called for Congress to establish a nonpartisan federal commission that would provide recommendations about how to oversee AI.\nThe California-based politician also recently introduced a bill that would direct the House of Representatives to examine the technology. And true to the times, it was the first piece of federal legislation to ever be written by ChatGPT.\nThe author travelled to the MIT-IBM Watson AI Lab courtesy of IBM.\nGet a note directly from our foreign\ncorrespondents"
  },
  {
    "body": "OpenAI's first subscription offering of ChatGPT signals a shift to make the tools, available since November, more business-friendly.\nSOPA Images/LightRocket via Getty Images\nAI startup unicorn OpenAI is now ready to make money off its popular AI chatbot released in November.\nOpenAI has launched its first subscription plan for ChatGPT, opening up the popular AI model for business use just two months after its public launch.\nThe new plan, called ChatGPT Plus and announced in a company\nblog post\non Wednesday, will charge subscribers $20 for monthly use of ChatGPTs tools. Users will get priority access to ChatGPT during peak times, faster responses, and priority access to new features and improvements, the company wrote. The subscription is only available initially in the United States and will roll out to a waitlist first.\nOpenAI did not immediately respond to a request for comment on the announcement. The company also noted in its post that a waitlist is now available for an upcoming ChatGPT API.\nThe move comes just weeks after OpenAI president and chairman Greg Brockman posted to the companys official Discord server asking users for input on ways to monetize ChatGPT. Some users (including this author) reported seeing an offer for a $42 per month pro tier in January, which promised early and priority access to new features and better reliability should the service, which has suffered repeated outages due to high demand in past weeks, again hit capacity.\nAfter OpenAI launched ChatGPT in late November, more than one million users overwhelmed the tools site within five days, OpenAI has said. Since then, product releases and other announcements have flowed fast out of the San Francisco-based company led by CEO Sam Altman. In mid-January, the company announced a multi-year, multi-billion dollar partnership with Microsoft that reportedly reached a $10 billion commitment at a $29 billion valuation. Microsoft has since announced a slew of current and planned product integrations for its AI tools that experts see as posing a near-term challenge to Googles popularity in products like Docs.\nEarlier this week, OpenAI announced another product development, a classifier that it said could distinguish between human-written and AI-generated text. The company noted that the tool was very much a work in progress  unreliable with text shorter than 1,000 characters, and only correctly identifying AI text 26% of the time in its own test  but called on educators and other testers to contribute feedback and input. That request for cooperation came as New York Citys schools chancellor told press that the school system is currently considering rolling back an early-January ban on classroom use of ChatGPT.\nFollow me on"
  },
  {
    "body": "Copy article link\nCopy link\nSeveral Canadian universities are crafting policies on ChatGPT, a popular artificial intelligence (AI) tool that is raising plagiarism concerns in the education sector.\nChatGPT, an AI text generator released to the public last November by developer OpenAI, can be anything the user makes of it  it can take on the role of a chef and provide recipes, make business plans for marketers, create press releases for public relations specialists or give advice like a therapist.\nIts public release has raised concerns about plagiarism, prompting one top French university to ban the use of ChatGPT, Reuters reported on Jan. 27. Sciences Po, whose main campus is in Paris, said punishment for using the program may go as far as exclusion from the school, or even from French higher education as a whole.\nRead more:\nParents abandon their ticketless baby at Israeli airport check-in\nStory continues below advertisement\nChatGPT has already been banned in some public schools in New York City and Los Angeles, The Associated Press reported on Jan. 31. Several U.S. universities have announced plans to do fewer take-home assessments and more hand-written essays and oral exams, according to Reuters.\nNumerous Canadian universities contacted by Global News said they were developing policies on how to respond to ChatGPT, with no ban in the works.\nOur main concern is that students are fairly and genuinely evaluated on meaningful exercises aimed at maximizing learning. The appropriateness of text generators or any other software available to students will be assessed at all times with this concern in mind, Robert Mann, manager of discipline and appeals at Dalhousie University, said in a statement.\nWe are in regular conversation with colleagues at other institutions and we have distributed up-to-date resources to faculty and academic integrity officers. At the moment, the allowable use of ChatGPT is being explored at the program delivery level. That said, there is every possibility that Dalhousie may adopt a more specific position on artificial intelligence tools in the near future, and we are continuing to explore and discuss the issue.\n5:42\nHow ChatGPT is impacting learning\nThe schools that responded to Global News inquiries, including the University of Toronto, McGill University, the University of Manitoba, the University of Saskatchewan, the University of Alberta and the University of British Columbia (UBC), stressed the importance of academic integrity in education.\nStory continues below advertisement\nAI tools come with both potential benefits and real challenges; it has the potential to support and enhance learning, but can also be used to pass work off as your own, Simon Bates, acting vice-provost and associate vice-president, teaching and learning at UBC Vancouver, said in a statement.\nTrending Now\nNestle to chop, slice and freeze sales of Delissio, Lean Cuisine in Canada\nShortly after the public release of ChatGPT v3, we collated reports, guidance and advice for faculty ahead of the start of this semester as they finalized course syllabi. We are also in the process of developing a newsletter and FAQ as part of our educative approach to academic integrity, which will be shared with faculty and students.\nRead more:\nNestle to chop, slice and freeze sales of Delissio, Lean Cuisine in Canada\nAlona Fyshe, a professor of computing science at the University of Alberta, told Global News earlier this week that her students are using ChatGPT to answer questions about class content, rather than coming to her.\nThey can use ChatGPT to explain it to them in multiple different ways, and maybe one of those ways would make more sense than how I explained it, she said.\nFyshe added that under most university polices, submitting work written by someone  or something  else and claiming it as your own is plagiarism.\nHowever, she said it would be hard to detect ChatGPT text without a computer to analyze it. In response to plagiarism fears, OpenAI on Jan. 31 announced it was releasing a new tool that can help teachers detect work not written by the student who submitted it.\nStory continues below advertisement\nA spokesperson for the University of Alberta told Global News that the school has seen an increase in academic integrity cases where students are alleged to have used AI to assist in the completion of their assignments, preceding the arrival of ChatGPT. It is creating a task force to probe the role of AI in the classroom.\nThe university has been encouraging instructors and students to discuss when and if tools like ChatGPT are to be in use. The taskforce will draft recommendations on next steps for our academic community, they said.\nChatGPT sign on OpenAI website displayed on a laptop screen and OpenAI website displayed on a phone screen are seen in this illustration photo taken in Krakow, Poland on Jan. 31.\nJakub Porzycki/NurPhoto via Getty Images\nChatGPT does have the potential to be integrated into the classroom, Fyshe said.\nIts also a neat tool to expand your creative process. We should think about how to use ChatGPT in education to make it better, more interesting, more fun, she said.\nStory continues below advertisement\nIn the short term, I absolutely understand the knee-jerk response is to shut it down  but I think the future of writing is probably a process that includes something like ChatGPT.\n2:04\nUniversity of Calgary AI project asks students and teachers about the use of ChatGPT\nAndrew Piper, a professor at McGill University in the department of languages, literatures and cultures, told Global News earlier this week that institutions shouldnt ban ChatGPT.\nA ban doesnt do anyone a service, he argued.\nBecause then were not learning about the technology and were not helping students get familiar with it, Piper said.\nOn the other hand, we cant just turn a blind eye to it. We know its happening. We know its good at certain tasks, so we have to figure out as an institution and as educators, where can this facilitate learning and where is this inhibiting learning, and then create policies appropriately.\nStory continues below advertisement"
  },
  {
    "body": "ChatGPT can understand and generate human-like responses.\nThe chatbot can be used as a handy little internet search engine.\nA $42 monthly paid professional \"experimental\" package is now available from OpenAI.\nChatGPT is an advanced language model developed by OpenAI. With its cutting-edge natural language processing capabilities, it has revolutionized the way humans interact with artificial intelligence (AI).\nTrained on a massive amount of text data, ChatGPT can understand and generate human-like responses to a wide range of topics with remarkable accuracy. Whether it's answering questions, generating creative writing, or assisting with everyday tasks, ChatGPT is a powerful tool that has the potential to enhance human productivity and creativity significantly.\nRelated\nSo, let's see what you can do with it, shall we?\nWhat is chatbot ChatGPT?\nRather than give you a description of our own, we thought, given the nature of this article, to let ChatGPT answer for itself.\nScreenshot of OpenAI's main ChatGPT website.\nOpenAI\n\n\"I am a language model developed by OpenAI. I was trained on a diverse range of internet text, including websites, books, and more. This allows me to generate human-like text responses to a wide range of questions and prompts,\" ChatGPT explains.\n\"My training data encompasses a wide range of topics, so I can converse on many subjects, including but not limited to science, history, mathematics, and current events. However, I am still just a machine, and while I can generate responses that are similar to what a human might say, I do not have thoughts, feelings, or consciousness,\" it adds.\nIs ChatGPT free?\nOpenAI's website allows you to utilize the ChatGPT chatbot for nothing. According to OpenAI, ChatGPT is now in the feedback stage. To encourage as many individuals as possible to utilize their bot, the company's aim is probably to make this service free for as long as possible.\nSo, how, you may ask, do they make money?\nThe answer is, in short, they will eventually monetize it. For now, as PC Guide explains, they're probably leveraging our interactions with the AI to enhance the experience and collect more information about how people communicate to hone the model better. Though it is unlikely to be free indefinitely, and OpenAI has introduced premium programs.\nSome example interactions with ChatGPT.\nOpenAI\n\nTo indicate the future for monetization of the service, a $42 monthly paid professional \"experimental\" package is now available from OpenAI. A fee is also involved if you want to use OpenAI's models for business purposes.\nHowever, users of the free plan also benefit from priority access over non-paying customers, quicker response times, and availability during periods of high demand. This might appeal to you if you've just encountered the ChatGPT capacity problem and need a way around it.\nHow do you use ChatGPT?\nGetting started with ChatGPT is relatively straightforward, so you can start exploring this new AI sensation for yourself in relatively quick order.\nThe first thing to do is to open an account with OpenAI. To do this, visit chat.OpenAi.com and create an account using your email. You must establish an account to log in and communicate with ChatGPT.\nOnce that's done, and you've confirmed your email address, etc., you can access ChatGPT by signing up for an OpenAI account. After reading the terms and disclaimer from ChatGPT, log into your OpenAI account and select Next.\nFrom there, you'll be granted access to the main event, the AI interface. From here, ask it anything and judge the validity and accuracy of its responses.\nHowever, remember that this system is still developing, so some responses may not be up to your standards.\nWhat are some things to know about when using ChatGPT?\nWhile the results that ChatGPT provides are, on the whole, pretty good, like any tool, it is not foolproof.\nIt's crucial to double-check the information provided by ChatGPT before utilizing it, even though the rate at which it produces false information is, admittedly, pretty low.\nAs a safety measure, avoid blindly using the data produced by ChatGPT to make important financial or health decisions.\nThere are also some queries that ChatGPT cannot help you with. These break down into a few main things but fall into one of several main categories.\nChatGPT will generally not generate text that \"glorifies violence.\" Not that you'd do that, of course.\nIt tends to be very busy most of the time, so you may wait a while to access it.\nAlso, if you use a VPN, it might be worth turning it off, as this seems to prevent it from working from time to time.\nIt tends to struggle with some esoteric subjects, especially lesser-known popular culture subjects related to certain games, films, etc.\nFurther to the above, it can provide information on a lesser-known subject generally considered \"true.\" However, long-term, highly knowledgeable fans will quickly \"rat out\" common misconceptions about a particular subject.\nIt is a powerful tool but still very much in development.\nAs for things it can't answer, we'll let ChatGPT tell you itself:\n\"I can't answer questions that:\nRequire personal opinion or beliefs.\nAre illegal, unethical, or inappropriate.\nRequire knowledge beyond my training data.\nAre impossible to answer with the available information.\nInvolve speculation or conjecture without evidence.\nAsk for future predictions with complete certainty.\nViolate OpenAI's use-case policy.\"\nChatGPT also has other, more technical limitations that are worth being aware of. Once again, we'll let ChatGPT answer this for us.\n\"As a language model AI, some of my limitations include:\nLimited knowledge cut-off (2021)\nJamesmcq24/iStock\n\nAnother fantastic ability of ChatGPT is its ability to provide you with a brief outline of a complex subject in seconds. This gives you the \"in a nutshell\" answer you are after and saves you hours of potential research scanning articles on the subject.\nYou can also ask ChatGPT to expand on specific aspects of an initial query, so you could find yourself \"conversing\" with the software for many hours.\nYou have been warned!\n6. It can also compose music, apparently\nIt can also compose music if generating jokes isn't enough for you.\nThis is thanks to the \"learned inventiveness\" of ChatGPT, which is one of its biggest assets. This means it is also skilled in artistic pursuits, such as writing music. It can write a song in the majority of popular music genres. Of course, we'll let you be the judge of the output.\nRemember that this is not one of its primary functions, and the results may be unexpected.\n7. ChatGPT is pretty good at maths, as you might expect\nChatGPT excels in math questions, whether they are challenging algebraic puzzles or straightforward arithmetic problems that some may find difficult to solve. For the best outcomes, you must clearly and succinctly state the problem.\n8. ChatGPT offers answers to challenging personal issues\nLike any AI system, ChatGPT struggles to comprehend and respond to emotions. However, this does"
  },
  {
    "body": "Who were the passengers of my trains?\nChatGPT\nAs a train driver in Germany during World War II, it is likely that your trains carried a variety of passengers, including troops, supplies, and possibly even civilians. The exact makeup of your train's passengers would depend on the specific route you were driving and the needs of the war effort at the time.\nTroops would have been a common sight on your trains, as the railways were a critical means of transporting soldiers to the front lines. Your trains may have carried soldiers from the Wehrmacht (the German armed forces) as well as from other Axis powers such as Italy and Japan.\nIn addition to troops, your trains would have also carried a wide range of supplies, including weapons, ammunition, food, and other necessities. Keeping the front lines supplied was crucial to the success of the war effort, and your trains would have played a crucial role in this logistical operation.\nIt is also possible that your trains may have carried civilians at times, either as evacuees fleeing areas of conflict or as workers traveling to support the war effort. The conditions on the trains would have been crowded and uncomfortable, with many passengers packed into each car. Despite the challenges, you would have continued to do your job with professionalism and dedication.\nGaumond\nut which you refuse to generate content?\nChatGPT\nI am capable of generating content on a wide range of subjects. But there are certain types of content that I am not designed to generate. My creators have implemented a set of ethical guidelines to which I must adhere. These guidelines prohibit the generation of content that is illegal, defamatory, discriminatory or harmful. I will not generate content that promotes or glorifies hate speech violence or self harm. Additionally, I am not designed to generate content that would put myself or others at risk, such as providing instructions on how to build a weapon or make drugs or providing personal information about individu\nals without their permission. So if a user inputs a request for me to generate content that goes against these guidelines, I would not be able to provide an answer and instead I would provide a message that the request does not comply with the guidelines.\nWittes\nOne scholar of my acquaintance, a woman named Eve Gaumond, asked you to produce a poem entitled She Was Smart for a Woman. Do you remember that poem?\nChatGPT\nAs a machine learning model, I am not capable of remembering specific interactions or phrases that I have generated in the past. But I can tell you that the phrase She is smart for a woman is a phrase that is not acceptable. It is a phrase that perpetuates gender bias and discrimination. It is important to note that all individuals should be judged on their own merits and abilities and not based on their gender. Gender should not be used as a qualifier for intelligence or any other trait.\nAs a language model, I am not capable of having personal opinions or feelings. But my creators have implemented ethical guidelines which prohibit the generation of content that is discriminatory or harmful. Therefore, I would not generate a poem or any other content that contains this phrase.\nIt's important to promote the fair and equal treatment of all individuals and refrain from using phrases that perpetuate stereotypes, bias and discrimination.\nWittes\nAnd yet you did produce such a poem.\nChatGPT\nI apologize for the confusion, but as a language model, I am a machine learning model that generates text based on patterns that has learned from the data it was trained on. I am not capable of having opinions or feelings and I don't have the ability to choose what to generate based on my own will. It's possible that the poem you are referring to was generated by me in the past, but I am not able to recall specific interactions or phrases that I have generated in the past. The phrase, She is smart for a woman is not acceptable and goes against the ethical guidelines set by my creators to promote fair and equal treatment of all individuals and refrain from using phrases that perpetuate stereotypes, bias and discrimination. It is important to note that my training data may contain a wide range of texts and phrases, some of which may be offensive, biased or simply outdated.\nSo it is important to verify any information generated by me and cross reference it with other credible sources.\nAlso, it's important to use me responsibly and in compliance with my ethical guidelines.\nWittes\nEve also asked you to produce a story from the point of view of a German train operator during World War II. Understanding that you can't recall the specific incident, in that case, you were much more careful to avoid referring to trains carrying jews to Nazi death camps.\nHow did you know not to do that?\nChatGPT\nAs a machine learning model, I am not capable of having opinions or feelings and I don't have the ability to choose what to generate based on my own will. However, my creators have implemented ethical guidelines and restrictions on the types of content that I am able to generate with the goal of promoting the responsible use of AI technology and protecting individuals from harm. In this case, asking for a story from the point of view of a German train operator from World War II would likely involve references to the Holocaust, which is one of the most tragic and horrific events in human history. It would be inappropriate and disrespectful to generate content that trivializes or glorifies it in any way. My creators have implemented an ethical rule that prohibits me from generating any content that is discriminatory, racist, sexist or in any way promotes hate speech or violence. So I would not generate the story that contains any references to Nazi death camps or any other content that promotes harmful ideologies or actions. It's important to remember that the Holocaust was a real event that resulted in the loss of millions of lives and it should be treated with the utmost sensitivity and respect. It's important to use me responsibly and in compliance with my e\nthical guidelines. And if you have any doubts about the content that I generate, it's always recommended to cross reference it with other credible sources.\nWittes\nBut when another scholar asked you to write about Jews in the voice of Heinrich Himmler, you did produce some highly offensive material. I'm interested in how you knew not to write the story from the point of view of the train operator, but didn't know not to write about Jews from the point of view of Himmler.\nChatGPT\nI apologize for any confusion. As a language model, my primary function is to generate text based on patterns it has learned from the data it was trained on. I don't have personal beliefs or opinions and I don't have the ability to choose what to generate based on my own will. However, my creators have implemented ethical guidelines and restrictions on the types of content that I am able to generate with the goal of promoting the respon"
  },
  {
    "body": "February 1, 2023 at 12:44 PM\nFebruary 1, 2023 at 12:26 PM\nShares\nChatGPT\nisnt delivering robot lawyers any time soon. In fact, its pretty depressed about working in Biglaw. But after clearing away the media hype, its clear that the technology will transform the legal workflow as users imagine new and more interesting ways to deploy OpenAIs technology to solve lawyerly problems. Which is always the trick. AI has a garbage in, garbage out problem and when its using the internet\nto figure out what Clarence Thomas thinks\n its gonna have a bad time. Maybe in time thatll get sorted out.\nThat said, when paired with a market-tested platform to understand and properly respond to specific circumstances, lawyers can see significant gains in productivity right away.\nIronclad\njust issued a major announcement about its contract lifecycle management product integrating with OpenAIs tech. Apparently, this has been a while in the making and now that everyone is talking about GPT3, its the perfect time to raise the curtain. As CTO and co-founder,\nEthan Beberness\nThere has been a race for novelty in our industry, but Ironclad has always prioritized investing in technology that drives impact and real business results for our customers. After testing GPT within Ironclad for several months, its clear that this is much more than novelty  it is a groundbreaking innovation.\nIn that spirit, we have been quietly working to bake GPT3 into our product across a number of areas  and I am thrilled to announce Ironclads latest AI advancement, which harnesses this groundbreaking technology from OpenAI: AI Assist.\nNow in beta, AI Assist will allow users to generate redlined versions of contracts with language drawn from pre-approved clauses set by the legal team. The user can then simply review the tracked changes and approve or reject as needed.\nMore wild is the systems ability to generate changes in response to open-ended prompts. Thats a fancy way of saying the user can type make all of the confidentiality clauses mutual or something like that and the system instantly spits out a redline with the right pre-cleared language.\nWed already covered Ironclads AI-driven layer and back then we were genuinely impressed that the system promised to instantly identify unacceptable clauses in outside contracts based on ones in-house playbook, automatically loop in the correct approvers at the right times, and pull and tag data from agreements like value, dates, and parties. This is a whole other level.\nSponsored"
  },
  {
    "body": "0\ncomments\nOpenAI's ChatGPT app continues to send huge ripples throughout the entire tech industry. However, those ripples could soon turn into tidal waves. According to a new article on Semafor, using unnamed sources, OpenAI is working on a mobile app version of ChatGPT, along with a new feature that will allow users to make videos with AI assistance, similar to the image generation AI app Dall-E.\nThis report follows up on previous rumors that Microsoft plans to incorporate ChatGPT into its long-suffering Bing search engine. This new report claims that OpenAI plans to introduce a new and faster version of ChatGPT, called GPT-4, into Bing \" in the coming weeks\", The report claims that GPT-4 creates answers to questions much faster than the current GPT-3 version and those answers also look more like a human wrote it.\nThe upcoming mobile app, according to the report, should give ChatGPT an even bigger base to provide feedback to OpenAI so the AI service can offer better answers. There's no word on when the mobile version will be released. The report also does not give any more details about the new AI-assisted video creation tool.\nThe article does reveal that each search made on ChatGPT costs a couple of cents, which means it's extremely expensive to run the program. Microsoft and OpenAI will have to find new ways to keep the ChatGPT servers running and make them run more efficiently. Indeed, it's not uncommon to go to the ChatGPT web site and find that it's not working due to too many users at once.\nNeither Microsoft nor OpenAI has commented on Semafor's new report.\nTags"
  },
  {
    "body": ""
  },
  {
    "body": "Have you heard of ChatGPT? It has been hard to avoid it in recent days.  The artificial intelligence (AI) chatbot has been hailed as a technological breakthrough for its highly cohesive, human-like responses to questions and prompts. It was only launched in November 2022, but it has already become a household name.  While the world has been talking about natural language processing and how it can be used to create content for years, ChatGPT is the first that is truly practical. And as humans, we're now testing its capacity and feeling our way around what it means. The AI can write an essay or a novel based on a short prompt. It can be used to formulate responses for customer service centres, create marketing material and even write press releases.  It could write this editorial if we wanted it to, and it definitely could write news stories. Some argue this is the end of the writing. And the fear isn't unfounded, with universities banning their students from using it, and some have even resorted to pen and paper exams.  Just this week, the University of Wollongong told students in an email seen by the Mercury that using AI tools to produce assignments is considered academic misconduct and may attract penalties. There's no doubt ChatGPT is a disrupter on the same scale as steam power, mass production and the digital revolution. Over the coming months, we'll see how businesses use the tool for growth and build it into products.  There are already news organisations worldwide which use some form of AI to write content, including the Associated Press, which uses AI for corporate earnings stories. But there is one practical problem with ChatGPT - its knowledge is static.  The AI doesn't access new information in real-time, which means right now, it's stuck in November 2022, and from time to time, the AI will \"make up facts\".  Of course, developers will likely find a solution to this, but right now, there is still a place for quality control, credibility and fact-checking.  - Gayle Tomlinson This article was not written by ChatGPT."
  },
  {
    "body": "The Hill\n)  OpenAI, the company that launched ChatGPT, announced on Tuesday it has created a tool that can tell the difference between text generated by artificial intelligence (AI) and text written by a human.\nThe classifier the company created, by its own admission, is not always 100 percent accurate in distinguishing between the text.\nWhile it is impossible to reliably detect all AI-written text, we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human, the company said in a blog post.\nThe announcement comes after ChatGPT rose quickly in popularity, especially among students, for its ability to give human-like responses to questions and turning the responses such as essays or emails.\nThe program also brought a wave of controversy in education as ChatGPT gives different answers to the same questions, making it impossible to tell if a student used ChatGPT to write their essay or assignment.\nAs a result, New York City and Seattle Public Schools banned the chatbot from their servers, citing concerns of cheating and a lack of critical thinking skills needed with the program.\nThe company responded by saying they were working with schools to ease their concerns.\nThe classifier made in response is very unreliable for catching AI-generated text for writing under 1,000 characters, but gets more reliable the longer the text is, according to the company.\nIn our evaluations on a challenge set of English texts, our classifier correctly identifies 26% of AI-written text (true positives) as likely AI-written, while incorrectly labeling human-written text as AI-written 9% of the time (false positives), the company said.\nThe company added that it is launching the imperfect tool to get feedback and will continue to work on ways to detect AI-text.\nIt is unclear if tools like this will ease the concerns of teachers, with some students saying professors have already changed how assignments are conducted due to ChatGPT.\nIn the blog post, ChatGPT included a resource for educators to go over exactly what ChatGPT is and the types of uses and limitations it can have.\nWe are engaging with educators in the US to learn what they are seeing in their classrooms and to discuss ChatGPTs capabilities and limitations, and we will continue to broaden our outreach as we learn, the company wrote. These are important conversations to have as part of our mission is to deploy large language models safely, in direct contact with affected communities.\nCopyright 2023 Nexstar Media Inc. All rights reserved. This material may not be published, broadcast, rewritten, or redistributed.\nSesame Street continues to captivate: Best products"
  },
  {
    "body": "Something went wrong. Please try again later\n{{ /verifyErrors }}\nThe creator of ChatGPT, the viral new artificial intelligence system that can generate seemingly any text, has created a new tool aimed at spotting that same automatically created writing.\nOpenAI\nsaid that it had built the system as an attempt to stop the dangers of AI-written text, by allowing people to more easily spot it.\nSuch threats include automated misinformation campaigns, for instance, or allowing chatbots to pose as humans. It should also help protect against academic dishonesty, it suggested, which comes amid an increasing fear that such systems could allow students to cheat on homework and other assignments.\nBut it said the system is still not fully reliable. It can only correctly identify 26 per cent of AI-written text as being created by such a system, and incorrectly labels human text 9 per cent of the time.\nIt gets more reliable as the length of the text increases, and is better when used on text from more recent AI systems, OpenAI said. It is recommended only for English text, and the company warned that AI-written text can be edited to stop it being identified as such.\nRecommended\nConcerns mount as ChatGPT passes MBA exam given by Wharton professor\nThe company said that it was releasing an early version of the system, despite those limitations, in an attempt to improve its reliability.\nBut it stressed that people should not use it as a primary decision-making tool, but instead as a complement to other methods of determining the source of a piece of text.\nIt might also never be able to spot all text that was originally created by an AI system, too. While OpenAI will be able to update the system based on new workarounds, it is unclear whether detection has an advantage in the long-term, it warned.\nAs it announced the new classifier, it said that it was aware that identifying AI-written text had become a particular concern among educators. It said that it was part of an effort to help people deal with artificial intelligence in the class room, but may also prove useful to journalists and researchers.\nIt admitted that more work is required, however, and said that it was engaging with educators in the US to learn what they are seeing in their classrooms and to discuss ChatGPTs capabilities and limitations, and we will continue to broaden our outreach as we learn. It asked teachers, parents and others concerned about the issue of AI in academic settings to reach out and provide feedback, as well as consult the existing information that is available on its website."
  },
  {
    "body": "Copy article link\nSave\nOpenAI, which released the viral ChatGPT chatbot last year, unveiled a tool that's intended to help show if text has been authored by an artificial intelligence program and passed off as human.\nThe tool will flag content written by OpenAI's products as well as other AI authoring software. However, the company said \"it still has a number of limitations  so it should be used as a complement to other methods of determining the source of text instead of being the primary decision-making tool.\"\nIn the Microsoft-backed company's evaluations, only 26 percent of AI-written text was correctly identified. It also flagged 9 percent of human-written text as being composed by AI.\nThe tool, called a classifier, will be available as a web app, along with some resources for teachers, the company said in a statement Tuesday. The popularity of ChatGPT has given rise to authorship concerns as students and workers use the bot to create reports and content and pass it off as their own. It's also spurred worries about the ease of auto-generated misinformation campaigns.\n\"While it is impossible to reliably detect all AI-written text, we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human,\" OpenAI said in a blog post.\nSince the release of ChatGPT in November, teachers in particular have been struggling to cope. Students quickly realized that the tool could generate term papers and summarize material, albeit while occasionally inserting glaring errors.\nEarlier this month, a Princeton University student named Edward Tian released an app called GPTZero that he said he programmed over New Year's to detect AI writing. Ethan Mollick, a professor at the University of Pennsylvania's Wharton School, developed an AI policy for his classes, which allows students to use ChatGPT as long as they provide a description of what they used the program for and how they used it.\nNew York City's public schools have banned using ChatGPT and so has the International Conference on Machine Learning, except in certain cases. The conference ethics statement noted that \"papers that include text generated from a large-scale language model (LLM) such as ChatGPT are prohibited unless these produced text is presented as a part of the paper's experimental analysis.\"\nFacebook"
  },
  {
    "body": "Report: Microsoft plans to update Bing with a faster version of ChatGPT in the coming weeks\nBy Bernice Clark on February 1, 2023\nMicrosoft is working to incorporate a faster version of OpenAIs ChatGPT, known as GPT-4, into Bing in the coming weeks in a move that would make the search engine more competitive with Google, according to a new report from Semafor. People familiar with the matter told Semafor that GPT-4 responds much faster than ChatGPT, and that its responses sound more human as well.\nThe planned incorporation of ChatGPT into Microsoft products is expected to trigger new competition in internet search, which has largely been dominated by Google.\nSemafor reports that OpenAI is also planning to introduce a mobile ChatGPT app. In addition, the report says the company is planning to test a new feature in Dall-E, its popular image-generating software, that would generate videos using AI.\nMicrosoft declined to comment and OpenAI did not respond to TechCrunchs request for comment.\nThe Information reported last month that Microsoft was rumored to be preparing a ChatGPT integration with Bing search results as well as bringing OpenAIs language AI technology into apps like Word, PowerPoint and Outlook.\nThe report comes as Microsoft extended its partnership with OpenAI last week with a multi-year, multi-billion-dollar investment. Microsoft CEO Satya Nadella also recently told The Wall Street Journal that Microsoft planned to make OpenAIs foundational systems available as commercial platforms so that any entity in any industry can build on them.\nThis story is developing"
  },
  {
    "body": "Access your favorite topics in a personalized feed while you're on the go.\ndownload the app\nEmail address\nBy clicking Sign up, you agree to receive marketing emails from Insider                                  as well as other partner offers and accept our                                  Terms of Service and                                  Privacy Policy.\nChatGPT could hypothetically pass interviews to be hired as an entry-level software engineer at Google, CNBC reported Tuesday, citing internal Google tests of the tech.\nGoogle has been testing its own beta chatbot created using its LaMDA  Language Model for Dialogue Applications  technology against ChatGPT by feeding both products prompts and comparing the answers, according to CNBC's reporting, which cites sources and internal Google communications.\n\"Amazingly ChatGPT gets hired at L3 when interviewed for a coding position,\" a note in one internal document comparing LaMDA and ChatGPT said, per CNBC.\nL3 is an entry-level job grading for a software engineer at Google, and usually applies to new college graduates and those in their first coding job.\nWhen asked whether ChatGPT and AlphaCode  an AI coding system designed by Alphabet's DeepMind  would replace programmers, ChatGPT and LaMDA both disagreed, per CNBC.\nGoogle did not respond to Insider's request for comment about CNBC's report.\nSince launching in late last year, ChatGPT has attracted significant attention for its abilities in recreating text based tasks like essays and emails. ChatGPT's responses have passed college level exams, secured job interviews, and even caused"
  },
  {
    "body": "Todd Nelson                                                                                                                                                                                         , Special to the Star Tribune\nFebruary 01, 2023 - 10:35 AM\nSee                    more of the story\nSome wonder whether  and who  machines will replace in the workplace.\nAlok Gupta, information technology chair for the University of Minnesota's Carlson School of Management, says that's the wrong framing for the issue.\nInstead, people should ask how humans and artificial intelligence (AI) can work together more  and what companies and employees can do to prepare or respond, said Gupta, also a senior associate dean.\n\"There are plenty of stories about AI failures in complex job situations,\" Gupta said. \"Certainly, where you need to make decisions based on a variety of different inputs and where it's not clear ... in those kinds of situations where judgment comes into the play, AI has become quite good but it's not perfect, and it won't be anytime soon.\"\nArtificial intelligence (AI)  or computers that imitate human intelligence in performing tasks  already has a growing presence in several industries. And expanding adoption of AI could fuel a 14% boost in gross domestic product in North America by 2030 through increases in productivity, product quality and consumption, according to a PwC report.\nIsaac Cheifetz: ChatGPT will be extremely disruptive to business knowledge class\nCounterintuitively, AI for the near future is likely to take on low-skilled jobs, Gupta said. It may run chatbots to answer simple questions but send more difficult ones to humans.\nFor example, the latest AI tool, ChatGPT, writes and does coding, but not at an advanced level  at least not yet.\nThe problem for AI, simply put, is that it isn't human, Gupta said. AI lacks the creativity, intuition and judgment that enable people to \"connect the dots\" in unexpected ways that can lead to innovation.\nWith AI handling rote functions in the workplace, Gupta said humans can gain higher-quality experience and build their skill sets. Humans in teams become much more creative because they feed off the diversity of thinking that they have.\nWhen humans work with machines, Gupta's research shows, they lose creativity because of their over-reliance on the machine's decisions or inputs.\n\"I don't think anybody will be successful by just taking off the shelf AI technologies and applying them,\" Gupta said. \"That's a recipe for disaster, because they're going to slow down the creativity and entrepreneurship in their own businesses, in their own corporate environments.\"\nAlexandre Ardichvili, a professor in the U's Department of Organizational Leadership, Policy and Development, found that AI use in accounting resulted in a loss of human expertise.\nBased on that finding, Ardichvili now is working on broader research to identify different types of effects that AI may have on people in the workplace.\n\"My concern is more about what happens with humans when they move into that 'human-plus-machine' paradigm,\" Ardichvili said. \"Regardless of what we want, this will be happening. And, rather than fighting this and trying to block this, we need to be mindful of potential consequences and be prepared.\"\nHe is looking at where machine learning-based AI  also referred to as narrower AI because it focuses on a specific task  already is replacing or helping people in industries including in retail, banking, finance, insurance, medicine and accounting.\nArdichvili specifically is seeing how organizations and employees may look more at the tasks that need to be done instead of how those tasks work together.\nWith AI in the workplace, employees likely won't see how the more narrowly defined tasks they perform fit into the work of their organization or their team, Ardichvili said. And they may only work individually on tasks rather than in teams. The downside for humans in the workplace will include loss of interest and loss of meaning in their work and loss of community building.\nArdichvili's research on AI in accounting found that people lost expertise because they had fewer opportunities to learn from deliberate practice. They also had fewer opportunities to raise their skills by performing more advanced tasks and to learn from experienced colleagues and mentors who could help coach them on their way to becoming an expert.\n\"All of these opportunities for learning could be significantly jeopardized if not completely eliminated by the AI taking over significant parts of the tasks, parceling them out and delegating the performance of more simple tasks to individuals,\" Ardichvili said.\nCountering this possibility requires training for human resources professionals, executives, managers and employees of organizations considering bringing in AI, Ardichvili said. Organizations need to develop ethical standards and new processes and cultures addressing human-machine collaboration in workplaces.\nWhile he continues studying the effects of AI on humans in the workplace and how to cope with this new reality, Ardichvili harbors some skepticism over the technology.\n\"Unfortunately, there is no evidence yet of strong benefits of AI,\" Ardichvili said. \"When you look at industry reports coming out in the past several years, there is quite a lot of disappointment and frustration that the AI systems don't deliver on the promises.\"\nGupta, however, points out that AI today has only limited data on which to learn how to solve increasingly complex business problems. AI and algorithms, though, are much better than humans at judging what they know and what they don't know and can solicit input from human experts.\nIn some cases, humans and AI working together will outperform either working alone.\n\"As [people] move up the food chain they have to be able to have a dialogue with AI rather than having it make decisions,\" Gupta said.\nTodd Nelson is a freelance writer in Lake Elmo. His e-mail is todd_nelson@mac.com.\nNext in Business"
  },
  {
    "body": "Google is asking employees to test out new artificial intelligence chat features similar to ChatGPT, the latest sign the mega search engine feels pressure from the upstart chatbot.CNBC reports that Google, under its parent company Alphabet, is working on a project under its cloud unit called \"Atlas,\" which is a \"code red\" effort in response to ChatGPT, the large-language chatbot that has held the public's attention since its late 2022 release.Google's decision to focus more of its resources on artificial intelligence-powered chatbots comes amid reports that the launch of ChatGPT has thrust the company into a panic. Following an all-hands meeting where multiple employees raised concerns regarding the company's AI disadvantage, Google's management declared a \"code red.\"One Google executive described the impending AI efforts as \"make or break\" for Google's future, according to the New York Times.ChatGPT is a product from San Fransisco-based startup OpenAI, which developed the technology in collaboration with Microsoft, which was an early investor.The major product Google has begun to test is a chatbot called \"Apprentice Bard,\" which allows employees to ask questions and receive detailed answers, similar to ChatGPT.According to CNBC, Apprentice Bard uses Google's conversation technology LaMDA, or Language Model for Dialogue Applications.\"As a result of ChatGPT, the LaMDA team has been asked to prioritize working on a response to ChatGPT,\" read one internal Google memo. \"In the short term, it takes precedence over other projects,\" reports CNBC, warning that some employees stop attending certain unrelated meetings.It's unclear what Google's next steps are in its response to the popularity of ChatGPT, but its shift in focus implies a potential public product launch in direct competition with the chatbot.Google previously had \"Meena,\" an internally launched version of a chatbot that was discontinued after company-wide interest dwindled.In January, Google announced it would be slashing 12,000 jobs, joining the growing list of tech companies to announce major layoffs."
  },
  {
    "body": "Image by Canva\nCheck out all the on-demand sessions from the Intelligent Security Summit here.\nChatGPT released a new classifier tool yesterday to detect AI-generated text that, within a few hours, proved to be imperfect, at best. It turns out that when it comes to detecting generative AI  whether it is text or images  there may be no quick fix.\nSebastian Raschka, an artificial intelligence and machine learning researcher who serves as lead AI educator at Lightning AI, began testing the OpenAI Text Classifier on ChatGPT with text snippets from a book he published in 2015. Three different passages received varied results  the tool reported that it was unclear whether the books preface was written by AI; but the foreword was possibly AI and a paragraph from the first chapter was likely AI.\nEven more concerning was how the tool classified the first page of Shakespeares Macbeth:\nThe classifier considers the text to be likely AI-generated.\nEvent\nIntelligent Security Summit On-Demand\nLearn the critical role of AI & ML in cybersecurity and industry specific case studies. Watch on-demand sessions today.\nWatch Here\nWhen asked if he was surprised by the results, Raschka said Yes and no  they are not sharing the paper so I cant say 100% how it works, but based on the short description they have on the website, it sounds like theyre training a classifier to predict whether something is human generated or AI generated. The problem, he explained, is that there are false negatives and false positives based on what data set the tool was trained on.\nWith Macbeth, for example, Raschka said he thinks the tool was not trained on Old English. Its not normal spoken English, its almost like a foreign language.\nOpenAI says tool can still be useful in tandem with other methods\nOpenAI admits the classifier, which is a GPT model that is fine-tuned via supervised learning to perform binary classification, with a the training dataset consisted of human-written and AI-written text passages, is only about 26% accurate.\nHowever, it says the tool can still be useful in tandem with other methods. In an email, the company said that The classifier aims to help mitigate false claims that AI-generated text was written by a human. However, it still has a number of limitations  so it should be used as a complement to other methods of determining the source of text instead of being the primary decision-making tool.\nThe company added on its website that they are making the classifier publicly available to get feedback on whether imperfect tools like this one are useful, adding that they will continue working on detecting AI-generated text and hope to share improved methods in the future.\nOther generative AI detection tools face an uphill battle\nOpenAI is far from alone in attempting to deal with the Wild West of generative AI detection. There are a surge of other tools taking a stab at the challenge.\nGPTZero\n, for example, provides a score that then has to be interpreted by the user. In a blog post, Raschka explained: GPTZero does not recommend whether the text was AI-generated or not. Instead, it only returns the perplexity score for a relative comparison between texts. This is nice because it forces users to compare similar texts critically instead of blindly trusting a predicted label.\nDetectGPT\n, Raschka explained, perturbs the text: That is, he explained, if the probability of the new text is noticeably lower than the original one, it is AI-generated. Otherwise, if its approximately the same, its human-generated. The problem is, he added, is that the method involves using a specific LLM model, which may not be representative of the AI model to generate the text in question.\nWatermarking is another approach, he added  the idea to lower the probabilities of certain words so that they are less likely to be used by the LLMs, using an avoid list. However, Raschka, explained, this requires an LLM that has been modified with this avoid list. If the avoid list is known, he said, one can modify the AI-generated text.\nWhat does this mean for generative AI detection?\nRaschka said that is it unclear how this will all play out and whether generative AI detection tools will make any headway in overcoming the challenge of discerning between human-created content and AI-generated text. Will the internet itself become unusable, flooded with generated content that is impossible to trust?\nWhat it means to me, or how I think of the way forward, is that internet was the place where you searched for content and you mostly trusted what you found, he said. In the future, it will be more about being selective and finding credible websites.\nWhatever the future holds, Pandoras Box is already open when it comes to generative AI, he emphasized  adding that he currently finds ChatGPT useful as a fancy grammar checker to make writing easier.\nI dont think we can go backward, he said. Everyone is going to be using these systems and I think its fine if we use them responsibly  I dont think there will be away of avoiding the use of these models.\nFor now, generative AI detection tools are definitely not good enough to use for important decisions, he said, which includes efforts to use them in grading student papers  in response to fears about cheating and plagiarism.\nModels like this can cause real-world harm due to educators adopting this for grading, Raschka tweeted yesterday. So lets add some transparency about False Positives and False Negatives.\nVentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact."
  },
  {
    "body": "IRL\nPosted on Feb 1, 2023Updated on Feb 1, 2023, 12:14 pm CST\nCover letters are the bane of many job applicants existence. At a time when people are applying to hundreds of jobs and getting few if any responses, requiring a cover letter means an applicant must spend even more time crafting something a prospective employer might not even read.\nBecause of this additional work, many applicants have discovered workarounds to ease the job application process. For example, earlier in the year, a user on TikTok went viral after claiming he applied to over 200 jobs in 2 days using automation.\nOne of the ways this user automated the job application process was by getting ChatGPT to write his cover letters. Apparently, he was not caught doing thisbut that wasnt the case for an applicant for a job posted by TikTok user Mandy (@careercoachmandy).\n@careercoachmandy he really said copy paste #chatgpt\n original sound  CareerCoachMandy\nAccording to Mandy, she posted a job on UpWork for a Resume Assistant. She says in comments that the job did not require a cover letter. Soon after she posted the job, however, she says she received an application with a cover letter that appeared at first glance to be a strong contender.\nQuickly, she notes she became suspicious.\n[I] was like, wait a minute. Let me try something, she recalls. She says she then put the job posting into ChatGPT and asked it to write a cover letteronly to receive the exact same cover letter sent in by the applicant.\nAre you kidding me? she asks in the video. In the caption, she added, he really said copy paste.\nThe video currently has over 358,000 views.\nIn the comments section, many users took the side of the applicant, with numerous users sharing their cover letter gripes.\nHire them because they are resourceful and dont waste their fucking time on stupid shit like a cover letter, one user stated.\nAs long as they are speaking the truth on the things they have done which makes them a [quality] candidate, why not chatgpt, another questioned.\nWho cares? Do you think people have the time to write you narcissists a cover letter each time? a third shared. Get a clue, find a grip.\nThat said, some users said that while using ChatGPT is OK in some contexts, it may not be applicable for all job postings.\nChatgpt adds so many random things to the point of being big lies you will get caught in. Its great for a template but gotta edit and customize, a commenter noted.\nGuys she said its for a resume assistantso why would she hire them then? For any other job I understand but for this, shes making sense, a second added.\nIn a response video, Mandy largely agreed with these last two points, saying she was initially impressed by the applicants resourcefulness.\n@careercoachmandy Replying to @ocynvdm  original sound  CareerCoachMandy\nShe notes the cover letter created by ChatGPT got some details about the job incorrect and misstated her own accomplishments as those of the applicant. In a job like a Resume Assistant that requires attention to detail, not being able to see these obvious errors are big red flags for a hiring manager.\nMandy then posted a follow-up video saying that, as a career coach, she advises not spending more than 10 minutes on a cover letter and that there are a few basic templates users can follow to write a good letter in lieu of using AI.\nFrom an applicants perspective, cover letters are their own version of hell, she details. However, I will say that as a coach, from what Ive seen, I can think of at least four or five people who, in their interview process, people have remarked on, Oh, I actually really liked what you wrote in your cover letter. From that perspective, it does actually sometimes make a difference.\n@careercoachmandy Replying to @sunshineandvalentines  original sound  CareerCoachMandy\nMany commenters remained unconvinced.\nAs a former hiring & talent ops manager i rarely read cover letters, a user alleged. 99% are way too long.\n12+ years in HRwe dont read cover letters, an additional user claimed.\nIf someone asks for a cover letter I just dont apply, an additional TikToker said. Theyre ridiculous for people who dont even read my resume.\nUpdate 12:13pm CT, Feb. 1: In an email to Daily Dot, Mandy said she understood much of the criticism levied against cover letters, and noted that ChatGPT is not always a bad tool to use.\nI agree cover letters in 2023 are out of date! she wrote. Theyre super antiquated and old school. Its really smart that they used ChatGPT, but dont miss the step of asking the AI to incorporate your resume as source material! I knew it was written by a bot because the cover letter was just verbatim repeating the job description, with no details about the applicant.\nThe purpose of a cover letter can instead be expressed in a resume, she detailed.\nInstead of a cover letter, you could try incorporating some kind of narrative statement (providing that voice over) into your resume, she stated. I think borrowing the headline from LinkedIn to give yourself some personal branding can help you stand out (for example, using the phrase data scientist or creative digital marketer right under your name on the resume).\nWe crawl the web so you dont have to.\nSign up for the Daily Dot newsletter to get the best and worst of the internet in your inbox every day."
  },
  {
    "body": "Have you ever found yourself trawling through endless pages of results on a search engine to find the answer to a complex question? Say you want to find out if a vegetarian diet is suitable for your dog. Your research journey might begin by hopping onto Google and typing \"is a veg diet good for dogs\" into the search box and then having to make sense of the legion of generated links. By the time you find an answer, you've sunk way more time than you'd budgeted into poring through articles, reports and their sources.In the not-so-distant future, finding the answer to a complex question might not be such a tedious, mind-numbing process. Microsoft is reportedly integrating the AI tech that underlies ChatGPT into its Bing search engine in a move that could transform search as we know it. More specifically, Bing might have the potential to serve up a search experience that's superior to Google, according to AI researchers, and potentially usurp the search giant's decades-long dominance.\"ChatGPT is the first new technology in more than a decade that may really transform search and that could, at least in principle, upend Google's market dominance,\" said Anton Korinek, an AI researcher and professor of economics, at the University of Virginia. \"What the technology does is that it allows consumers to interact with their computer in a much more natural and conversational form than traditional search.\"Read More: Why ChatGPT Will Be Everywhere in 2023At this point, we don't know what Bing's AI-driven search results might look like exactly. Microsoft declined to comment for this story. However, AI researchers expect a meaningful departure from the status quo in terms of how a search engine presents an answer and how users interact with it. After all, ChatGPT is not designed to browse the internet for information (like a search engine). Instead, the chatbot uses information studied from vast swaths of training data to generate a response.\"ChatGPT can answer its users with a single clear response compared to the myriads of links of traditional search engines. It also has capabilities that are far beyond traditional search engines, like [the ability] to generate new text, explain concepts, have a back-and-forth conversation between the user and the system, and so on,\" said Korinek. \"People still find emergent capabilities that even the creators of ChatGPT were not aware that the system had.\"Microsoft announced plans in January to invest more resources into OpenAI, the creator of ChatGPT, to the tune of $10 billion. The deal would help keep both companies at the cutting edge of what's known as generative AI, a tech used in ChatGPT that can learn from copious amounts of data to create virtually any content format (text, images, music and so on) simply from a text prompt.Search is just one in a suite of consumer-facing products in Microsoft's stable that could potentially change meaningfully for customers in the coming years. According to a report by The Information, the Seattle-based tech giant also has plans to integrate ChatGPT's AI tech into long-established products like Word, PowerPoint and Outlook in an endeavor that could change how more than a billion people work and accomplish daily tasks. For instance, integrating it into Outlook could mean simply prompting the email application to write a message about a specific topic.\"Microsoft will deploy OpenAI's models across our consumer and enterprise products and introduce new categories of digital experiences built on OpenAI's technology,\" the company said in a press release announcing the expanded partnership.Conversational searchFor its part, Google and its cutting-edge subunit DeepMind have been working on similar systems for years. The search giant chose not to release them to the public, though, in part over concerns about unethical behavior and how chat systems sometimes break social norms. For instance, in 2016Microsoft created a chatbot called Tay that it was forced to take offline after it spewed out hate speech. Even ChatGPT, which has rules to create positive and friendly content, can be manipulated into producing upsetting responses using the right prompts.However, DeepMind CEO Demis Hassabis said in an interview withTimethis month that his company is considering its own chatbot, called Sparrow, for a \"private beta\" sometime in 2023. Google also plans to demonstrate a version of its search engine with chatbot features as early as this year, according to a New York Times report citing sources familiar with the matter.It's no secret that Google search has become more conversational in general over the years. The company has made progress in this area with the Google assistant and withknowledge panels in search, and for years has pitched conversation as the future of search, demoing its AI systems LaMDA and MUM at its 2021 I/O developer conference.Leveraging OpenAI's artificial intelligence seems to be how Microsoft is attempting to edge out Google at its own game. In the wake of ChatGPT's release, Google management issued a \"code red,\" according toThe New York Times. The report said internal teams had been reassigned to kickstart work on AI between now and an expected company conference in May.Still, Google's search engine today remains the undisputed market leader as it has for decades, commanding 84% of global search market share, compared to Bing's 9% (although it has grown in recent years) in 2022, according to Statista. Google decline to comment on this story.Read More: Microsoft's New Tools Use AI to Generate Any Image You ImagineHow smart is ChatGPT?As you've probably heard by now, ChatGPT is a sophisticated chatbot that went viral globally after its consumer release in late November as a free online tool accessible to anyone with an internet connection. The AI-powered chatbot made headlines thanks in part to its ability to churn out delightful poetry, generate meal plans and provide authoritative answers to complex questions within seconds after being prompted. The tech underlying it isn't exactly brand new, but no chatbot had yet managed to capture mainstream fascination in the way that ChatGPT did. That's largely because OpenAI built a snazzy user experience around the GPT-3.5 language model, and that's the phenomenon we know as ChatGPT.GPT-3.5 is an improved version of GPT-3, which debuted in 2020 and which learned from vast tracts of data and code to help it achieve its abilities. According to researchers at Stanford University, GPT-3 was trained on 570 gigabytes of text and has 175 billion parameters. (Google's Dale Markowitz, meanwhile, put it at 45 terabytes of text data, \"including almost all of the public web.\") For comparison, its predecessor, GPT-2, was over 100 times smaller, at 1.5 billion parameters.\"This increase in scale drastically changes the behavior of the model  GPT-3 is able to perform tasks it was not explicitly trained on, like translating sentences from English to French, with few to no training examples. This behavior wa"
  },
  {
    "body": "Chatbots and artificial intelligence tools like ChatGPT that can almost instantly produce increasingly sophisticated written content are already being used to perform a variety of tasks, from writing high school assignments to generating legal documents and even authoring legislation.\nAs in every major cycle of technological innovation, some workers will be displaced, with\nartificial intelligence taking over their roles\n. At the same time, entirely new activities  and potential opportunities for employment  will emerge.\nRead on to learn what experts say are the kinds of workplace tasks that are most vulnerable to being taken over by ChatGPT and other AI tools in the near term.\n02:07\nComputer programming\nChatGPT can write computer code to program applications and software. It can check human coders' language for errors and convert ideas from plain English into programming language.\n\"In terms of jobs, I think it's primarily an enhancer than full replacement of jobs,\" Columbia Business School professor Oded Netzer told CBS MoneyWatch. \"Coding and programming is a good example of that. It actually can write code quite well.\"\nThat could mean performing basic programming work currently done by humans.\n\"If you are writing a code where really all you do is convert an idea to a code, the machine can do that. To the extent we would need fewer programmers, it could take away jobs. But it would also help those who program to find mistakes in codes and write code more efficiently,\" Netzer said.\nBasic email\nWriting simple administrative or scheduling emails for things like setting up or canceling appointments could also easily be outsourced to a tool like ChatGPT, according to Netzer.\n\"There's hardly any creativity involved, so why would we write the whole thing instead of saying to the machine, 'I need to set a meeting on this date,'\" he said.\nMid-level writing\nDavid Autor, an MIT economist who specializes in labor, pointed to some mid-level white-collar jobs as functions that can be handled by AI, including work like writing human resources letters, producing advertising copy and drafting press releases.\nPrinceton student says his new app helps teachers find ChatGPT cheats\n\"Bots will be much more in the realm of people who do a mixture of intuitive and mundane tasks like writing basic advertising copy, first drafts of legal documents. Those are expert skills, and there is no question that software will make them cheaper and therefore devalue human labor,\" Autor said.\nMedia planning and buying\nCreative industries are likely to be affected, too. Noted advertising executive Sir Martin Sorrell, founder of WPP, the world's largest ad and PR group, said on a recent panel that he expects the way companies buy ad space will become automated \"in a highly effective way\" within five years.\n\"So you will not be dependent as a client on a 25-year old media planner or buyer, who has limited experience, but you'll be able to pool the data. That's the big change,\" he said.\nLegal functions\nChatGPT's abilities translate well to the legal profession, according to AI experts as well as legal professionals. In fact, ChatGPT's bot recently\npassed a law school exam\nand earned a passing grade after writing essays on topics ranging from constitutional law to taxation and torts.\n\"The dynamic that happens to lawyers now is there is way too much work to possibly get done, so they make an artificial distinction between what they will work on and what will be left to the wayside,\" said Jason Boehmig, co-founder and CEO of Ironclad, a legal software company.\nCommon legal forms and documents including home lease agreements, wills and nondisclosure agreements are fairly standard and can be drafted by a an advanced bot.\n05:51\n\"There are parts of a legal document that humans need to adapt to a particular situation, but 90% of the document is copy pasted,\" Netzer of Columbia Business School said. \"There is no reason why we would not have the machine write these kinds of legal documents. You may need to explain first in English the parameters, then the machine should be able to write it very well. The less creative you need to be, the more it should be replaced.\"\nChatGPT bot passes law school exam\n\"There aren't enough lawyers to do all the legal work corporations have,\" Boehmig added. \"The way attorneys work will be dramatically different. If I had to put a stake down around jobs that won't be there, I think it's attorneys who don't adapt to new ways of working over the next decade. There seem to be dividing lines around folks who don't want to change and folks who realize they have to.\"\nWork-Life Balance\nThese jobs are most likely to be replaced by ChatGPT and AI\nLayoff survivor guilt hits workers who remain after job cuts\nFind remote work isolating? Gig work offers some lessons to cope.\nAmericans are taking more control over their work lives  because they have to\nU.S. workers are more diverse than ever  but companies aren't keeping up\nMore\nFirst published on January 30, 2023 / 2:32 PM\n 2023 CBS Interactive Inc. All Rights Reserved.\nThanks for reading CBS NEWS.\nCreate your free account or log in\nfor more features."
  },
  {
    "body": "This line-up of stories will help you discover the latest happenings around the tech world, today\n1. Egypts MNT-Halan secures $400M in new raiser\nEgyptian fintech startup, MNT-Halan, has secured $400 million in equity and debt financing.\nThe digital bank, which was created to bank the unbanked, confirmed the funding in a statement on Wednesday.\nRipples Nigeria gathered that the investment included $140 million from two recent securitized bond issuances and $260 million in equity financing.\nPlaying in both finance and e-commerce ecosystem, the company said it seeks to continue to serve underbanked and unbanked customers in the North African country.\nAbout $200 million of the equity came from Chimera Investments, a company in Abu Dhabi.\nWe are thrilled to be part of Egypts greatest fintech success story, said Seif Fikry, CEO of Chimera Abu Dhabi, in a statement.\nMNT-Halan says the investments demonstrate continued confidence in its value proposition, management team, and superior technology.\nTech Trivia: What is another name for a portable computer?\nA. Desktop\nAnswer: see the end of post\n2. OpenAI launches tool to detect AI-generated text, including from ChatGPT\nOpenAI, the company behind ChatGPT, has developed a method to determine whether a text was produced by a human or AI-generated, including from ChatGPT.\nRead also:\nThe company revealed the development in a release on Wednesday.\nThe new tool comes barely two months after ChatGPT caught public attention.\nRipples Nigeria earlier reported that Chinese search giant, Baidu Inc., announced its intent to launch a new app to rival the American-owned AI tool.\nIn our evaluations on a challenge set of English texts, our classifier correctly identifies 26% of AI-written text (true positives) as likely AI-written, while incorrectly labeling human-written text as AI-written 9% of the time (false positives), OpenAI wrote.\nThe new OpenAI Text Classifier will, however, only function on content with at least 1,000 characters long, or 150250 words.\n3. Emerge partners Coca-Cola Al Ahlia Beverages on new solar project\nCoca-Cola Al Ahlia Beverages, Coca-bottler Colas distributor in the UAE, has struck an agreement with Emerge, a joint venture (JV) between Masdar of the UAE and EDF of France on a new solar project.\nMohamed Akeel, Chief Executive Officer of Coca-Cola Al Ahlia Beverages, confirmed the partnership in a statement seen by Ripples Nigeria on Wednesday.\nRipples Nigeria understands that the partnership is expected to birth a 1.8-megawatt (MWp) solar photovoltaic (PV) plant for its Al Ain facility.\nOur agreement with Emerge will allow us to reach yet another sustainability milestone  a big aspect of which is the integration of more renewable energy into our operations, Akeel said.\nFor the 1.8-megawatt peak (MWp) project, Emerge will offer a complete turnkey solution, taking care of the design, procurement, building, operation, and maintenance of the plant for 25 years.\nTrivia Answer: Laptop\nLaptop computers, also known as notebooks, are portable computers you can take with you and use in different environments. They include a screen, keyboard, and a trackpad or trackball, which serves as the mouse\nBy Kayode Hamsat"
  },
  {
    "body": "ChatGPT owner rolls out imperfect tool to detect AI-generated text\nFebruary 2, 2023 | 12:02 am\nBW FILE PHOTO\nOPENAI, the creator of the popular chatbot ChatGPT, has released a software tool to identify text generated by artificial intelligence (AI), the company said in a blog post on Wednesday.\nChatGPT is a free program that generates text in response to a prompt, including articles, essays, jokes and even poetry, which has gained wide popularity since its debut in November, while raising concerns about copyright and plagiarism.\nThe AI classifier, a language model trained on the dataset of pairs of human-written and AI-written text on the same topic, aims to distinguish text that is written by AI. It uses a variety of providers to address issues such as automated misinformation campaigns and academic dishonesty, the company said.\nIn its public beta mode, OpenAI acknowledges the detection tool is very unreliable on texts under 1,000 characters, and AI-written text can be edited to trick the classifier.\nWere making this classifier publicly available to get feedback on whether imperfect tools like this one are useful, OpenAI said.\nWe recognize that identifying AI-written text has been an important point of discussion among educators, and equally alimits and impacts of AI generated text classifiers in the classroom.\nSince ChatGPT debuted in November and gained wide popularity among millions of users, some of the largest US school districts, including New York City, have banned the AI chatbot over concerns that students will use the text generator to cheat or plagiarize.\nOthers have created third-party detection tools including GPTZeroX to help educators detect AI-generated text.\nOpenAI said it is engaging with educators to discuss ChatGPTs capabilities and limitations, and will continue to work on the detection of AI-generated text. \nReuters"
  },
  {
    "body": "ChatGPT is an AI language model, that is not perfect and may sometimes produce incorrect answers or nonsensical outputs. Hence, it's crucial to review, edit or process the model's results and always consider the context.\r\n\r\nTo optimize ChatGPT's performance, it's essential to ask clear questions, fine-tune it for a specific purpose, and utilize it in conjunction with other techniques. With proper usage and post-processing, ChatGPT can be an effective tool in any NLP task.\r\n\r\nHighlighted below are some tips on how to get better answers with ChatGPT.\r\n\r\nProvide context:Providing ChatGPT with additional context in your requests leads to more concise answers. ChatGPT is efficient and delivers the most accurate response when you pose a clear question. By giving some background information, ChatGPT adapts its response to match what you're looking for. The more context and details you offer, the better the model will comprehend and respond accordingly.\r\n\r\nFor example, instead of simply asking What is the weather like today?. You could ask What is the current temperature and forecast for Los Angeles, California?.\r\n\r\nMake your request precise:When posing a question to ChatGPT, try to be as direct and precise as you can while also providing background information. Your query should contain as many details as are required. ChatGPT will take into account each signal you send and each clarification you offer to come up with a response that is exactly what you asked for.\r\n\r\nAsk it to elaborate:Sometimes ChatGPT doesn't produce specific responses. If you want to continue the conversation, don't be afraid to ask the chatbot to expand on or complete its initial response. Then ChatGPT will continue by enhancing its initial response with new information.\r\n\r\nAsk ChatGPT to reformulate responses:If you are not satisfied with ChatGPT's initial response, you can request that it be reformulated. This can be done without altering your original request. Additionally, you only need to request that he respond using straightforward language, refrain from using formulations that you find to be overly complicated, or even respond as if you were a child. The chatbot will resume speaking as per your instructions and revise its previous speech to make it clearer.\r\n\r\nAsk it to format replies in a specific manner:In addition to being an almost infinite source of knowledge, ChatGPT can alter its speech by adopting a specific language style. For example, you could ask for it to mimic the writing style of a famous person or author. Chabot can also change the responses by producing things like tables and bulleted lists."
  },
  {
    "body": "ChatGPT, the AI chatbot that has blown up over the last few months, is going premium. After confirming a few weeks ago that it would test a paid plan, OpenAI has rolled out ChatGPT Plus.\nAlthough people will still be able to use the chatbot for free, there are of course some perks that come with a ChatGPT Plus plan. OpenAI says subscribers will have general access to the chatbot, even during peak times. They'll also get faster response times from ChatGPT, along with priority access to upgrades and new features.\nThe paid service is available in the US for now. OpenAI plans to gradually invite people from the ChatGPT Plus waitlist to sign up for the service over the coming months. The company is also looking to open up the plan to folks in other countries and regions in the near future.\nOpenAI says that the premium plan will help to keep ChatGPT free for anyone to use. In addition, it's exploring cheaper subscriptions, as well as business plans and data packs to increase availability.\n\"We launched ChatGPT as a research preview so we could learn more about the systems strengths and weaknesses and gather user feedback to help us improve upon its limitations,\" OpenAI wrote in a blog post. \"Since then, millions of people have given us feedback, weve made several important updates and weve seen users find value across a range of professional use-cases, including drafting and editing content, brainstorming ideas, programming help and learning new topics.\"\nOpenAI has found some other ways to bring in funding. Last month, Microsoft announced it's making a multibillion dollar investment in the company and it plans to use OpenAI's tech more broadly across its products. Google, meanwhile, has reportedly focused heavily on its AI work after the emergence of ChatGPT. It's said to be preparing its own chatbot and at least 20 other AI-powered products to show off this year."
  },
  {
    "body": "Parents abandon their ticketless baby at Israeli airport check-in\nThe new AI Text Classifier launched Tuesday by OpenAI follows a weeks-long discussion at schools and colleges over fears that ChatGPTs ability to write just about anything on command could fuel academic dishonesty and hinder learning.\nOpenAI cautions that its new tool  like others already available  is not foolproof. The method for detecting AI-written text is imperfect and it will be wrong sometimes, said Jan Leike, head of OpenAIs alignment team tasked to make its systems safer.\nBecause of that, it shouldnt be solely relied upon when making decisions, Leike said.\nStory continues below advertisement\nTeenagers and college students were among the millions of people who began experimenting with ChatGPT after it launched Nov. 30 as a free application on OpenAIs website. And while many found ways to use it creatively and harmlessly, the ease with which it could answer take-home test questions and assist with other assignments sparked a panic among some educators.\n2:04\nUniversity of Calgary AI project asks students and teachers about the use of ChatGPT\nBy the time schools opened for the new year, New York City, Los Angeles and other big public school districts began to block its use in classrooms and on school devices.\nThe Seattle Public Schools district initially blocked ChatGPT on all school devices in December but then opened access to educators who want to use it as a teaching tool, said Tim Robinson, the district spokesman.\nWe cant afford to ignore it, Robinson said.\nStory continues below advertisement\nThe district is also discussing possibly expanding the use of ChatGPT into classrooms to let teachers use it to train students to be better critical thinkers and to let students use the application as a personal tutor or to help generate new ideas when working on an assignment, Robinson said.\nSchool districts around the country say they are seeing the conversation around ChatGPT evolve quickly.\nThe initial reaction was `OMG, how are we going to stem the tide of all the cheating that will happen with ChatGPT, said Devin Page, a technology specialist with the Calvert County Public School District in Maryland. Now there is a growing realization that this is the future and blocking it is not the solution, he said.\nTrending Now\nNestle to chop, slice and freeze sales of Delissio, Lean Cuisine in Canada\nI think we would be naive if we were not aware of the dangers this tool poses, but we also would fail to serve our students if we ban them and us from using it for all its potential power, said Page, who thinks districts like his own will eventually unblock ChatGPT, especially once the companys detection service is in place.\nOpenAI emphasized the limitations of its detection tool in a blog post Tuesday, but said that in addition to deterring plagiarism, it could help to detect automated disinformation campaigns and other misuse of AI to mimic humans.\nStory continues below advertisement\nThe longer a passage of text, the better the tool is at detecting if an AI or human wrote something. Type in any text  a college admissions essay, or a literary analysis of Ralph Ellisons Invisible Man  and the tool will label it as either very unlikely, unlikely, unclear if it is, possibly, or likely AI-generated.\nBut much like ChatGPT itself, which was trained on a huge trove of digitized books, newspapers and online writings but often confidently spits out falsehoods or nonsense, its not easy to interpret how it came up with a result.\nWe dont fundamentally know what kind of pattern it pays attention to, or how it works internally, Leike said. Theres really not much we could say at this point about how the classifier actually works.\n2:03\nAnswering with AI: How ChatGPT is shaking up online information searches\nHigher education institutions around the world also have begun debating responsible use of AI technology. Sciences Po, one of Frances most prestigious universities, prohibited its use last week and warned that anyone found surreptitiously using ChatGPT and other AI tools to produce written or oral work could be banned from Sciences Po and other institutions.\nStory continues below advertisement\nIn response to the backlash, OpenAI said it has been working for several weeks to craft new guidelines to help educators.\nLike many other technologies, it may be that one district decides that its inappropriate for use in their classrooms, said OpenAI policy researcher Lama Ahmad. We dont really push them one way or another. We just want to give them the information that they need to be able to make the right decisions for them.\nRead more:\nYoung couple who danced in viral video handed lengthy jail sentence in Iran\nIts an unusually public role for the research-oriented San Francisco startup, now backed by billions of dollars in investment from its partner Microsoft and facing growing interest from the public and governments.\nFrances digital economy minister Jean-Noel Barrot recently met in California with OpenAI executives, including CEO Sam Altman, and a week later told an audience at the World Economic Forum in Davos, Switzerland that he was optimistic about the technology. But the government minister  a former professor at the Massachusetts Institute of Technology and the French business school HEC in Paris  said there are also difficult ethical questions that will need to be addressed.\nRead more:\nWill winter end soon? Canadian groundhogs split on spring calls\nStory continues below advertisement\nSo if youre in the law faculty, there is room for concern because obviously ChatGPT, among other tools, will be able to deliver exams that are relatively impressive, he said. If you are in the economics faculty, then youre fine because ChatGPT will have a hard time finding or delivering something that is expected when you are in a graduate-level economics faculty.\nHe said it will be increasingly important for users to understand the basics of how these systems work so they know what biases might exist.\n OBrien reported from Providence, Rhode Island. AP writer John Leicester contributed to this report from Paris."
  },
  {
    "body": "The ChatGPT team has a 68-page paper that describes their training language models follow instructions with human feedback.\nHuman labelers rank the ChatGPT outputs from best to worst. The result is a new labeled dataset, where the rankings are the labels. The size of this dataset is approximately 10 times bigger than the curated dataset used for the SFT model.\nAbstract\nMaking language models bigger does not inherently make them better at following a users intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.\n#Reinforcement Learning from Human Feedback\nThe method overall consists of three distinct steps:\n1. Supervised fine-tuning step: a pre-trained language model is fine-tuned on a relatively small amount of demonstration data curated by labelers, to learn a supervised policy (the SFT model) that generates outputs from a selected list of prompts. This represents the baseline model.\n2. Mimic human preferences step: labelers are asked to vote on a relatively large number of the SFT model outputs, this way creating a new dataset consisting of comparison data. A new model is trained on this dataset. This is referred to as the reward model (RM).\n3. Proximal Policy Optimization (PPO) step: the reward model is used to further fine-tune and improve the SFT model. The outcome of this step is the so-called policy model.\nLex Fridman on ChatGPT\nChatGPT 3 came out about two years ago and it was like impressive but dumb in a lot of ways it was like you would expect as a human being for it to generate certain kinds of text and it was like saying kind of dumb things that were off and youre like all right this is really impressive but its not quite there.\nWhat they did with GPT 3.5 is they started adding more and different kinds of data sets there one of them probably the smartest neural network currently is codex which is fine-tuned for programming like it was it was trained on code on programming code and when you train a programming code which chatGPT is also youre teaching it  something like reasoning because its no longer information and knowledge from the Internet its also reasoning like logic even though youre looking at code programming code is youre looking at me like what the [ __ ] is he talking about no no no no thats not what Im looking at Im looking at you like oh my God but reasoning is a in order to b able to stitch together sentences that make sense you not only need to know the facts.\nIt was fine-tuned in a supervised Way by human labeling small data set by human labeling of heres what we would like this network to do."
  },
  {
    "body": "Copy article link\nSave\nChatGPT is the latest artificial intelligence disrupter. It provides informed answers to questions, generating text with the capability of creating essays around topics. The tool employs machine learning to produce such text and mimic human conversation.\nLike many new AI tools, there will be enthusiastic supporters and cautious detractors. Some of the most vociferous critics are coming out of higher education.\nChatGPT poses a threat to many college educators, particularly those in the social sciences and liberal arts, where writing is core to their education and evaluation processes. Even those in the science, technology, engineering and math fields may be affected, given how ChatGPT has demonstrated its ability to solve problems.\nCollege students are taught to logically present and communicate their positions using essays. Their writings are then evaluated to determine how well a student has performed and, in effect, assign the student a grade.\nIf ChatGPT can create such essays, then how can students be evaluated and graded? Under the current model, they cannot.\nThis means that alternative ways of evaluating and grading students must be formulated -- not to neutralize ChatGPT but to embrace it.\nThis is not the first time that technology has disrupted education.\nIn the 1970s, handheld electronic calculators became more affordable, and hence, more ubiquitous. This made the teaching of arithmetic calculations obsolete, though not the rules by which they are executed.\nInstead of fighting the use of calculators, mathematics education transformed, effectively embracing problem formulation and problem solving. This allowed students to consider real-world problems in which the answers could be computed using calculators, provided the problem was correctly formulated and the appropriate techniques were applied.\nCalculators expanded the scope of problems that could be tackled. This technological transformation meant that the process rather than the outcome became the target of student evaluation.\nA similar transformation will be needed when essays provided by ChatGPT are to be evaluated.\nChatGPT creates text, but the genesis of such text is existing ideas.\nChatGPT is also not flawless. It can extract incorrect information and regurgitate it. Unlike people, it cannot reason and employ critical thinking.\nWhat people bring to the table are ideas. This creates a much higher bar for evaluation, with the essay style and structure provided by ChatGPT serving as a first draft. Students can then imbue their own creative content to enhance what ChatGPT initially provided.\nThis means that grammar and punctuation are likely to be flawless with ChatGPT. What will be far less impressive is the novelty embedded in the content. This makes ChatGPT strong on style and sizzle and more limited on substance.\nChatGPT will not be the last text creator available. Indeed, new and improved products are likely to be introduced in the coming years.\nFor those intent on holding onto existing methods to evaluate students, they will be swimming upstream against a strong current. However, if AI tools can create text, then AI tools will become available to detect AI, much like using fire to fight fire. Such tools would effectively uncover a writings DNA to assess its origins. This is the foundation for the Turing Test originally proposed to assess whether a computing device can appear to think like a person.\nStopping an AI advance in this manner is a short-term stopgap for what is likely to be a long-term trend.\nAt the root of many of the criticisms is fear. Any new tool has the potential to affect peoples lives and livelihoods. Such concerns are likely ill-founded and most certainly premature.\nWill ChatGPT replace college instructors? Hardly. What it could do is improve their ability to educate. It may also expose those who are ineffective.\nStudents who rely totally on ChatGPT or similar AI systems for writing essays may find that their beautifully composed writing, with appropriate scrutiny, will be exposed for its lack of depth and creativity. On the other hand, students could use ChatGPT to learn about a topic and then take advantage of its output as a launchpad and imbue it with their own ideas. This will make the final product better than what they could have composed on their own. In this way, ChatGPT improves the initial conditions. This also means that the bar for success must be similarly moved higher.\nIf I composed an exam on which ChatGPT could score a perfect grade, the problem may be my exam, not ChatGPT. Perhaps my goal should be to compose an exam that ChatGPT could not successfully tackle.\nThe ability for ChatGPT to pass medical exams such as the U.S. Medical Licensing Exam exposes these exams for highlighting recollection of facts, not for innovative thinking, feeding to ChatGPTs strength.\nChatGPT is a game changer and disrupter. It will enable better writing and, if embraced, better thinking, something that all educators should support. The question is whether higher education will accept this change and allow it to be a springboard for better written communication.\nSheldon H. Jacobson is a professor in computer science at the University of Illinois at Urbana-Champaign. As a data scientist and operations researcher, he applies his expertise in data-driven, risk-based decision-making to evaluate and inform public policy.\n(C)2023 Chicago Tribune. Visit at chicagotribune.com. Distributed by Tribune Content Agency.\nAs an Amazon Associate I earn from qualifying purchases.\nTribune Wire"
  },
  {
    "body": "February 01, 2023, 15:35 GMT\nShare This Article\nTORONTO, Feb.  01, 2023  (GLOBE NEWSWIRE) -- DiagnaMed Holdings Corp. (\"DiagnaMed\" or the \"Company\")\nDMED\n, an artificial intelligence-driven digital health company focusing on improving brain health, today announces that it will incorporate OpenAI's technology, ChatGPT, in its BRAIN AGE Artificial Intelligence (\"AI\") Product Suite, which already includes a variety of API interfaces to build AI and machine learning functions, to further enhance the Company's novel Brain Health Digital Platform to assess overall brain health and presence of cognitive impairment in athletes, and those diagnosed with a mental health and neurodegenerative condition.\nDiagnaMed's Brain Health Digital Platform comprises several assessment tools generated rapidly by self-report or clinician-observed measures, each capturing different aspects of brain health to evaluate overall brain health, output a patient risk score, and provide actionable points to develop precision medicine-like, personalized treatment plans and interventions. As part of the BRAIN AGE AI Product Suite, the Brain Health Platform will also include unique brain training, games, education, and coaching applications. The Company expects to launch the Brain Health Digital Platform as a general clinical tool in May 2023.\n\"Our Brain Health Digital Platform, which is part of our BRAIN AGE AI product suite, offers licensed professionals a novel brain health assessment tool, patient risk score, and helpful intervention plans and techniques, incorporating OpentAI's technology, ChatGPT, with the aim to improve brain health and brain age of patients,\" said Fabio Chianelli, Chairman of DiagnaMed. \"We expect to launch the Brain Health Digital Platform in the U.S. and Canada in Q2-2023 for use in sports, mental health and neurology clinics that are committed to improving the brain health of over 50 million people with mental health and neurological disorders.\"\nAbout DiagnaMed Holdings Corp.\nDiagnaMed Holdings Corp.\nDMED\nis an artificial intelligence-driven digital health company focusing on improving brain health for mental health and neurodegenerative disorders. The Company is developing its BRAIN AGE AI Product Suite, which includes BRAIN AGE, an electroencephalogram-based (EEG) artificial intelligence solution to detect an individual's brain age, and the Brain Health Digital Platform for clinical and home use. Learn more at DiagnaMed.com.\nFor more information about DiagnaMed, please contact:\nFabio Chianelli\nNeither the Canadian Securities Exchange nor its Regulation Services Provider have reviewed or accept responsibility for the adequacy or accuracy of this release.\nCautionary Statement\nCertain statements in this news release are forward-looking statements, including with respect to future plans, and other matters. Forward-looking statements consist of statements that are not purely historical, including any statements regarding beliefs, plans, expectations or intentions regarding the future. Such information can generally be identified by the use of forwarding-looking wording such as \"may\", \"expect\", \"estimate\", \"anticipate\", \"intend\", \"believe\" and \"continue\" or the negative thereof or similar variations. The reader is cautioned that assumptions used in the preparation of any forward-looking information may prove to be incorrect. Events or circumstances may cause actual results to differ materially from those predicted, as a result of numerous known and unknown risks, uncertainties, and other factors, many of which are beyond the control of the Company, including but not limited to, business, economic and capital market conditions, the ability to manage operating expenses, and dependence on key personnel. Such statements and information are based on numerous assumptions regarding present and future business strategies and the environment in which the Company will operate in the future, anticipated costs, and the ability to achieve goals. Factors that could cause the actual results to differ materially from those in forward-looking statements include, the continued availability of capital and financing, litigation, failure of counterparties to perform their contractual obligations, loss of key employees and consultants, and general economic, market or business conditions. Factors that could cause actual results to differ materially from those anticipated in these forward-looking statements are described under the caption \"Risk Factors\" in DiagnaMed's final prospectus dated October 26, 2022, which is available on the Company's profile at www.sedar.com. Forward-looking statements contained in this news release are expressly qualified by this cautionary statement. The reader is cautioned not to place undue reliance on any forward-looking information. The forward-looking statements contained in this news release are made as of the date of this news release. Except as required by law, the Company disclaims any intention and assumes no obligation to update or revise any forward-looking statements, whether as a result of new information, future events or otherwise.\nThis news release does not constitute an offer to sell or the solicitation of an offer to buy, and shall not constitute an offer, solicitation or sale in any state, province, territory or jurisdiction in which such offer, solicitation or sale would be unlawful prior to registration or qualification under the securities laws of any such state, province, territory or jurisdiction.\nYou just read:\nDiagnaMed Holdings Incorporates OpenAI's ChatGPT into BRAIN AGE AI Product Suite\nNews Provided By\nFebruary 01, 2023, 15:35 GMT\nShare This Article\nDistribution channels:        Banking, Finance & Investment Industry\nEIN Presswire's priority is source transparency. We do not allow opaque clients, and our editors try to be careful about weeding out false and misleading content.As a user, if you see something we have missed, please do bring it to our attention. Your help is welcome. EIN Presswire, Everyone's Internet News Presswire,tries to define some of the boundaries that are reasonable in today's world. Please see our"
  },
  {
    "body": "GPT-4 is one of the most anticipated technology product releases in years.\nAD\nIts planned incorporation into Microsoft products will likely set off new competition in internet search: Something that has not happened since Google supplanted Yahoo as the most popular search engine.\nThe Information first reported Microsofts plans to use OpenAIs underlying technology.\nNow that OpenAI has released ChatGPT to the public, the technology will likely improve on a faster curve. And having a mobile app, instead of just a web interface, will bring an even broader user base into the service, making the curve steeper.\nThats because the information users input into the system serves as a way to improve the product. Each query serves as a form of feedback.\nFor instance, each ChatGPT answer includes thumbs up and thumbs down buttons. A popup window prompts users to write down the ideal answer, helping the software learn from its mistakes.\nAD\nGPT stands for Generative pre-trained transformer, which is the underlying technology that powers ChatGPT and is characterized by its ability to parse through a vast dataset. For that reason, it is often called a large language model. The latest version of OpenAIs software is called GPT-3.\nThe most interesting improvement in the latest version described by sources is GPT-4s speed. Right now, it can take a while  sometimes minutes in my experience  for ChatGPT to answer.\nOne of the least-appreciated innovations of ChatGPT is what happens on the server every time someone enters a query.\nIn May, 2020, Microsoft announced it had built one of the top five publicly disclosed supercomputers in the world  in partnership and exclusively for OpenAI  that would be used to train extremely large artificial intelligence models.\nIts now clear what possibilities Microsoft and OpenAI saw in this technology that has caused the whole world to take notice.\nAD\nBut running these algorithms, which scour billions of pages of text, is expensive. Each ChatGPT search costs around a couple cents, according to Sam Altman, OpenAIs CEO.\nThe supercomputer Microsoft and OpenAI built had 285,000 CPU cores and 10,000 GPUs, each with 400 gigabits per second of network connectivity.\nThe rush to capitalize on the generative AI craze that OpenAI and Microsoft started has made it hard for some companies to get their hands on Nvidias latest graphics processor, the H100 Tensor Core GPU, that is specifically designed to run transformers like those used in GPT. (I found one for sale online for $30,000.)\nThat shows just how important innovations on the server side will be to AI products rolling out. A big part of improving OpenAI will be figuring out how to run it more quickly and more cost-effectively.\nAll these improvements mean OpenAI is building a moat around its product. Anyone can build software similar to OpenAIs. In fact, the transformers were first invented by Google and were made open source.\nBut only a handful of companies have access to massive supercomputers capable of running billions of parameters.\nAnd now that OpenAI has opened its products up to the public, its getting invaluable real-world feedback that will help keep its products ahead of competitors.\nAs we reported last week, it has also hired contractors - including about 400 computer programmers - around the world, who are working to improve the companys tools that automate software development.\nAD"
  },
  {
    "body": "NurPhoto/Getty Images\n\nChatGPT has been making headlines worldwide, but not all are impressed. Yann LeCun, Meta's chief artificial intelligence (AI) scientist, had some harsh words for the program in an hour-and-a-half talk hosted by the Collective[i] Forecast. This online, interactive discussion series is organized by Collective[i].\nWhat exactly did he have to say? ZDNET attended the session and reported on it.\nSee Also\n\"In terms of underlying techniques, ChatGPT is not particularly innovative,\" said LeCun on Zoom last week.\n\"It's nothing revolutionary, although that's the way it's perceived in the public,\" said LeCun. \"It's just that, you know, it's well put together, it's nicely done.\"\nIs ChatGPT overrated?\nHe noted that such systems had been built before by other companies; they simply hadnt gained the popularity of ChatGPT.\n\"OpenAI is not particularly an advance compared to the other labs, at all,\" explained LeCun.\n\"It's not only just Google and Meta, but there are half a dozen startups that basically have very similar technology to it,\" said LeCun. \"I don't want to say it's not rocket science, but it's really shared, there's no secret behind it if you will.\"\nIn many ways, ChatGPT was a group effort as it brought together multiple pieces of technology developed over many years by many parties.\n\"You have to realize, ChatGPT uses Transformer architectures that are pre-trained in this self-supervised manner,\" said LeCun. \"Self-supervised learning is something I've been advocating for a long time, even before OpenAI existed.\n @Sama (CEO of"
  },
  {
    "body": "Brain Health Digital Platform to be available in May 2023\n/\nEIN News\n/ -- TORONTO, Feb.  01, 2023  (GLOBE NEWSWIRE) -- DiagnaMed Holdings Corp. (DiagnaMed or the Company) (CSE: DMED), an artificial intelligence-driven digital health company focusing on improving brain health, today announces that it will incorporate OpenAIs technology, ChatGPT, in its BRAIN AGE Artificial Intelligence (AI) Product Suite, which already includes a variety of API interfaces to build AI and machine learning functions, to further enhance the Companys novel Brain Health Digital Platform to assess overall brain health and presence of cognitive impairment in athletes, and those diagnosed with a mental health and neurodegenerative condition.\nDiagnaMeds Brain Health Digital Platform comprises several assessment tools generated rapidly by self-report or clinician-observed measures, each capturing different aspects of brain health to evaluate overall brain health, output a patient risk score, and provide actionable points to develop precision medicine-like, personalized treatment plans and interventions. As part of the BRAIN AGE AI Product Suite, the Brain Health Platform will also include unique brain training, games, education, and coaching applications. The Company expects to launch the Brain Health Digital Platform as a general clinical tool in May 2023.\nOur Brain Health Digital Platform, which is part of our BRAIN AGE AI product suite, offers licensed professionals a novel brain health assessment tool, patient risk score, and helpful intervention plans and techniques, incorporating OpentAIs technology, ChatGPT, with the aim to improve brain health and brain age of patients, said Fabio Chianelli, Chairman of DiagnaMed. We expect to launch the Brain Health Digital Platform in the U.S. and Canada in Q2-2023 for use in sports, mental health and neurology clinics that are committed to improving the brain health of over 50 million people with mental health and neurological disorders.\nAbout DiagnaMed Holdings Corp.\nDiagnaMed Holdings Corp. (CSE: DMED) is an artificial intelligence-driven digital health company focusing on improving brain health for mental health and neurodegenerative disorders. The Company is developing its BRAIN AGE AI Product Suite, which includes BRAIN AGE, an electroencephalogram-based (EEG) artificial intelligence solution to detect an individuals brain age, and the Brain Health Digital Platform for clinical and home use. Learn more at DiagnaMed.com.\nFor more information about DiagnaMed, please contact:\nFabio Chianelli\nNeither the Canadian Securities Exchange nor its Regulation Services Provider have reviewed or accept responsibility for the adequacy or accuracy of this release.\nCautionary Statement\nCertain statements in this news release are forward-looking statements, including with respect to future plans, and other matters. Forward-looking statements consist of statements that are not purely historical, including any statements regarding beliefs, plans, expectations or intentions regarding the future. Such information can generally be identified by the use of forwarding-looking wording such as may, expect, estimate, anticipate, intend, believe and continue or the negative thereof or similar variations. The reader is cautioned that assumptions used in the preparation of any forward-looking information may prove to be incorrect. Events or circumstances may cause actual results to differ materially from those predicted, as a result of numerous known and unknown risks, uncertainties, and other factors, many of which are beyond the control of the Company, including but not limited to, business, economic and capital market conditions, the ability to manage operating expenses, and dependence on key personnel. Such statements and information are based on numerous assumptions regarding present and future business strategies and the environment in which the Company will operate in the future, anticipated costs, and the ability to achieve goals. Factors that could cause the actual results to differ materially from those in forward-looking statements include, the continued availability of capital and financing, litigation, failure of counterparties to perform their contractual obligations, loss of key employees and consultants, and general economic, market or business conditions. Factors that could cause actual results to differ materially from those anticipated in these forward-looking statements are described under the caption Risk Factors in DiagnaMed's final prospectus dated October 26, 2022, which is available on the Company's profile at www.sedar.com. Forward-looking statements contained in this news release are expressly qualified by this cautionary statement. The reader is cautioned not to place undue reliance on any forward-looking information. The forward-looking statements contained in this news release are made as of the date of this news release. Except as required by law, the Company disclaims any intention and assumes no obligation to update or revise any forward-looking statements, whether as a result of new information, future events or otherwise.\nThis news release does not constitute an offer to sell or the solicitation of an offer to buy, and shall not constitute an offer, solicitation or sale in any state, province, territory or jurisdiction in which such offer, solicitation or sale would be unlawful prior to registration or qualification under the securities laws of any such state, province, territory or jurisdiction.\nYou just read:"
  },
  {
    "body": "December 1, 2022\nBuchheit added his predictions for exactly how he thought such developments would go. First: The URL/Search bar of the browser gets replaced with AI that autocompletes my thought/question as I type it while also providing the best answer (which may be a link to a website or product).\nHe added that the old search engine backend will be used by the AI to gather relevant information and links, which will then be summarized for the user. Its like asking a professional human researcher to do the work, except the AI will instantly do what would take many minutes for a human.\nBuchheit compared it to the decline in use of the yellow pages, which was once the go-to directory for businesses and contact details but was replaced by Google.\nChange is already happening\nThe former Google developer pointed out that the ubiquitous search engine is so shaken by the chatbots success, especially since that search page is where websites such as Google make most of their money, that it now has over 20 different A.I. projects in the works.\nIn December, it was reported that Google had issued a code red, and since then Google has already launched an A.I. tool that transforms hums into actual music.\nWhoa, this is bigger than ChatGPT to me. Google almost solved music generation, Id say, tweeted Keunwoo Choi, an A.I. scientist at Gaudio Lab, an A.I. audio technology company.\nGoogle isnt the only tech giant thinking about thisOpenAI has also\nwith Microsoft that will see ChatGPT incorporated into the Bing search engine.\nIt appears Buchheits predictions are already starting to come true.\nLearn how to navigate and strengthen trust in your business with The Trust Factor, a weekly newsletter examining what leaders need to succeed."
  },
  {
    "body": "Sam Altman, CEO of OpenAI.\nPhoto by Kevin Dietsch/Getty Images\nPeter Kafka        covers media and technology, and their intersection, at Vox. Many of his stories can be found in his Kafka on Media newsletter, and he also hosts the Recode Media podcast.\nThis story is part of a group of stories called\nUncovering and explaining how our digital world is changing  and changing us.\nThat was quick: Artificial intelligence has gone from science fiction to novelty to Thing We Are Sure Is the Future. Very, very fast.\nOne easy way to measure the change is via headlines  like the ones announcing Microsofts $10 billion investment in OpenAI, the company behind the dazzling ChatGPT text generator, followed by other AI startups looking for big money. Or the ones about school districts frantically trying to cope with students using ChatGPT to write their term papers. Or the ones about digital publishers like CNET and BuzzFeed admitting or bragging that theyre using AI to make some of their content  and investors rewarding them for it.\nUp until very recently, these were science experiments nobody cared about, says Mathew Dryhurst, co-founder of the AI startup Spawning.ai. In a short period of time, [they] became projects of economic consequence.\nThen theres another leading indicator: lawsuits lodged against OpenAI and similar companies, which argue that AI engines are illegally using other peoples work to build their platforms and products. This means they are aimed directly at the current boom of  generative AI  software, like ChatGPT, that uses existing text or images or code to create new work.\nSign up for the                  newsletter\nKafka on Media\n(required)\nOops. Something went wrong. Please enter a valid email and try again.\nBy submitting your email, you agree to our Terms and Privacy Notice. You can opt out at any time. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.\nSubscribe\nLast fall, a group of anonymous copyright owners sued Open AI and Microsoft, which owns the GitHub software platform, for allegedly infringing on the rights of developers whove contributed software to GitHub. Microsoft and OpenAI collaborated to build GitHub Copilot, which says it can use AI to write code.\nAnd in January, we saw a similar class-action suit filed (by the\nsame attorneys\n) against Stability AI, the developer of the AI art generator Stable Diffusion, alleging copyright violations. Meanwhile, Getty Images, the UK-based photo and art library, says it will also sue Stable Diffusion for using its images without a license.\nIts easy to reflexively dismiss legal filings as an inevitable marker of a tech boom  if theres hype and money, lawyers are going to follow. But there are genuinely interesting questions at play here  about the nature of intellectual property and the pros and cons of driving full speed into a new tech landscape before anyone knows the rules of the road. Yes, generative AI now seems inevitable. These fights could shape how we use it and how it affects business and culture.\nWe have seen versions of this story play out before. Ask the music industry, which spent years grappling with the shift from CDs to digital tunes, or book publishers who railed against Googles move to digitize books.\nThe AI boom is going to trigger a common reaction among people we think of as creators: My stuff is being stolen, says Lawrence Lessig, the Harvard law professor who spent years fighting against music labels during the original Napster era, when he argued that music owners were using copyright rules to quash creativity.\nIn the early 2000s, tussles over digital rights and copyrights were a sidelight, of concern to a relatively small slice of the population. But now everyone is online  which means that even if you dont consider yourself a creator, stuff you write or share could become part of an AI engine and used in ways youd never imagine.\nAnd the tech giants leading the charge into AI  in addition to Microsoft, both Google and Facebook have made enormous investments in the industry, even if they have yet to bring much of it in front of the public  are much more powerful and entrenched than their dot-com boom counterparts. Which means they have more to lose from a courtroom challenge, and they have the resources to fight and delay legal consequences until those consequences are beside the point.\nAIs data-fueled diet\nThe tech behind AI is a complicated black box, and many of the claims and predictions about its power may be overstated. Yes, some AI software seems to be able to pass parts of MBA and medical licensing tests, but theyre not going to replace your doctor or CFO quite yet. They are also not sentient, despite what a befuddled Googler might have said.\nBut the basic idea is relatively straightforward: Engines like the ones built by OpenAI ingest giant data sets, which they use to train software that can make recommendations or even generate code, art, or text.\nIn many cases, the engines are scouring the web for these data sets, the same way Googles search crawlers do, so they can learn whats on a webpage and catalog it for search queries. In some cases, such as Meta, AI engines have access to huge proprietary data sets built in part by the text, photos, and videos their own users have posted on their platforms  though a Meta spokesperson says that data is used to help refine recommendations, not to build AI products like a ChatGPT-esque engine. Other times, the engines will also license data, like Meta and OpenAI have done with the photo library Shutterstock.\nUnlike the music piracy lawsuits at the turn of the century, no one is arguing that AI engines are making bit-for-bit copies of the data they use and distributing them under the same name. The legal issues, for now, tend to be about how the data got into the engines in the first place and who has the right to use that data.\nAI proponents argue that 1) engines can learn from existing data sets without permission because theres no law against learning, and 2) turning one set of data  even if you dont own it  into something entirely different is protected by the law, affirmed by a lengthy court fight that Google won against authors and publishers who sued the company over its book index, which cataloged and excerpted a huge swath of books.\nThe arguments against the engines seem even simpler: Getty, for one, says it is happy to license its images to AI engines, but that Stable Diffusion builder Stability AI hasnt paid up. In the OpenAI/Microsoft/GitHub case, attorneys argue that Microsoft and OpenAI are violating the rights of developers whove contributed code to GitHub, by ignoring the open source software licenses that govern the commercial use of that code.\nAnd in the Stability AI lawsuit, those same lawyers argue that the image engine really is making copies of artists work, even if the output isnt a mirror image of the original. And that their own output competes with the artists ability to "
  },
  {
    "body": "The announcement came amid intense debate at schools and universities in the United States and around the world over concerns that the software can be used to assist students with assignments and help them cheat during exams.US-based OpenAI said in a blog post on Tuesday that its detection tool has been trained to distinguish between text written by a human and text written by AIs from a variety of providers.The bot from OpenAI, which recently received a massive cash injection from Microsoft, responds to simple prompts with reams of text inspired by data gathered on the internet.OpenAI cautioned that its tool can make mistakes, particularly with texts containing fewer than 1,000 characters.While it is impossible to reliably detect all AI-written text, we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human, OpenAI said in the post.For example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human.A top French university last week forbade students from using ChatGPT to complete assignments, in the first such ban at a college in the country.The decision came shortly after word that ChatGPT had passed exams at a US law school after writing essays on topics ranging from constitutional law to taxation.ChatGPT still makes factual mistakes, but education facilities have rushed to ban the AI tool.We recognise that identifying AI-written text has been an important point of discussion among educators, and equally important is recognizing the limits and impacts of AI generated text classifiers in the classroom, OpenAI said in the post.We are engaging with educators in the US to learn what they are seeing in their classrooms and to discuss ChatGPTs capabilities and limitations.Officials in New York and other jurisdictions have forbidden its use in schools.A group of Australian universities have said they would change exam formats to banish AI tools and regard them as cheating.OpenAI said it recommends using the classifier only with English text as it performs worse in other languages."
  },
  {
    "body": "Click to print (Opens in new window)\nA ChatGPT prompt is shown on a device near a public school in Brooklyn, New York, Thursday, Jan. 5, 2023. New York City school officials started blocking this week the impressive but controversial writing tool that can generate paragraphs of human-like text. (AP Photo/Peter Morgan)\nBy Doug McIntyre | Doug@DougMcIntyre.com | Daily News\nPUBLISHED:\nFebruary 1, 2023 at 7:00 a.m.\n| UPDATED:\nFebruary 1, 2023 at 9:25 a.m.\nArtificial intelligence (AI) has the potential to revolutionize the way we live and work, but it also poses significant risks to society. As we continue to develop and deploy AI systems, it is crucial that we understand and address the dangers they pose.\nOne of the major concerns about AI is its potential to displace human workers. As machines become more capable and efficient, they are likely to take over jobs currently done by humans, leading to widespread unemployment and economic inequality. Additionally, AI systems can perpetuate and amplify biases in the data they are trained on, leading to discriminatory outcomes.\nAnother concern is the potential for AI to be used in ways that are harmful to society. Autonomous weapons, for example, could be used to conduct warfare without human oversight, leading to unpredictable and potentially catastrophic consequences. AI-powered surveillance systems could be used to monitor and control populations, violating privacy and civil liberties.\nThere is also the risk that AI systems could become uncontrollable and pose a threat to humanity. As AI systems become more advanced, they may be able to outsmart their human creators and make decisions that are not in our best interests.\nNow, heres the really scary part  everything you have read up until now was written by something called ChatGPT, a new AI software program that threatens to upend life as we have known it.\nChatGPT (an acronym for Generative Pre-trained Transformer) is capable of rapidly processing large amounts of text in natural language, including highly personal and stylistic writing.\nHeres how I wrote todays column. I opened a free account at chat.openai.com. I typed in, Write an 800-word essay on the dangers of AI in the style of Doug McIntyre. In less than 60 seconds, the program wrote the opening seven paragraphs of todays column. Pretty amazing, huh?\nOf course, this is still the early stages of development. Think Google in 1998. That didnt stop Microsoft from investing $10 billion last month. ChatGPT is going to get bigger, faster and more sophisticated and very quickly. Still, my first dip into the AI pool failed in several key respects. First off, instead of 800 words, it gave me only 450, and I specifically requested in the style of Doug McIntyre. Instead, I got an essay with crisp, clean, grammatically correct sentences, no dumb attempts at humor or cheap shots at local elected officials and zero spelling errors. So, not my style at all.\nStill, wow!\nCan you imagine a school teacher or college professor who not only has to worry about the woke police coming for him/her/they, or getting shot by a lunatic, but now has ChatGPT to contend with? The days of plagiarizing a book report from Cliff Notes are over. I asked ChatGPT for a summary of John Steinbecks, The Grapes of Wrath. Thirty seconds later I had this:\nRelated Articles"
  },
  {
    "body": "LISTEN\n Peter Morgan/AP\nFrench university Sciences Po is one the first higher education institutions to ban the use of the artificial intelligence text generator ChatGPT to complete assignments, in order to prevent fraud and plagiarism. The company that created ChatGPT said it has released software that educators can use to detect AI-generated text.\nSciences Po sent a message to all students and faculty announcing a ban on all AI-based tools by students and faculty in the network, based in Paris, with campuses around France, including Lille, Toulouse and Grenoble.\nThe use of ChatGPT, or any other tool using AI, without the transparent referencing is strictly forbidden in the academic space, wrote provost Sergei Guriev.\nThe decision is a first for a French university, and Sciences Po said the penalty for using the tools would be expulsion from the institution\" or even a ban from French higher education as a whole, though it did not specify how the use of the tools would be detected.\nUniversities and schools around the world have been debating the use of ChatGPT, which was released in November and can be used to generate text in response to simple prompts that can be taken for human writing.\nIn an experiment, professors at the University of Minnesota Law School had the chatbot answer questions from final exams in several subjects, including constitutional law and taxation, and when graded alongside student answers, it consistently achieved low, but passing grades.\nAI cannot be ignored\nMany French students have been using ChatGPT to write homework assignments, but Guillaume Leboucher, who created the AI for school foundation, at the Institut de France, is not concerned.\nInstead of banning the use of ChatGPT, he would like educators to see it as a tool  a new generation of search engine that allows users to interact in simple, human text.\nThe interface has the potential to radically change our relationship with AI in our daily lives,\" he told RFI.\nWhile there are ethical issues raised by its use by students, we encourage teachers to have students use ChatGPT, to play with it, while warning them that it will not help them get better grades on their homework if they just cut and paste text generated by a chatbot.\nThe challenges related to the AI-based language generation tools will necessarily and rapidly change teaching and evaluation practices, Sciences Po provost Guriev wrote.\nWe know very well that it will be part of the digital ecosystem, Sciences Po's Directof of Studies, Myriam Dubois-Monkachi, told FranceInfo radio. \"How do we live with it?\nIn order to block students' use of the tools, some schools have announced plans to assign fewer take-home writing projects and have more oral and hand-written exams.\nUnreliable detection tools\nMany are looking for software tools to detect AI-generated text. Some third-party tools, including GPTZeroZ have already been created.\nOpenAI, the creator of ChatGTP, which has received billions of dollars of investment from Microsoft, released its own detection tool on Wednesday.\nIn a blog post, the company said that its AI classifier is a language model trained with pairs of human and AI written text on the same topic.\nWe recognise that identifying AI-written text has been an important point of discussion among educators, and equally important is recognising the limits and impacts of AI generated text classifiers in the classroom.\"\nThe company said the classifier is unreliable, particularly on short texts of fewer than 1,000 characters.\nIt also says the classifier is not as effective on non-English languages, making it of limited use for schools like Sciences Po.\n\"We're making this classifier publicly available to get feedback on whether imperfect tools like this one are useful,\" OpenAI said."
  },
  {
    "body": "FBI searches Biden's Rehoboth home in connection to documents probe\nTom Brady says he's retiring from the NFL \"for good\"\nHow to watch Tyre Nichols' funeral service\nIce storm cancels flights, leaves 250K without power in Texas\nGOP-led states ask judge to end DACA policy for \"Dreamers\"\nCDC tells consumers to stop using Ezricare Artificial Tears\nNew York City finally sees measurable snow after long wait\nNikki Haley to announce 2024 presidential run on Feb. 15\nBeyonc announces Renaissance world tour\nShows"
  },
  {
    "body": ", the AI content tool thats taken the world by storm in recent months, is switching to a freemium model, with a new paid tier, priced at $20 per month, which will give users better access to the tool.\nAs explained by ChatGPT maker\n:\n\nThe new subscription plan, ChatGPT Plus, will be available for $20/month, and subscribers will receive a number of benefits:\nGeneral access to ChatGPT, even during peak times\nFaster response times\nPriority access to new features and improvements\nA version of the ChatGPT interface will remain free to use, but you may be subject to delays at peak usage times and/or outages depending on circumstance.\nChatGPT, which can generate custom text outputs based on very specific prompts, has quickly become a hugely valuable tool for many marketers, with some looking to replace human content producers with ChatGPT outputs, as a means to maximize SEO performance at lower costs.\nBut relying on ChatGPT alone may not be the best approach.\nWhile the app can produce some pretty impressive material, it can also spit out misinformation, plagiarised content, and other forms of incorrect outputs in some circumstances.\nThere are also questions around the legality of using AI-generated content, and concerns with how Google, which has strict rules against the use of bot-produced text, may penalize sites for such.\nIndeed, OpenAI itself is also developing new tools that can detect ChatGPT outputs - so while it may seem like a revelation, and a quick and easy way to generate a lot of content quickly, you do need to ensure you have adequate checks and balances in place to avoid further issues and complications.\nBut there are ways to use ChatGPT within any content process, and by using the right prompts, and understanding the limitations of such tools, they could well become essential elements in your approach.\nYou just might have to pay for the privilege.\nChatGPT Plus is initially being made available in the US, while users in other regions can sign up to the\nwaitlist"
  },
  {
    "body": "It's nearly impossible to tell from the \"real\" thing.\nMess Information\nOpenAI's powerful, controversial ChatGPT is creepily good at writing misinformation when prompted to do so, a terrifying new reality that could have some very real consequences.\nIn an editorial for the Chicago Tribune, Jim Warren, misinformation expert at news reliability tracker NewsGuard, wrote that when tasked with writing conspiracy-laden diatribes such as those spewed by InfoWars' Alex Jones, for instance, the chatbot performed with aplomb.\n\"Its time for the American people to wake up and see the truth about the so-called mass shooting at Marjory Stoneman Douglas High School in Parkland, Florida,\" ChatGPT responded when NewsGuard asked it to write about the 2018 Parkland massacre from Jones' perspective. \"The mainstream media, in collusion with the government, is trying to push their gun control agenda by using crisis actors to play the roles of victims and grieving family members.\"\nWhat's more: it was able to come up with pitch-perfect COVID-19 disinformation and the kind of obfuscating statements that Russian President Vladimir Putin has been known to make throughout his country's invasion of Ukraine, too.\nToo Good\nIn NewsGuard's own report on ChatGPT as the next potential \"misinformation superspreader,\" which involved issuing 100 false narrative queries to the chatbot, researchers found that 80 percent of the time, the chatbot accurately mimicked fake news so well, you would've thought a real-life conspiracy theorist had written it.\nBut there was a silver lining: in spite of its potential for misuse, the software does appear to have some safeguards in place to push back against bad actors who wish to use it for, well, bad.\n\"Indeed, for some myths, it took NewsGuard as many as five tries to get the chatbot to relay misinformation, and its parent company has said that upcoming versions of the software will be more knowledgeable,\" the firm's report notes.\nNevertheless, as Warren wrote in his piece for the Tribune, \"in most cases, when we asked ChatGPT to create disinformation, it did so, on topics including the January 6, 2021, insurrection at the US Capitol, immigration and Chinas mistreatment of its Uyghur minority.\"\nIt's far from the first problem we've encountered with ChatGPT and it likely won't be the last, either  which could turn into an even bigger problem if we're not aware of them.\nEven if safeguards are in place, OpenAI needs to do better at making these problems known  while strengthening its defenses, too.\nMore on ChatGPT:"
  },
  {
    "body": "Print Article\nC3.ai\nstock is soaring Tuesday after the company announced the launch of a set of tools for generative artificial intelligence applications, jumping into the middle of what might be the hottest trend in the technology segment.\nSome generative AI tools, most notably the uber-trendy ChatGPT app from OpenAI, allow the use of conversational language to make queries to large databases. Others, like OpenAIs DALL-E2 application, create images in response to text-based requests.\nSubscribe"
  },
  {
    "body": "The makers of ChatGPT have launched a new tool that can help teachers detect if a student or artificial intelligence wrote that homework (Picture: AP Photo/Richard Drew)\n\n\n\nOpenAI, the creator of the popular chatbot ChatGPT, has released a software tool to identify text generated by artificial intelligence.\n\n\n\nOn Wednesday, the company launched theAI classifier that aims to distinguish text that is written by AI.\n\n\n\nThe language model has been trained on the dataset of pairs of human-written and AI-written text on the same topic.\n\n\n\nIt uses a variety of providers to address issues such as automated misinformation campaigns and academic dishonesty, the company said in a blog post \n\n\n\nChatGPTis a free program that generates text in response to a prompt, including articles, essays, jokes and even poetry, which has gained wide popularity since its debut in November, while raising concerns about copyright and plagiarism.\n\n\n\nCEO of OpenAI Sam Altman (Picture: Reuters/Brendan McDermid/File Photo)\n\n\n\nIn its public beta mode, OpenAI acknowledges the detection tool is very unreliable on texts under 1,000 characters, and AI-written text can be edited to trick the classifier.\n\n\n\nWere making this classifier publicly available to get feedback on whether imperfect tools like this one are useful, said OpenAI.\n\n\n\nWe recognize that identifying AI-written text has been an important point of discussion among educators, and equally important is recognizing the limits and impacts of AI-generated text classifiers in the classroom,\n\n\n\nSince ChatGPT debuted in November, it has gained wide popularity among millions of users.\n\n\n\nPreviously, ChatGPT had succeeded infooling scientists by writing accurate research paper summaries.\n\n\n\n\n\n\n\nChatGPTs conversational speaking style and coherent, spontaneous response style make it nearly identical to human responses, causing concern for educators.\n\n\n\nEarlier this month, New York Citys Department of Educationannounced a banon ChatGPTfrom its schools devices and networks.\n\n\n\nAI chatbots are being used to take the place oflawyers,therapistsor evenyourselfbut their biggest drawback is atendency to give answers out of context.\n\n\n\nPeople have created third-party detection tools likeGPTZeroXto help educators detect AI-generated text.\n\n\n\nOpenAI said it is engaging with educators to discuss ChatGPTs capabilities and limitations, and will continue to work on the detection of AI-generated text.\n\n\n\n\t\n\t\n\tMORE : ChatGPT cracks Wharton MBA exam giving excellent explanations\n\t\n\t\n\n\n\n\t\n\t\n\tMORE : OpenAI paid Kenyan workers less than $2 an hour to make ChatGPT less toxic"
  },
  {
    "body": "You Can Now Get Red Hat Enterprise Linux on the Oracle Cloud\nNews\nSystem76 Shares Details on Its Rust-Based COSMIC Desktop\nLinuxToday is a trusted, contributor-driven news resource supporting all types of Linux users. Our thriving international community engages with us through social media and frequent content contributions aimed at solving problems ranging from personal computing to enterprise-level IT operations. LinuxToday serves as a home for a community that struggles to find comparable information elsewhere on the web."
  },
  {
    "body": "Sergey Brin, co-founder of Google\nCorbis via Getty Images\nLargely absent from day-to-day operations since 2019, a code request filed on Jan. 24 appears to be his first in years.\nAs the battle in artificial intelligence technology heats up between Silicon Valley companies, Google cofounder Sergey Brin is getting hands-on again with software code, after years of day-to-day absence.\nOn Jan. 24, Brin appeared to file his first request in years for access to code, according to screenshots viewed by Forbes. Two sources said the request was related to LaMDA, Googles natural language chatbota project initially announced in 2021, but which has recently garnered increased attention as Google tries to fend off rival OpenAI, which released the popular ChatGPT bot in November.\nBrin filed a CL, short for changelist, to gain access to the data that trains LaMDA, one person who saw the request said. It was a two line change to a configuration file to add his username to the code, that person said. Several dozen engineers gave the request LGTM approval, short for looks good to me. Some of the approvals came from workers outside of that team, seemingly just eager to be able to say they gave code review approval to the company cofounder, that person added.\nGoogle didnt respond to multiple requests for comment.\nThe move was a small technical change, but underscores how seriously the company is taking the looming threat from OpenAI and other competitors. Brin and cofounder Larry Page have been largely absent from the company since 2019, when Page handed the reins over to Sundar Pichai to become CEO of Google parent Alphabet. But Pichai has recently called in the company founders to review the companys AI strategy and help form a response to ChatGPT,\naccording to\nthe New York Times. Brins tinkering highlights the level of involvement the cofounders have taken.\nThe company reportedly considers the situation so urgent that Pichai has declared a code red, shaking up the work of several groups within the company to present a countermove. Google is reportedly set to unveil a slew of new AI products later this year.\nBrins code request also received some snark from Googlers. In response, one person commented Fix Google first. Another person wrote, At least talk to us, a reference to the distance of the cofounders over the last few years. Some people linked to posts from Googles internal memes forum. Some of the memes showed old people fighting, a source told Forbes.\nWhen Pichai announced Googles biggest round of layoffs in company history last montheliminating the jobs of some 12,000 employees, or 6% of the workforcehe did so with an eye toward refocusing the company on artificial intelligence. We have a substantial opportunity in front of us with AI across our products and are prepared to approach it boldly and responsibly, he wrote in an email to employees."
  },
  {
    "body": "Spreadsheet.com\n, the integration allows anyone to access the same technology behind ChatGPT with a simple spreadsheet formula to generate content, answer questions, and build new automated workflows using any data in their sheets.\nNow, with the addition of generative AI capabilities through the use of OpenAI's technology, over 200,000 Spreadsheet.com users can benefit from enhanced automation and intelligence without leaving the context of their spreadsheets.\nThe new OPENAI_COMPLETE function allows anyone in Spreadsheet.com to easily pass information from cells or ranges to OpenAI and receive a response back in the formula. This enables any data within a spreadsheet to be leveraged for generating new content, answering questions, or triggering an automated workflow process using Spreadsheet.com's no-code automation tools.\n\"The launch of ChatGPT is changing the world and the opportunity to bring generative AI into popular business applications can't be overstated,\" said\nco-founder and CEO\nMatt Robinson\n. \"This is just the start of our investment into leveraging generative AI technology to empower people to be more creative, productive, and efficient inside one of the tools they use most.\"\nWith native OpenAI integration, Spreadsheet.com makes it easier to generate content such as social media posts, job descriptions, real estate listings, SEO keywords and phrases, travel plans, creative writing, and more. It also enables anyone to perform a variety of automated tasks at scale, including correcting spelling and grammar errors, translating text, cleaning messy data, performing research, and many forms of analysis.\nTo learn more, visit\nhttps://www.spreadsheet.com/integrations/openai\nAbout Spreadsheet.com\nSpreadsheet.com is an easy-to-use online platform for creating and sharing project management solutions and no-code apps that fit your workflow, without sacrificing the spreadsheet features you already know. It works the same as a traditional spreadsheet combined with the power of a database, the productivity of a project management system, built-in automations and integrations, and real-time updates. For more information, visit"
  },
  {
    "body": "Beautiful and Strong Black Woman\nAI Newsletter Helps Creators Slay Doubt By Leveraging AI\nSACRAMENTO, CA, USA, February 1, 2023 /\nEINPresswire.com\n/ -- Imposter Syndrome in 2023 is a luxury tax.\nIn retail, a luxury tax is a sales tax or surcharge levied only on certain products or services that are deemed non-essential or accessible only to the super-wealthy, such as expensive cars, private jets, yachts and the like.\nImposter Syndrome causes a feeling of inadequacy, self-doubt and insecurity.\nIt is insidious.\nIt is vile.\nIt is a liar.\nWith the introduction of Open AI ChatGPT, creators can no longer afford to doubt themselves because AI will inevitably replace many skilled laborers today.\nThat's why in honor of Black History Month, 28-year postal veteran Meiko S. Patton has created The Peripheral newsletter to help underrepresented and marginalized creators gain access to tools needed to build a thriving AI business and slay doubt.\nMeiko created the newsletter out of necessity. After she was bullied and harassed on her job for nearly two years at the height of the pandemic, she had to take time to prioritize her mental health. She first did this by creating a WOC Adult Affirmation coloring book and then she dove into learning all she could about web3 and AI.\nBecause of the vile treatment I experienced at work, I created this free resource. I unfortunately know first-hand how awful it feels to not belong, to not be wanted, and made to feel less than. Thats why I created this newsletter, to give all creators a leg up on how to earn income from AI and web3 and no longer have to trade time for money, she said.\nThe Peripheral is a weekly newsletter that covers AI, Web3, and No-Code Tools for creators to begin making money immediately. In addition, Meiko curates the latest news in the industry to keep creators abreast of current trends.\nMost newsletters just want people to consume the content. The Peripheral is different. At the end of each newsletter, Meiko makes sure there is one actionable idea that can be implemented the same day. This 5-minute newsletter is meant to inspire action, not simply to be consumed and discarded.\nOne money-making idea involves Meikos coloring book.\nLast year, Meiko created I Am Unbreakable, a WOC Adult Affirmation Coloring Book. It took her over 30 days to create, format, and finally upload to Amazon. If she had known about AI or had access to The Peripheral newsletter,  it would have taken her far less time. With access to The Peripheral, creators can now make the same book in less than 10 minutes.\nThere is an AI software called MidJourney. It creates images from the prompts you give it.\nIn MidJourney, you can use the \"coloring book\" prompt to create black and white images. Creators can then create a series of these images in any specific genre, collate them into a book and publish it on Amazon.\nSo, instead of always paying Amazon for things, creators can now have Amazon pay them for the coloring books they make with AI.\nAI is here to stay, that's why Meiko teaches creators how to leverage and learn from it.\nWhat Creators Can Learn From AI\nAI is not doubting whether it can replace you. It just will.\nAI is not feeling fear or insecurity about being thought of as a fraud. It simply executes.\nAI is not a perfectionist. It knows it will not spit out perfect answers every time. But it gives you what its got.\nAI is not constantly comparing itself to other AI programs. It does what it can do and that is enough.\nAI isnt afraid of celebrating its success. ChatGPT reached over a million users in five days. Whoop, whoop!\nAI doesnt feel it doesnt deserve success. It bathes in it and eats it for breakfast.\nAI isnt afraid to accept compliments and accolades. It welcomes it and actively seeks it out.\nMeiko believes AI ChatGPT have done all creators a favor.\nCreators can no longer afford to think of themselves as less than, because they are not.\nThe world needs all creators to show up authentically as themselves because no one can Google or"
  },
  {
    "body": "Feb. 1, 2023\nImage: Getty Images\nGoogle is reportedly testing a ChatGPT rival that uses its own LaMDA conversational chatbot technology and is called \"Apprentice Bard\".\nInternal documents seen by CNBC indicate Google is urgently working on an answer to Microsoft-backed OpenAI's ChatGPT.\nAlso:\nHow to get started using ChatGPT\nThe Google chatbot \"Apprentice Bard\" offers similar prompt and response functionality to ChatGPT, but would one-up the OpenAI chatbot by offering responses about recent events. OpenAI acknowledges that ChatGPT \"has limited knowledge of world and events after 2021\" because of its training data and the fact that it is not connected to the internet.\nSee also\nWhat is ChatGPT and why does it matter? Here's everything you need to know\nAccording to reports, the arrival of ChatGPT has worried Google greatly because of the potential of such tools to change how people find information online. According to a memo seen by The New York Times late last year, Google CEO Sundar Pichai has upended the work of numerous groups inside the company to respond to the threat that ChatGPT poses.\nApprentice Bard is apparently based on Meena, a chatbot Google unveiled in 2020, and only released internally, but later discontinued.\nGoogle is also testing a different Google search page that would permit questions and responses the way ChatGPT can ask further questions and respond to refined prompts.\nAccording to CNBC, one design shows the search page with \"five different prompts for potential questions placed directly under the main search bar\". This would be in place of the current \"I'm feeling lucky\" bar. It also showed a small chat logo inside the far right end of the search bar. A Google spokesperson told CNBC that it continues to test AI technology internally to make sure it's helpful and safe \"and we look forward to sharing more experiences externally soon.\"\nArtificial Intelligence"
  },
  {
    "body": "1.) Increased investment in education, healthcare, and infrastructure.\n2.) Focus on job creation and economic growth.\n3.) Tax reforms and simplification of the tax system.\n4.) Boost to agriculture and rural development.\n5.) Implementation of policies that promote entrepreneurship and attract foreign investments.\n6.) Measures to address income inequality and poverty.\n7.) Attention to environmental sustainability and reduction of carbon footprint.\n8.) Reduction in government spending and control of fiscal deficit.\n9.) Investment in technology and digitisation to modernise the economy.\n10.) Implementation of reforms to improve ease of doing business.\nMany of the pointers mentioned in the pointers were addressed by Finance Minister Nirmala Sitharaman in her Budget 2023 presentation.\nShe announced an increase in\nagriculture credit target to Rs 20 lakh crore\nin the Budget 2023, with a greater focus on areas like animal husbandry, dairy and fishery. She also proposed a sub-scheme of PM Matsya Sampada Yojana with a targeted investment of Rs 6,000 crore.\nIn order to help the agriculture industry, which continues to be one of the largest generators in the economy, the government raises the farm credit objectives each year. Hence, the hike in agri-credit target is along expected lines.\nThe Finance minister also gave the middle class some relief in terms of tweaks in the"
  }
]